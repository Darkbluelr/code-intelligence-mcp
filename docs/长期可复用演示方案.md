# 长期可复用演示方案（Code Intelligence MCP）

版本：2026-01-22

本方案用于构建可长期复用、可对比升级效果的演示体系，覆盖软件开发与维护的关键维度。
目标是让观众在 5-15 分钟内直观看到价值，并支持后续版本的对比与归档。

---

## 1. 结论与建议（先给决策）

### 1.1 是否可实现
可以实现。仓库已具备：上下文注入、图谱检索、调用链、影响分析、基准评测、缓存与预热、性能回归检测等能力。只需把现有工具组织成“演示基线 + 版本对比 + 可视化输出”即可。

### 1.2 是否最佳实践
不是行业绝对“最佳实践”，但接近可落地的工程化最佳实践。最佳实践应满足：
- 有固定的演示对象（可重现实验环境）
- 有量化指标（MRR/Recall/P95/准确率/回归阈值）
- 有升级对比（同一脚本、同一数据集、同一版本仓库）
- 有可视化输出（图、表、对比报告）

本方案通过“固定开源项目 + 固定脚本 + 固定数据集 + 对比报告”实现以上目标。

### 1.3 是否需要改进
建议优化方向：
- 从“单一场景 demo”升级为“维度矩阵式 demo”，体现工程全生命周期价值。
- 加入版本对比报告（同一数据集跑出对比表）。
- 输出格式标准化（JSON + Markdown），便于自动化归档与对外展示。

---

## 2. 演示目标与覆盖维度

参照 `dev-playbooks/docs/如何构建完备的系统.md` 的“完备性 + 正交性”思路，演示维度建议如下（MECE）：

### 维度 A：理解与检索（Understanding）
- 目标：AI 快速理解代码结构与定位相关上下文
- 工具：`ci_search`、`ci_graph_rag`

### 维度 B：诊断与定位（Diagnosis）
- 目标：快速定位 Bug 与上下文
- 工具：`ci_bug_locate`、`ci_call_chain`

### 维度 C：影响与变更（Impact）
- 目标：分析修改影响面，降低改错风险
- 工具：`ci_impact`、`ci_call_chain`

### 维度 D：质量与风险（Quality & Risk）
- 目标：识别复杂度、热点、语义异常、架构漂移
- 工具：`ci_hotspot`、`ci_complexity`、`ci_semantic_anomaly`、`ci_drift_detect`

### 维度 E：性能与稳定性（Performance）
- 目标：验证检索性能、缓存收益、回归检测
- 工具：`ci_benchmark`、`ci_warmup`、`performance-regression.sh`

### 维度 F：演进与持续化（Evolution）
- 目标：记录版本差异、演示升级收益
- 工具：基准结果对比 + 固定报告输出

这些维度基本覆盖了“软件开发与维护的所有核心方面”，且彼此正交。

---

## 3. 演示对象选择与建议

### 3.1 开源项目建议
推荐分为两类：

1) 中型代码库（演示首选）
- 例如：`fastify/fastify`、`pnpm/pnpm`、`vercel/next.js`（子模块多）
- 优点：足够复杂但仍可在本机索引

2) 大型项目（高阶演示）
- `microsoft/vscode`
- 优点：震撼力大
- 风险：索引耗时长，演示时间不可控

建议：
- 首轮演示选“中型项目”作为稳定可复用演示。
- 如需展示“规模化能力”，再用 VSCode 做专场（需要预热与缓存）。

### 3.2 是否要人工注入 Bug
可以，但要保证可验证。推荐方式：
- 在演示项目创建一个小分支，注入一个可定位 bug，并记录 bug 位置与期望定位结果。
- 同时准备“非注入的正常场景”，以证明工具在真实问题上的价值。

---

## 4. 演示方式（建议方案）

### 方案 1：终端脚本 + 对比报告（最佳性价比）
- 优点：稳定、可复现、易比对
- 输出：终端日志 + `report.md` + `metrics.json`

### 方案 2：终端 + 截图/短录屏（更直观）
- 将关键步骤输出截图
- 最后附一张对比图表

### 方案 3：Web Demo Dashboard（后续升级）
- 成本较高，不建议作为首版

推荐：先做方案 1 + 少量截图。

---

## 5. 建议的演示流程（15 分钟内）

### 第 0 步：准备
- 选择演示项目（建议中型项目）
- 执行索引预热（daemon warmup）

### 第 1 步：有/无上下文注入对比
- 目标：展示“减少多轮对话”
- 使用：`demo/demo-context-injection.sh --compare`

### 第 2 步：理解维度
- 目标：语义检索 + 图谱上下文
- 使用：`ci_search` + `ci_graph_rag`

### 第 3 步：诊断维度
- 目标：定位 bug 或错误相关函数
- 使用：`ci_bug_locate` + `ci_call_chain`

### 第 4 步：影响维度
- 目标：修改影响面可视化
- 使用：`ci_impact`

### 第 5 步：质量维度
- 目标：热点 + 复杂度 + 漂移
- 使用：`ci_hotspot` + `ci_complexity` + `ci_drift_detect`

### 第 6 步：性能维度
- 目标：P95、MRR/Recall 输出
- 使用：`ci_benchmark`（输出 JSON）

### 第 7 步：输出对比报告
- 记录结果：`metrics.json` + `report.md`

---

## 6. 版本对比机制（长期复用核心）

### 6.1 固定输出格式
- `metrics.json`：包含 MRR/Recall/P95/命中率/缓存命中等
- `report.md`：包含 Demo 版本、环境、数据集摘要、关键截图

### 6.2 对比逻辑
- 新版本跑相同脚本
- 对比 `metrics.json` 关键字段
- 输出差异表（Markdown）

### 6.3 建议目录结构

```
docs/
  demos/
    2026-01-22-v1/
      metrics.json
      report.md
      screenshots/
    2026-02-xx-v2/
      metrics.json
      report.md
```

---

## 7. 可复用演示资产与现有工具

### 已有工具/脚本
- `demo/demo-context-injection.sh`（上下文注入对比）
- `hooks/context-inject-global.sh`（结构化上下文）
- `scripts/benchmark.sh`（MRR/Recall/P95 评测）
- `scripts/graph-rag.sh`（图谱检索）
- `scripts/call-chain.sh`（调用链）
- `scripts/hotspot-analyzer.sh`（热点）
- `scripts/drift-detector.sh`（漂移）

### 可用“轮子”
- 对比报告生成：可直接用 `benchmark.sh --compare` 输出
- 数据集：`tests/fixtures/benchmark/queries.jsonl` 可作为初始查询集

---

## 8. 对 demo 目录的建议改造

当前 `demo/` 只提供“上下文注入演示”。建议升级为“全维度演示”。

### 方案：目录重命名 + 分层
- `demo/` -> `demo-suite/` 或 `demo-lab/`
- 结构示例：

```
demo-suite/
  00-quick-compare.sh
  01-understanding.sh
  02-diagnosis.sh
  03-impact.sh
  04-quality.sh
  05-performance.sh
  README.md
```

如你确认，我可以直接改名并整理脚本。

---

## 9. 下一步建议（可执行）

1. 选定演示项目（中型或 VSCode）
2. 固定 queries.jsonl（用于评测）
3. 输出第一份 `docs/demos/2026-01-22-v1/` 报告
4. 后续版本按同脚本跑，形成对比表

---

## 10. 需要你确认的点

1) 演示项目选哪个？（建议先中型，再 VSCode 专场）
2) 是否允许我重命名 `demo/` 并按“维度矩阵”改造？
3) 是否需要我直接生成第一份 `docs/demos/2026-01-22-v1/` 产物？

如你确认，我可以继续执行目录改造与演示脚本编排。
