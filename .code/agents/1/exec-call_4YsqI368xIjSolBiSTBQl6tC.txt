scripts/cache-utils.sh:8:: "${CACHE_DIR:=${TMPDIR:-/tmp}/.devbooks-cache}"
scripts/cache-utils.sh:12:ensure_cache_dir() {
scripts/cache-utils.sh:21:get_cache_key() {
scripts/cache-utils.sh:37:get_cached() {
scripts/cache-utils.sh:40:  key="$(get_cache_key "$raw_key")"
scripts/cache-utils.sh:41:  local cache_file="$CACHE_DIR/$key"
scripts/cache-utils.sh:43:  if [ -f "$cache_file" ]; then
scripts/cache-utils.sh:46:    if stat -f %m "$cache_file" &>/dev/null; then
scripts/cache-utils.sh:48:      file_mtime=$(stat -f %m "$cache_file")
scripts/cache-utils.sh:49:    elif stat -c %Y "$cache_file" &>/dev/null; then
scripts/cache-utils.sh:51:      file_mtime=$(stat -c %Y "$cache_file")
scripts/cache-utils.sh:62:      cat "$cache_file"
scripts/cache-utils.sh:71:set_cache() {
scripts/cache-utils.sh:75:  key="$(get_cache_key "$raw_key")"
scripts/cache-utils.sh:76:  local cache_file="$CACHE_DIR/$key"
scripts/cache-utils.sh:78:  ensure_cache_dir
scripts/cache-utils.sh:79:  echo "$content" > "$cache_file" 2>/dev/null
scripts/cache-utils.sh:84:cleanup_expired_cache() {
scripts/cache-utils.sh:92:clear_all_cache() {
scripts/cache-utils.sh:99:ensure_cache_dir
src/ast-delta.ts:12: * @module ast-delta
src/ast-delta.ts:44:  ast?: AstNode;
src/ast-delta.ts:118:let cachedParser: TreeSitterParser | null = null;
src/ast-delta.ts:151:  if (cachedParser) {
src/ast-delta.ts:152:    return cachedParser;
src/ast-delta.ts:168:    cachedParser = parser;
src/ast-delta.ts:171:    console.error('[ast-delta] Failed to load tree-sitter:', error);
src/ast-delta.ts:265:function parseWithRegex(code: string, filePath: string): AstNode {
src/ast-delta.ts:266:  const lines = code.split('\n');
src/ast-delta.ts:287:  while ((match = funcRegex.exec(code)) !== null) {
src/ast-delta.ts:288:    const lineNum = code.substring(0, match.index).split('\n').length;
src/ast-delta.ts:300:  while ((match = classRegex.exec(code)) !== null) {
src/ast-delta.ts:301:    const lineNum = code.substring(0, match.index).split('\n').length;
src/ast-delta.ts:313:  while ((match = interfaceRegex.exec(code)) !== null) {
src/ast-delta.ts:314:    const lineNum = code.substring(0, match.index).split('\n').length;
src/ast-delta.ts:326:  while ((match = typeRegex.exec(code)) !== null) {
src/ast-delta.ts:327:    const lineNum = code.substring(0, match.index).split('\n').length;
src/ast-delta.ts:339:  while ((match = arrowFuncRegex.exec(code)) !== null) {
src/ast-delta.ts:340:    const lineNum = code.substring(0, match.index).split('\n').length;
src/ast-delta.ts:366: * @param code - Ê∫ê‰ª£Á†ÅÂ≠óÁ¨¶‰∏≤
src/ast-delta.ts:371:  code: string,
src/ast-delta.ts:379:    return parseWithRegex(code, filePath);
src/ast-delta.ts:385:    return parseWithRegex(code, filePath);
src/ast-delta.ts:389:    const tree = parser.parse(code);
src/ast-delta.ts:403:      endLine: code.split('\n').length,
src/ast-delta.ts:407:    console.error('[ast-delta] Parse error, falling back to regex:', error);
src/ast-delta.ts:408:    return parseWithRegex(code, filePath);
src/ast-delta.ts:523: * @param ast - AST ËäÇÁÇπ
src/ast-delta.ts:526:export function serializeAst(ast: AstNode): string {
src/ast-delta.ts:527:  return JSON.stringify(ast, null, 2);
src/ast-delta.ts:543: * @param code - Ê∫ê‰ª£Á†Å
src/ast-delta.ts:548:  code: string,
src/ast-delta.ts:554:    const ast = parseTypeScript(code, config);
src/ast-delta.ts:559:      ast,
src/ast-delta.ts:575: * @param ast - AST Ê†πËäÇÁÇπ
src/ast-delta.ts:579:  ast: AstNode
src/ast-delta.ts:602:  collect(ast);
scripts/bug-locator.sh:12:#   bug-locator.sh --error "ÈîôËØØ‰ø°ÊÅØ" [ÈÄâÈ°π]
scripts/bug-locator.sh:77:IMPACT_ANALYZER="${SCRIPT_DIR}/impact-analyzer.sh"
scripts/bug-locator.sh:80:HOTSPOT_ANALYZER="${SCRIPT_DIR}/hotspot-analyzer.sh"
scripts/bug-locator.sh:83:CACHE_MANAGER="${SCRIPT_DIR}/cache-manager.sh"
scripts/bug-locator.sh:92:    w=$(get_feature_value "hotspot_weight" "")
scripts/bug-locator.sh:111:  bug-locator.sh --error "ÈîôËØØ‰ø°ÊÅØ" [ÈÄâÈ°π]
scripts/bug-locator.sh:119:  --with-impact         ÂêØÁî®ÂΩ±ÂìçÂàÜÊûêËûçÂêàÔºàAC-G08Ôºâ
scripts/bug-locator.sh:120:  --impact-depth <n>    ÂΩ±ÂìçÂàÜÊûêÊ∑±Â∫¶ÔºàÈªòËÆ§: 3Ôºâ
scripts/bug-locator.sh:133:        "is_hotspot": true,
scripts/bug-locator.sh:137:          "hotspot_score": 0.7,
scripts/bug-locator.sh:144:Â∏¶ÂΩ±ÂìçÂàÜÊûêÁöÑËæìÂá∫Ê†ºÂºè (--with-impact):
scripts/bug-locator.sh:154:        "impact": {
scripts/bug-locator.sh:165:  bug-locator.sh --error "TypeError: Cannot read property 'id' of undefined"
scripts/bug-locator.sh:168:  bug-locator.sh --error "NullPointerException at User.getName" --history-depth 60
scripts/bug-locator.sh:171:  bug-locator.sh --error "Error in payment processing" --format text
scripts/bug-locator.sh:174:  bug-locator.sh --error "authentication error" --with-impact --impact-depth 3
scripts/bug-locator.sh:180:  echo "bug-locator.sh version 1.0.0"
scripts/bug-locator.sh:209:      --with-impact)
scripts/bug-locator.sh:213:      --impact-depth)
scripts/bug-locator.sh:357:  local call_chain_tool="${SCRIPT_DIR}/call-chain-tracer.sh"
scripts/bug-locator.sh:472:# Trace: AC-001 - Ë∞ÉÁî® hotspot-analyzer.sh Ëé∑ÂèñÁÉ≠ÁÇπÂàÜÊï∞
scripts/bug-locator.sh:473:get_hotspot_files() {
scripts/bug-locator.sh:479:  # AC-010: Ê£ÄÊü• hotspot_analyzer ÂäüËÉΩÊòØÂê¶ÂêØÁî®
scripts/bug-locator.sh:480:  local hotspot_enabled=true
scripts/bug-locator.sh:482:    is_feature_enabled "hotspot_analyzer" || hotspot_enabled=false
scripts/bug-locator.sh:485:  # ‰ºòÂÖà‰ΩøÁî® hotspot-analyzer.shÔºàAC-001Ôºâ
scripts/bug-locator.sh:486:  if [ "$hotspot_enabled" = true ] && [ -x "$HOTSPOT_ANALYZER" ]; then
scripts/bug-locator.sh:487:    local hotspot_result
scripts/bug-locator.sh:488:    hotspot_result=$("$HOTSPOT_ANALYZER" --format json --path "$CWD" --top 20 --days "$HISTORY_DEPTH" 2>/dev/null) || true
scripts/bug-locator.sh:490:    # È™åËØÅÊòØÂê¶‰∏∫ÊúâÊïà JSONÔºàÊñ∞Ê†ºÂºèÔºö{schema_version, hotspots: [...]}Ôºâ
scripts/bug-locator.sh:491:    if echo "$hotspot_result" | jq -e '.hotspots' >/dev/null 2>&1; then
scripts/bug-locator.sh:492:      # Êñ∞Ê†ºÂºèÔºöhotspot-analyzer ËæìÂá∫ {schema_version: "1.0", hotspots: [{file, score, frequency, complexity}...]}
scripts/bug-locator.sh:493:      # bug-locator ÈúÄË¶Å [{file_path, change_count, score, complexity}]
scripts/bug-locator.sh:494:      echo "$hotspot_result" | jq '[.hotspots[] | {
scripts/bug-locator.sh:498:        complexity: .complexity
scripts/bug-locator.sh:503:    _maybe_log_warn "hotspot-analyzer.sh ËæìÂá∫Êó†ÊïàÔºåÈôçÁ∫ßÂà∞ÂÜÖÁΩÆÂÆûÁé∞"
scripts/bug-locator.sh:505:    if [ "$hotspot_enabled" = false ]; then
scripts/bug-locator.sh:506:      _maybe_log_warn "hotspot_analyzer ÂäüËÉΩÂ∑≤Á¶ÅÁî®Ôºå‰ΩøÁî®ÂÜÖÁΩÆÁÉ≠ÁÇπËÆ°ÁÆó"
scripts/bug-locator.sh:508:      _maybe_log_warn "hotspot-analyzer.sh ‰∏çÂèØÁî®Ôºå‰ΩøÁî®ÂÜÖÁΩÆÁÉ≠ÁÇπËÆ°ÁÆó"
scripts/bug-locator.sh:521:    grep -vE 'node_modules|dist|build|\.lock|\.md$|\.json$|__pycache__|\.pyc$' | \
scripts/bug-locator.sh:524:  local hotspots='[]'
scripts/bug-locator.sh:534:      hotspots=$(echo "$hotspots" | jq --arg f "$file" --argjson c "$count" \
scripts/bug-locator.sh:539:  echo "$hotspots"
scripts/bug-locator.sh:543:# Trace: AC-001 - ‰ΩøÁî® hotspot-analyzer.sh ÁöÑÁªºÂêàÂàÜÊï∞ÔºàFrequency √ó ComplexityÔºâ
scripts/bug-locator.sh:544:add_hotspot_scores() {
scripts/bug-locator.sh:546:  local hotspots_json="$2"
scripts/bug-locator.sh:554:  max_score=$(echo "$hotspots_json" | jq '[.[].score // 0] | max // 1')
scripts/bug-locator.sh:564:    local is_hotspot hotspot_score raw_score
scripts/bug-locator.sh:565:    local hotspot_entry
scripts/bug-locator.sh:566:    hotspot_entry=$(echo "$hotspots_json" | jq --arg f "$file_path" '.[] | select(.file_path == $f)')
scripts/bug-locator.sh:568:    if [ -n "$hotspot_entry" ] && [ "$hotspot_entry" != "null" ]; then
scripts/bug-locator.sh:569:      is_hotspot=true
scripts/bug-locator.sh:570:      raw_score=$(echo "$hotspot_entry" | jq -r '.score // 0')
scripts/bug-locator.sh:572:      # ‰ΩøÁî® hotspot-analyzer.sh ÁöÑÁªºÂêàÂàÜÊï∞ÔºàÂ∑≤ËÄÉËôë Frequency √ó ComplexityÔºâ
scripts/bug-locator.sh:575:        hotspot_score=$(float_calc "$raw_score / $max_score")
scripts/bug-locator.sh:577:        cmp_result=$(float_calc "$hotspot_score > 1" 0)
scripts/bug-locator.sh:578:        [ "$cmp_result" = "1" ] && hotspot_score=1.0
scripts/bug-locator.sh:580:        hotspot_score=$(echo "scale=2; $raw_score / $max_score" | bc 2>/dev/null || echo "0.5")
scripts/bug-locator.sh:581:        [ "$(echo "$hotspot_score > 1" | bc 2>/dev/null || echo 0)" -eq 1 ] && hotspot_score=1.0
scripts/bug-locator.sh:584:      is_hotspot=false
scripts/bug-locator.sh:585:      hotspot_score=0
scripts/bug-locator.sh:589:      --argjson is_hot "$is_hotspot" \
scripts/bug-locator.sh:590:      --argjson score "$hotspot_score" \
scripts/bug-locator.sh:591:      '. + {is_hotspot: $is_hot, hotspot_score: $score}')
scripts/bug-locator.sh:674:    local call_chain_score history_score hotspot_score error_pattern_score
scripts/bug-locator.sh:677:    hotspot_score=$(echo "$candidate" | jq -r '.hotspot_score // 0')
scripts/bug-locator.sh:682:    local expr="$WEIGHT_CALL_CHAIN * $call_chain_score + $WEIGHT_HISTORY * $history_score + $WEIGHT_HOTSPOT * $hotspot_score + $WEIGHT_ERROR_PATTERN * $error_pattern_score"
scripts/bug-locator.sh:703:    [ "$(echo "$candidate" | jq -r '.is_hotspot')" = "true" ] && reasons+=("ÁÉ≠ÁÇπÊñá‰ª∂")
scripts/bug-locator.sh:723:_compute_bug_locator_cache_key() {
scripts/bug-locator.sh:742:# ÈÄâÊã©‰∏Ä‰∏™Â≠òÂú®ÁöÑÁºìÂ≠òÈîöÁÇπÊñá‰ª∂ÔºàÁî®‰∫é cache-manager Ê†°È™åÔºâ
scripts/bug-locator.sh:743:_resolve_bug_locator_cache_anchor() {
scripts/bug-locator.sh:750:    "$SCRIPT_DIR/bug-locator.sh"
scripts/bug-locator.sh:764:_get_cached_bug_result() {
scripts/bug-locator.sh:777:  # ‰ΩøÁî®ÁúüÂÆûÊñá‰ª∂‰Ωú‰∏∫ÁºìÂ≠òÈîöÁÇπÔºåÈÅøÂÖç cache-manager Áõ¥Êé• miss
scripts/bug-locator.sh:778:  local cache_anchor
scripts/bug-locator.sh:779:  cache_anchor=$(_resolve_bug_locator_cache_anchor "$CWD") || return 1
scripts/bug-locator.sh:781:  local cache_result
scripts/bug-locator.sh:782:  cache_result=$("$CACHE_MANAGER" --get "$cache_anchor" --query "$query_hash" 2>/dev/null)
scripts/bug-locator.sh:784:  if [[ -n "$cache_result" ]] && echo "$cache_result" | jq -e '.candidates' &>/dev/null; then
scripts/bug-locator.sh:786:    echo "$cache_result"
scripts/bug-locator.sh:794:_set_cached_bug_result() {
scripts/bug-locator.sh:808:  local cache_anchor
scripts/bug-locator.sh:809:  cache_anchor=$(_resolve_bug_locator_cache_anchor "$CWD") || return 0
scripts/bug-locator.sh:812:  "$CACHE_MANAGER" --set "$cache_anchor" --query "$query_hash" --value "$result" 2>/dev/null || true
scripts/bug-locator.sh:817:locate_bug() {
scripts/bug-locator.sh:822:  query_hash=$(_compute_bug_locator_cache_key "$error")
scripts/bug-locator.sh:824:  local cached_result
scripts/bug-locator.sh:825:  if cached_result=$(_get_cached_bug_result "$query_hash"); then
scripts/bug-locator.sh:826:    echo "$cached_result"
scripts/bug-locator.sh:843:    candidates=$(get_hotspot_files | jq '[.[] | {file_path, call_chain_score: 0.3}]')
scripts/bug-locator.sh:850:  local hotspots
scripts/bug-locator.sh:851:  hotspots=$(get_hotspot_files)
scripts/bug-locator.sh:852:  candidates=$(add_hotspot_scores "$candidates" "$hotspots")
scripts/bug-locator.sh:899:  _set_cached_bug_result "$query_hash" "$result"
scripts/bug-locator.sh:907:# Ê†ºÂºè: impact:${symbol_or_file}:${depth}
scripts/bug-locator.sh:908:_compute_impact_cache_key() {
scripts/bug-locator.sh:911:  echo "impact:${symbol_or_file}:${depth}"
scripts/bug-locator.sh:916:# ËøîÂõû: impact JSON ÊàñÁ©∫
scripts/bug-locator.sh:918:_get_candidate_impact() {
scripts/bug-locator.sh:946:  local cache_key
scripts/bug-locator.sh:947:  cache_key=$(_compute_impact_cache_key "$analysis_target" "$depth")
scripts/bug-locator.sh:950:    local cached_result
scripts/bug-locator.sh:951:    cached_result=$("$CACHE_MANAGER" cache-get "$cache_key" 2>/dev/null) || true
scripts/bug-locator.sh:953:    if [[ -n "$cached_result" ]] && echo "$cached_result" | jq -e '.' >/dev/null 2>&1; then
scripts/bug-locator.sh:954:      _maybe_log_info "ÂΩ±ÂìçÂàÜÊûêÁºìÂ≠òÂëΩ‰∏≠ (key=${cache_key:0:30}...)"
scripts/bug-locator.sh:955:      echo "$cached_result"
scripts/bug-locator.sh:961:  local impact_result
scripts/bug-locator.sh:978:      impact_result=$($timeout_cmd "$IMPACT_ANALYZER" analyze "$analysis_target" --depth "$depth" --format json 2>/dev/null) || true
scripts/bug-locator.sh:980:      impact_result=$("$IMPACT_ANALYZER" analyze "$analysis_target" --depth "$depth" --format json 2>/dev/null) || true
scripts/bug-locator.sh:985:      impact_result=$($timeout_cmd "$IMPACT_ANALYZER" file "$analysis_target" --depth "$depth" --format json 2>/dev/null) || true
scripts/bug-locator.sh:987:      impact_result=$("$IMPACT_ANALYZER" file "$analysis_target" --depth "$depth" --format json 2>/dev/null) || true
scripts/bug-locator.sh:992:  if [[ -z "$impact_result" ]] || ! echo "$impact_result" | jq -e '.' >/dev/null 2>&1; then
scripts/bug-locator.sh:1002:    "$CACHE_MANAGER" cache-set "$cache_key" "$impact_result" 2>/dev/null &
scripts/bug-locator.sh:1005:  echo "$impact_result"
scripts/bug-locator.sh:1011:add_impact_analysis() {
scripts/bug-locator.sh:1013:  local impact_weight="${BUG_LOCATOR_IMPACT_WEIGHT:-0.2}"
scripts/bug-locator.sh:1033:      local impact_result
scripts/bug-locator.sh:1034:      impact_result=$(_get_candidate_impact "$symbol_id" "$file_path")
scripts/bug-locator.sh:1036:      if [[ -n "$impact_result" && "$impact_result" != '{}' ]]; then
scripts/bug-locator.sh:1039:        total_affected=$(echo "$impact_result" | jq -r '.total_affected // 0')
scripts/bug-locator.sh:1040:        max_depth=$(echo "$impact_result" | jq -r '.depth // 3')
scripts/bug-locator.sh:1043:        affected_files=$(echo "$impact_result" | jq '[.affected_nodes[].file_path // empty] | unique | .[0:20]')
scripts/bug-locator.sh:1046:        # ËÆ°ÁÆóÂΩí‰∏ÄÂåñÂΩ±ÂìçÂàÜÊï∞ (normalized_impact = min(total_affected / 100, 1.0))
scripts/bug-locator.sh:1047:        local normalized_impact impact_score
scripts/bug-locator.sh:1049:          normalized_impact=$(float_calc "$total_affected / 100")
scripts/bug-locator.sh:1051:          cmp=$(float_calc "$normalized_impact > 1" 0)
scripts/bug-locator.sh:1052:          [[ "$cmp" = "1" ]] && normalized_impact="1.0"
scripts/bug-locator.sh:1053:          impact_score=$(float_calc "$normalized_impact")
scripts/bug-locator.sh:1055:          normalized_impact=$(echo "scale=4; $total_affected / 100" | bc 2>/dev/null || echo "0")
scripts/bug-locator.sh:1056:          [[ $(echo "$normalized_impact > 1" | bc 2>/dev/null || echo 0) -eq 1 ]] && normalized_impact="1.0"
scripts/bug-locator.sh:1057:          impact_score="$normalized_impact"
scripts/bug-locator.sh:1061:        # final_score = original_score * (1 + impact_weight * normalized_impact)
scripts/bug-locator.sh:1064:          final_score=$(float_calc "$original_score * (1 + $impact_weight * $normalized_impact)")
scripts/bug-locator.sh:1066:          final_score=$(echo "scale=4; $original_score * (1 + $impact_weight * $normalized_impact)" | bc 2>/dev/null || echo "$original_score")
scripts/bug-locator.sh:1074:          --argjson impact_score "$impact_score" \
scripts/bug-locator.sh:1080:            impact: {
scripts/bug-locator.sh:1084:              impact_score: $impact_score
scripts/bug-locator.sh:1125:      local file_path confidence reason is_hotspot line_range
scripts/bug-locator.sh:1129:      is_hotspot=$(echo "$candidate" | jq -r '.is_hotspot')
scripts/bug-locator.sh:1132:      local hotspot_marker=""
scripts/bug-locator.sh:1133:      [ "$is_hotspot" = "true" ] && hotspot_marker=" üî•"
scripts/bug-locator.sh:1135:      echo "$((i+1)). $file_path:$line_range$hotspot_marker"
scripts/bug-locator.sh:1149:  result=$(locate_bug "$ERROR_INFO")
scripts/bug-locator.sh:1158:    enhanced_candidates=$(add_impact_analysis "$candidates")
scripts/daemon.sh:2:# daemon.sh - Â∏∏È©ªÂÆàÊä§ËøõÁ®ãÁÆ°ÁêÜ
scripts/daemon.sh:12:# Socket: Unix domain socket ($DEVBOOKS_DIR/daemon.sock)
scripts/daemon.sh:22:: "${DAEMON_SOCK:=$DEVBOOKS_DIR/daemon.sock}"
scripts/daemon.sh:23:: "${DAEMON_PID_FILE:=$DEVBOOKS_DIR/daemon.pid}"
scripts/daemon.sh:26:: "${GRAPH_DB_PATH:=$DEVBOOKS_DIR/graph.db}"
scripts/daemon.sh:59:_get_restart_count() { cat "$DEVBOOKS_DIR/daemon.restarts" 2>/dev/null || echo "0"; }
scripts/daemon.sh:60:_set_restart_count() { echo "$1" > "$DEVBOOKS_DIR/daemon.restarts"; }
scripts/daemon.sh:61:_get_state() { cat "$DEVBOOKS_DIR/daemon.state" 2>/dev/null || echo "STOPPED"; }
scripts/daemon.sh:62:_set_state() { echo "$1" > "$DEVBOOKS_DIR/daemon.state"; }
scripts/daemon.sh:63:_get_queue_size() { cat "$DEVBOOKS_DIR/daemon.queue" 2>/dev/null || echo "0"; }
scripts/daemon.sh:64:_set_queue_size() { echo "$1" > "$DEVBOOKS_DIR/daemon.queue"; }
scripts/daemon.sh:73:_get_items_cached() { cat "$DEVBOOKS_DIR/warmup.items_cached" 2>/dev/null || echo "0"; }
scripts/daemon.sh:74:_set_items_cached() { echo "$1" > "$DEVBOOKS_DIR/warmup.items_cached"; }
scripts/daemon.sh:151:    cat "$DEVBOOKS_DIR/daemon.active_request" 2>/dev/null || echo ""
scripts/daemon.sh:156:    echo "$1" > "$DEVBOOKS_DIR/daemon.active_request"
scripts/daemon.sh:161:    rm -f "$DEVBOOKS_DIR/daemon.active_request" 2>/dev/null || true
scripts/daemon.sh:228:            if [ -x "$SCRIPT_DIR/graph-store.sh" ] && [ -f "$GRAPH_DB_PATH" ]; then
scripts/daemon.sh:229:                result=$("$SCRIPT_DIR/graph-store.sh" query "$payload" 2>/dev/null || echo '[]')
scripts/daemon.sh:254:_daemon_loop() {
scripts/daemon.sh:263:    local request_file="$DEVBOOKS_DIR/daemon.request"
scripts/daemon.sh:264:    local response_file="$DEVBOOKS_DIR/daemon.response"
scripts/daemon.sh:265:    local stop_flag="$DEVBOOKS_DIR/daemon.stop"
scripts/daemon.sh:332:    local daemon_pid="$1"
scripts/daemon.sh:339:        if ! _process_exists "$daemon_pid"; then
scripts/daemon.sh:351:            _daemon_loop </dev/null >/dev/null 2>&1 &
scripts/daemon.sh:352:            daemon_pid=$!
scripts/daemon.sh:353:            echo "$daemon_pid" > "$DAEMON_PID_FILE"
scripts/daemon.sh:360:ci_daemon_start() {
scripts/daemon.sh:376:    rm -f "$DEVBOOKS_DIR/daemon.stop" "$DEVBOOKS_DIR/monitor.stop"
scripts/daemon.sh:377:    rm -f "$DEVBOOKS_DIR/daemon.request" "$DEVBOOKS_DIR/daemon.response"
scripts/daemon.sh:380:    _daemon_loop </dev/null >/dev/null 2>&1 &
scripts/daemon.sh:381:    local daemon_pid=$!
scripts/daemon.sh:382:    echo "$daemon_pid" > "$DAEMON_PID_FILE"
scripts/daemon.sh:385:    _monitor_loop "$daemon_pid" </dev/null >/dev/null 2>&1 &
scripts/daemon.sh:395:    echo "Daemon started (pid=$daemon_pid)" >&2
scripts/daemon.sh:401:        if ! ci_daemon_warmup --async --format json >/dev/null 2>&1; then
scripts/daemon.sh:402:            log_error "Warmup failed during daemon startup"
scripts/daemon.sh:409:ci_daemon_stop() {
scripts/daemon.sh:411:    touch "$DEVBOOKS_DIR/daemon.stop"
scripts/daemon.sh:440:    rm -f "$DEVBOOKS_DIR/daemon.stop" "$DEVBOOKS_DIR/monitor.stop"
scripts/daemon.sh:441:    rm -f "$DEVBOOKS_DIR/daemon.request" "$DEVBOOKS_DIR/daemon.response"
scripts/daemon.sh:442:    rm -f "$DEVBOOKS_DIR/daemon.state" "$DEVBOOKS_DIR/daemon.restarts"
scripts/daemon.sh:443:    rm -f "$DEVBOOKS_DIR/daemon.queue"
scripts/daemon.sh:444:    rm -f "$DEVBOOKS_DIR/daemon.active_request"
scripts/daemon.sh:448:    rm -f "$DEVBOOKS_DIR/warmup.completed_at" "$DEVBOOKS_DIR/warmup.items_cached"
scripts/daemon.sh:454:ci_daemon_status() {
scripts/daemon.sh:477:    local items_cached=$(_get_items_cached)
scripts/daemon.sh:488:    json="$json,\"items_cached\":$items_cached}"
scripts/daemon.sh:493:ci_daemon_request() {
scripts/daemon.sh:509:    local request_file="$DEVBOOKS_DIR/daemon.request"
scripts/daemon.sh:510:    local response_file="$DEVBOOKS_DIR/daemon.response"
scripts/daemon.sh:534:ci_daemon_ping() { ci_daemon_request "ping" ""; }
scripts/daemon.sh:541:    local hotspot_limit="$2"
scripts/daemon.sh:545:    local items_cached=0
scripts/daemon.sh:546:    local hotspot_cached=0
scripts/daemon.sh:547:    local symbols_cached=0
scripts/daemon.sh:562:    if [ -x "$SCRIPT_DIR/hotspot-analyzer.sh" ]; then
scripts/daemon.sh:563:        local hotspots=""
scripts/daemon.sh:565:            hotspots=$($timeout_cmd "$timeout" "$SCRIPT_DIR/hotspot-analyzer.sh" --top "$hotspot_limit" --format json 2>/dev/null | jq -r '.hotspots[].file // empty' 2>/dev/null) || true
scripts/daemon.sh:567:            hotspots=$("$SCRIPT_DIR/hotspot-analyzer.sh" --top "$hotspot_limit" --format json 2>/dev/null | jq -r '.hotspots[].file // empty' 2>/dev/null) || true
scripts/daemon.sh:570:        if [[ -n "$hotspots" ]]; then
scripts/daemon.sh:575:                if [ -x "$SCRIPT_DIR/cache-manager.sh" ]; then
scripts/daemon.sh:576:                    local cache_key="hotspot:$file"
scripts/daemon.sh:578:                    "$SCRIPT_DIR/cache-manager.sh" cache-set "$cache_key" "warmed" 2>/dev/null && {
scripts/daemon.sh:579:                        hotspot_cached=$((hotspot_cached + 1))
scripts/daemon.sh:580:                        items_cached=$((items_cached + 1))
scripts/daemon.sh:583:            done <<< "$hotspots"
scripts/daemon.sh:588:    if [ -x "$SCRIPT_DIR/graph-store.sh" ] && [ -f "$GRAPH_DB_PATH" ]; then
scripts/daemon.sh:594:            "$SCRIPT_DIR/graph-store.sh" search "$query" 2>/dev/null >/dev/null || true
scripts/daemon.sh:597:            if [ -x "$SCRIPT_DIR/cache-manager.sh" ]; then
scripts/daemon.sh:598:                local cache_key="query:$query"
scripts/daemon.sh:599:                "$SCRIPT_DIR/cache-manager.sh" cache-set "$cache_key" "warmed" 2>/dev/null && {
scripts/daemon.sh:600:                    items_cached=$((items_cached + 1))
scripts/daemon.sh:607:    if [ -x "$SCRIPT_DIR/cache-manager.sh" ]; then
scripts/daemon.sh:610:        stats=$("$SCRIPT_DIR/cache-manager.sh" stats --format json 2>/dev/null) || true
scripts/daemon.sh:612:            symbols_cached=$(echo "$stats" | jq -r '.total_entries // 0' 2>/dev/null) || symbols_cached=0
scripts/daemon.sh:619:    _set_items_cached "$items_cached"
scripts/daemon.sh:623:        echo "{\"warmup_status\":\"completed\",\"items_cached\":$items_cached,\"hotspot_cached\":$hotspot_cached,\"symbols_cached\":$symbols_cached}"
scripts/daemon.sh:625:        echo "Warmup completed: $items_cached items cached ($hotspot_cached hotspots, $symbols_cached symbols)"
scripts/daemon.sh:638:ci_daemon_warmup() {
scripts/daemon.sh:640:    local hotspot_limit="$DAEMON_WARMUP_HOTSPOT_LIMIT"
scripts/daemon.sh:656:            --hotspot-limit)
scripts/daemon.sh:657:                hotspot_limit="$2"
scripts/daemon.sh:680:            echo '{"warmup_status":"disabled","items_cached":0}'
scripts/daemon.sh:700:        _warmup_background "$timeout" "$hotspot_limit" "$queries" "$format" &
scripts/daemon.sh:719:            $timeout_cmd bash -c "_warmup_background '$timeout' '$hotspot_limit' '$queries' '$format'" 2>/dev/null || _warmup_failed "timeout"
scripts/daemon.sh:721:            _warmup_background "$timeout" "$hotspot_limit" "$queries" "$format"
scripts/daemon.sh:733:        start)  ci_daemon_start ;;
scripts/daemon.sh:734:        stop)   ci_daemon_stop ;;
scripts/daemon.sh:735:        status) ci_daemon_status ;;
scripts/daemon.sh:736:        ping)   ci_daemon_ping ;;
scripts/daemon.sh:737:        query)  ci_daemon_request "query" "${1:-}" ;;
scripts/daemon.sh:738:        warmup) ci_daemon_warmup "$@" ;;
scripts/daemon.sh:741:Usage: daemon.sh {start|stop|status|ping|query|warmup} [OPTIONS]
scripts/daemon.sh:744:  start           Start the daemon
scripts/daemon.sh:745:  stop            Stop the daemon
scripts/daemon.sh:746:  status          Show daemon status (including warmup status)
scripts/daemon.sh:749:  warmup          Warm up cache (REQ-DME-001/002/003)
scripts/daemon.sh:754:  --hotspot-limit N Number of hotspot files to cache (default: 10)
scripts/daemon.sh:764:  daemon.sh start
scripts/daemon.sh:765:  daemon.sh warmup --timeout 60 --format json
scripts/daemon.sh:766:  daemon.sh warmup --async
scripts/daemon.sh:767:  daemon.sh status
scripts/cod-visualizer.sh:2:# cod-visualizer.sh - COD (Code Overview Diagram) Êû∂ÊûÑÂèØËßÜÂåñÊ®°Âùó
scripts/cod-visualizer.sh:10:#   GRAPH_DB_PATH - Êï∞ÊçÆÂ∫ìË∑ØÂæÑÔºåÈªòËÆ§ .devbooks/graph.db
scripts/cod-visualizer.sh:20:LOG_PREFIX="cod-visualizer"
scripts/cod-visualizer.sh:26:: "${GRAPH_DB_PATH:=$DEVBOOKS_DIR/graph.db}"
scripts/cod-visualizer.sh:43:# Ê£ÄÊü• cod_visualizer ÂäüËÉΩÊòØÂê¶ÂêØÁî®
scripts/cod-visualizer.sh:53:        BEGIN { in_cod = 0 }
scripts/cod-visualizer.sh:54:        /^[[:space:]]*cod_visualizer:/ { in_cod = 1; next }
scripts/cod-visualizer.sh:55:        /^[[:space:]]*[a-zA-Z_]+:/ && !/^[[:space:]]*enabled:/ { in_cod = 0 }
scripts/cod-visualizer.sh:56:        in_cod && /enabled:/ {
scripts/cod-visualizer.sh:111:_get_hotspot_score() {
scripts/cod-visualizer.sh:113:    local hotspot_script="$SCRIPT_DIR/hotspot-analyzer.sh"
scripts/cod-visualizer.sh:115:    if [[ ! -x "$hotspot_script" ]]; then
scripts/cod-visualizer.sh:122:    local hotspot_data
scripts/cod-visualizer.sh:123:    hotspot_data=$("$hotspot_script" --format json --top 100 2>/dev/null || echo '{"hotspots":[]}')
scripts/cod-visualizer.sh:129:        max_score=$(echo "$hotspot_data" | jq '[.hotspots[].score] | max // 1' 2>/dev/null || echo "1")
scripts/cod-visualizer.sh:130:        file_score=$(echo "$hotspot_data" | jq --arg f "$file_path" '.hotspots[] | select(.file == $f) | .score // 0' 2>/dev/null || echo "0")
scripts/cod-visualizer.sh:144:_get_file_complexity() {
scripts/cod-visualizer.sh:169:_get_hotspot_color() {
scripts/cod-visualizer.sh:184:    local include_hotspots="${1:-false}"
scripts/cod-visualizer.sh:185:    local include_complexity="${2:-false}"
scripts/cod-visualizer.sh:187:    echo "graph TD"
scripts/cod-visualizer.sh:188:    echo "    subgraph System[\"Code Intelligence System\"]"
scripts/cod-visualizer.sh:199:    if [[ "$include_hotspots" == "true" ]]; then
scripts/cod-visualizer.sh:206:    local include_hotspots="${1:-false}"
scripts/cod-visualizer.sh:207:    local include_complexity="${2:-false}"
scripts/cod-visualizer.sh:215:    {"id": "core", "group": "system", "label": "Core Modules", "hotspot": 0.5, "complexity": 10},
scripts/cod-visualizer.sh:216:    {"id": "scripts", "group": "system", "label": "Scripts", "hotspot": 0.7, "complexity": 15},
scripts/cod-visualizer.sh:217:    {"id": "config", "group": "system", "label": "Configuration", "hotspot": 0.2, "complexity": 5},
scripts/cod-visualizer.sh:218:    {"id": "user", "group": "external", "label": "User", "hotspot": 0, "complexity": 0},
scripts/cod-visualizer.sh:219:    {"id": "mcp_client", "group": "external", "label": "MCP Client", "hotspot": 0, "complexity": 0}
scripts/cod-visualizer.sh:240:    local include_hotspots="${1:-false}"
scripts/cod-visualizer.sh:241:    local include_complexity="${2:-false}"
scripts/cod-visualizer.sh:243:    echo "graph TD"
scripts/cod-visualizer.sh:270:            if [[ "$include_complexity" == "true" ]]; then
scripts/cod-visualizer.sh:271:                local complexity
scripts/cod-visualizer.sh:272:                complexity=$(_get_file_complexity "$dir")
scripts/cod-visualizer.sh:273:                label="$dir [$complexity]"
scripts/cod-visualizer.sh:307:            if [[ "$include_complexity" == "true" ]]; then
scripts/cod-visualizer.sh:308:                local complexity
scripts/cod-visualizer.sh:309:                complexity=$(_get_file_complexity "$file_path")
scripts/cod-visualizer.sh:310:                label="$symbol [$complexity]"
scripts/cod-visualizer.sh:340:    if [[ "$include_hotspots" == "true" ]]; then
scripts/cod-visualizer.sh:351:    local include_hotspots="${1:-false}"
scripts/cod-visualizer.sh:352:    local include_complexity="${2:-false}"
scripts/cod-visualizer.sh:372:                local hotspot="0.0"
scripts/cod-visualizer.sh:373:                local complexity="1"
scripts/cod-visualizer.sh:375:                if [[ "$include_hotspots" == "true" ]]; then
scripts/cod-visualizer.sh:376:                    hotspot=$(_get_hotspot_score "$file_path")
scripts/cod-visualizer.sh:379:                if [[ "$include_complexity" == "true" ]]; then
scripts/cod-visualizer.sh:380:                    complexity=$(_get_file_complexity "$file_path")
scripts/cod-visualizer.sh:383:                nodes_arr+=("{\"id\": \"$symbol\", \"group\": \"module\", \"hotspot\": $hotspot, \"complexity\": $complexity}")
scripts/cod-visualizer.sh:389:                nodes_json=$(printf '%s\n' "${nodes_arr[@]}" | paste -sd ',' -)
scripts/cod-visualizer.sh:414:                links_json=$(printf '%s\n' "${links_arr[@]}" | paste -sd ',' -)
scripts/cod-visualizer.sh:435:            local hotspot="0.0"
scripts/cod-visualizer.sh:436:            local complexity="1"
scripts/cod-visualizer.sh:438:            if [[ "$include_hotspots" == "true" ]]; then
scripts/cod-visualizer.sh:439:                hotspot=$(_get_hotspot_score "$dir")
scripts/cod-visualizer.sh:442:            if [[ "$include_complexity" == "true" ]]; then
scripts/cod-visualizer.sh:443:                complexity=$(_get_file_complexity "$dir")
scripts/cod-visualizer.sh:446:            nodes_arr+=("{\"id\": \"$dir\", \"group\": \"module\", \"hotspot\": $hotspot, \"complexity\": $complexity}")
scripts/cod-visualizer.sh:450:        nodes_json=$(printf '%s\n' "${nodes_arr[@]}" | paste -sd ',' -)
scripts/cod-visualizer.sh:464:            links_json=$(printf '%s\n' "${links_arr[@]}" | paste -sd ',' -)
scripts/cod-visualizer.sh:487:    local include_hotspots="${2:-false}"
scripts/cod-visualizer.sh:488:    local include_complexity="${3:-false}"
scripts/cod-visualizer.sh:490:    echo "graph TD"
scripts/cod-visualizer.sh:511:            local subgraph_name
scripts/cod-visualizer.sh:512:            subgraph_name=$(_escape_mermaid_label "$module_path")
scripts/cod-visualizer.sh:513:            echo "    subgraph $subgraph_name"
scripts/cod-visualizer.sh:521:                if [[ "$include_complexity" == "true" ]]; then
scripts/cod-visualizer.sh:522:                    local complexity
scripts/cod-visualizer.sh:523:                    complexity=$(_get_file_complexity "$file")
scripts/cod-visualizer.sh:524:                    label="$label [$complexity]"
scripts/cod-visualizer.sh:536:        local subgraph_name
scripts/cod-visualizer.sh:537:        subgraph_name=$(_escape_mermaid_label "$module_path")
scripts/cod-visualizer.sh:538:        echo "    subgraph $subgraph_name"
scripts/cod-visualizer.sh:546:            if [[ "$include_complexity" == "true" ]]; then
scripts/cod-visualizer.sh:547:                local complexity
scripts/cod-visualizer.sh:548:                complexity=$(_get_file_complexity "$file_path")
scripts/cod-visualizer.sh:549:                label="$symbol [$complexity]"
scripts/cod-visualizer.sh:583:    if [[ "$include_hotspots" == "true" ]]; then
scripts/cod-visualizer.sh:590:                local hotspot
scripts/cod-visualizer.sh:591:                hotspot=$(_get_hotspot_score "$file_path")
scripts/cod-visualizer.sh:593:                color=$(_get_hotspot_color "$hotspot")
scripts/cod-visualizer.sh:606:    local include_hotspots="${2:-false}"
scripts/cod-visualizer.sh:607:    local include_complexity="${3:-false}"
scripts/cod-visualizer.sh:630:                local hotspot="0.0"
scripts/cod-visualizer.sh:631:                local complexity="1"
scripts/cod-visualizer.sh:633:                if [[ "$include_hotspots" == "true" ]]; then
scripts/cod-visualizer.sh:634:                    hotspot=$(_get_hotspot_score "$file_path")
scripts/cod-visualizer.sh:637:                if [[ "$include_complexity" == "true" ]]; then
scripts/cod-visualizer.sh:638:                    complexity=$(_get_file_complexity "$file_path")
scripts/cod-visualizer.sh:641:                nodes_arr+=("{\"id\": \"$file_path\", \"group\": \"$module_path\", \"hotspot\": $hotspot, \"complexity\": $complexity}")
scripts/cod-visualizer.sh:646:                nodes_json=$(printf '%s\n' "${nodes_arr[@]}" | paste -sd ',' -)
scripts/cod-visualizer.sh:673:                links_json=$(printf '%s\n' "${links_arr[@]}" | paste -sd ',' -)
scripts/cod-visualizer.sh:690:                    local hotspot="0.0"
scripts/cod-visualizer.sh:691:                    local complexity="1"
scripts/cod-visualizer.sh:695:                    if [[ "$include_hotspots" == "true" ]]; then
scripts/cod-visualizer.sh:696:                        hotspot=$(_get_hotspot_score "$file")
scripts/cod-visualizer.sh:699:                    if [[ "$include_complexity" == "true" ]]; then
scripts/cod-visualizer.sh:700:                        complexity=$(_get_file_complexity "$file")
scripts/cod-visualizer.sh:703:                    nodes_arr+=("{\"id\": \"$file\", \"group\": \"$module_path\", \"hotspot\": $hotspot, \"complexity\": $complexity}")
scripts/cod-visualizer.sh:708:                    nodes_json=$(printf '%s\n' "${nodes_arr[@]}" | paste -sd ',' -)
scripts/cod-visualizer.sh:735:    local include_hotspots="false"
scripts/cod-visualizer.sh:736:    local include_complexity="false"
scripts/cod-visualizer.sh:749:            --include-hotspots)
scripts/cod-visualizer.sh:750:                include_hotspots="true"
scripts/cod-visualizer.sh:753:            --include-complexity)
scripts/cod-visualizer.sh:754:                include_complexity="true"
scripts/cod-visualizer.sh:788:                result=$(_generate_level1_mermaid "$include_hotspots" "$include_complexity")
scripts/cod-visualizer.sh:790:                result=$(_generate_level1_d3json "$include_hotspots" "$include_complexity")
scripts/cod-visualizer.sh:795:                result=$(_generate_level2_mermaid "$include_hotspots" "$include_complexity")
scripts/cod-visualizer.sh:797:                result=$(_generate_level2_d3json "$include_hotspots" "$include_complexity")
scripts/cod-visualizer.sh:802:                result=$(_generate_level3_mermaid "." "$include_hotspots" "$include_complexity")
scripts/cod-visualizer.sh:804:                result=$(_generate_level3_d3json "." "$include_hotspots" "$include_complexity")
scripts/cod-visualizer.sh:823:    local include_hotspots="false"
scripts/cod-visualizer.sh:824:    local include_complexity="false"
scripts/cod-visualizer.sh:834:            --include-hotspots)
scripts/cod-visualizer.sh:835:                include_hotspots="true"
scripts/cod-visualizer.sh:838:            --include-complexity)
scripts/cod-visualizer.sh:839:                include_complexity="true"
scripts/cod-visualizer.sh:877:        result=$(_generate_level3_mermaid "$module_path" "$include_hotspots" "$include_complexity")
scripts/cod-visualizer.sh:879:        result=$(_generate_level3_d3json "$module_path" "$include_hotspots" "$include_complexity")
scripts/cod-visualizer.sh:895:cod-visualizer.sh - COD Êû∂ÊûÑÂèØËßÜÂåñÁîüÊàêÂô®
scripts/cod-visualizer.sh:898:    cod-visualizer.sh <command> [options]
scripts/cod-visualizer.sh:912:    --include-hotspots      ÂåÖÂê´ÁÉ≠ÁÇπÁùÄËâ≤
scripts/cod-visualizer.sh:913:    --include-complexity    ÂåÖÂê´Â§çÊùÇÂ∫¶Ê†áÊ≥®
scripts/cod-visualizer.sh:918:    --include-hotspots      ÂåÖÂê´ÁÉ≠ÁÇπÁùÄËâ≤
scripts/cod-visualizer.sh:919:    --include-complexity    ÂåÖÂê´Â§çÊùÇÂ∫¶Ê†áÊ≥®
scripts/cod-visualizer.sh:929:    cod-visualizer.sh generate --level 2 --format mermaid
scripts/cod-visualizer.sh:932:    cod-visualizer.sh generate --level 2 --format d3json --include-hotspots
scripts/cod-visualizer.sh:935:    cod-visualizer.sh module scripts/ --format d3json
scripts/cod-visualizer.sh:938:    cod-visualizer.sh generate --level 2 --format mermaid --output arch.mmd
scripts/cod-visualizer.sh:941:    GRAPH_DB_PATH       ÂõæÊï∞ÊçÆÂ∫ìË∑ØÂæÑÔºàÈªòËÆ§: .devbooks/graph.dbÔºâ
tests/mcp-contract.bats:2:# mcp-contract.bats - MCP Tool Contract Tests
tests/mcp-contract.bats:6:# Run: bats tests/mcp-contract.bats
tests/mcp-contract.bats:63:# ci_hotspot Contract Tests (CT-001, CT-002)
tests/mcp-contract.bats:66:@test "CT-001: ci_hotspot tool registered" {
tests/mcp-contract.bats:67:    run grep -l "ci_hotspot" "$SERVER_TS"
tests/mcp-contract.bats:71:@test "CT-001b: ci_hotspot input parameter - path optional" {
tests/mcp-contract.bats:72:    run grep -A 20 "ci_hotspot" "$SERVER_TS"
tests/mcp-contract.bats:73:    [[ "$output" == *"path"* ]] || skip "ci_hotspot not yet implemented"
tests/mcp-contract.bats:76:@test "CT-001c: ci_hotspot input parameter - top_n optional" {
tests/mcp-contract.bats:77:    run grep -A 20 "ci_hotspot" "$SERVER_TS"
tests/mcp-contract.bats:78:    [[ "$output" == *"top"* ]] || [[ "$output" == *"n"* ]] || skip "ci_hotspot not yet implemented"
tests/mcp-contract.bats:81:@test "CT-002: ci_hotspot output format - schema_version" {
tests/mcp-contract.bats:82:    HOTSPOT_ANALYZER="./scripts/hotspot-analyzer.sh"
tests/mcp-contract.bats:84:        skip "hotspot-analyzer.sh not yet implemented"
tests/mcp-contract.bats:90:@test "CT-002b: ci_hotspot output format - hotspots array" {
tests/mcp-contract.bats:91:    HOTSPOT_ANALYZER="./scripts/hotspot-analyzer.sh"
tests/mcp-contract.bats:93:        skip "hotspot-analyzer.sh not yet implemented"
tests/mcp-contract.bats:96:    [[ "$output" == *"hotspots"* ]]
tests/mcp-contract.bats:100:# ci_boundary Contract Tests (CT-003, CT-004)
tests/mcp-contract.bats:103:@test "CT-003: ci_boundary tool registered" {
tests/mcp-contract.bats:104:    run grep -l "ci_boundary" "$SERVER_TS"
tests/mcp-contract.bats:108:@test "CT-003b: ci_boundary input parameter - path required" {
tests/mcp-contract.bats:109:    run grep -A 20 "ci_boundary" "$SERVER_TS"
tests/mcp-contract.bats:110:    [[ "$output" == *"path"* ]] || skip "ci_boundary not yet implemented"
tests/mcp-contract.bats:113:@test "CT-004: ci_boundary output format - type field" {
tests/mcp-contract.bats:114:    BOUNDARY_DETECTOR="./scripts/boundary-detector.sh"
tests/mcp-contract.bats:116:        skip "boundary-detector.sh not yet implemented"
tests/mcp-contract.bats:122:@test "CT-004b: ci_boundary output format - confidence field" {
tests/mcp-contract.bats:123:    BOUNDARY_DETECTOR="./scripts/boundary-detector.sh"
tests/mcp-contract.bats:125:        skip "boundary-detector.sh not yet implemented"
tests/mcp-contract.bats:145:@test "CT-005c: ci_bug_locate tool still available" {
tests/mcp-contract.bats:146:    run grep "ci_bug_locate" "$SERVER_TS"
tests/mcp-contract.bats:150:@test "CT-005d: ci_complexity tool still available" {
tests/mcp-contract.bats:151:    run grep "ci_complexity" "$SERVER_TS"
tests/mcp-contract.bats:155:@test "CT-005e: ci_graph_rag tool still available" {
tests/mcp-contract.bats:156:    run grep "ci_graph_rag" "$SERVER_TS"
tests/mcp-contract.bats:176:@test "CT-006b: enhanced_hotspot toggle defined" {
tests/mcp-contract.bats:179:    run grep "enhanced_hotspot" "$CONFIG_FILE"
tests/mcp-contract.bats:180:    [ "$status" -eq 0 ] || skip "enhanced_hotspot flag not yet defined"
tests/mcp-contract.bats:187:@test "CT-CLI-001: call-chain.sh --trace-data-flow parameter" {
tests/mcp-contract.bats:188:    CALL_CHAIN="./scripts/call-chain.sh"
tests/mcp-contract.bats:189:    [ -x "$CALL_CHAIN" ] || skip "call-chain.sh not executable"
tests/mcp-contract.bats:195:@test "CT-CLI-002: call-chain.sh compatible without new parameters" {
tests/mcp-contract.bats:196:    CALL_CHAIN="./scripts/call-chain.sh"
tests/mcp-contract.bats:197:    [ -x "$CALL_CHAIN" ] || skip "call-chain.sh not executable"
tests/mcp-contract.bats:241:@test "CT-CKB-001: ci_graph_rag works when CKB_ENABLED=false" {
tests/mcp-contract.bats:242:    GRAPH_RAG="./scripts/graph-rag.sh"
tests/mcp-contract.bats:243:    [ -x "$GRAPH_RAG" ] || skip "graph-rag.sh not executable"
tests/mcp-contract.bats:253:    skip_if_not_ready "$status" "$json_output" "graph-rag no CKB"
tests/mcp-contract.bats:260:@test "CT-CKB-002: ci_graph_rag returns valid results without CKB" {
tests/mcp-contract.bats:261:    GRAPH_RAG="./scripts/graph-rag.sh"
tests/mcp-contract.bats:262:    [ -x "$GRAPH_RAG" ] || skip "graph-rag.sh not executable"
tests/mcp-contract.bats:273:    skip_if_not_ready "$status" "$json_output" "graph-rag results"
tests/mcp-contract.bats:280:@test "CT-CKB-003: ci_graph_rag uses local graph when CKB unavailable" {
tests/mcp-contract.bats:281:    GRAPH_RAG="./scripts/graph-rag.sh"
tests/mcp-contract.bats:282:    [ -x "$GRAPH_RAG" ] || skip "graph-rag.sh not executable"
tests/mcp-contract.bats:292:    skip_if_not_ready "$status" "$json_output" "graph-rag local graph"
tests/mcp-contract.bats:300:    CALL_CHAIN="./scripts/call-chain.sh"
tests/mcp-contract.bats:301:    [ -x "$CALL_CHAIN" ] || skip "call-chain.sh not executable"
tests/mcp-contract.bats:307:    skip_if_not_ready "$status" "$output" "call-chain no CKB"
tests/mcp-contract.bats:312:    GRAPH_RAG="./scripts/graph-rag.sh"
tests/mcp-contract.bats:313:    [ -x "$GRAPH_RAG" ] || skip "graph-rag.sh not executable"
tests/mcp-contract.bats:319:    skip_if_not_ready "$status" "$output" "graph-rag degradation"
scripts/graph-store.sh:2:# graph-store.sh - SQLite ÂõæÂ≠òÂÇ®ÁÆ°ÁêÜËÑöÊú¨
scripts/graph-store.sh:10:#   GRAPH_DB_PATH - Êï∞ÊçÆÂ∫ìË∑ØÂæÑÔºåÈªòËÆ§ .devbooks/graph.db
scripts/graph-store.sh:21:LOG_PREFIX="graph-store"
scripts/graph-store.sh:27:: "${GRAPH_DB_PATH:=$DEVBOOKS_DIR/graph.db}"
scripts/graph-store.sh:157:    contract_type TEXT NOT NULL CHECK(contract_type IN ('proto', 'openapi', 'graphql', 'typescript')),
scripts/graph-store.sh:162:    contract_bonus REAL DEFAULT 0.0,
scripts/graph-store.sh:170:CREATE INDEX IF NOT EXISTS idx_virtual_edges_contract ON virtual_edges(contract_type);
scripts/graph-store.sh:211:    log_info "Initializing graph database at $GRAPH_DB_PATH"
scripts/graph-store.sh:606:    has_version_table=$(sqlite3 "$db_path" "SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='schema_version';" 2>/dev/null || echo "0")
scripts/graph-store.sh:629:    create_sql=$(sqlite3 "$db_path" "SELECT sql FROM sqlite_master WHERE type='table' AND name='edges';" 2>/dev/null)
scripts/graph-store.sh:1055:graph-store.sh - SQLite ÂõæÂ≠òÂÇ®ÁÆ°ÁêÜ
scripts/graph-store.sh:1058:    graph-store.sh <command> [options]
scripts/graph-store.sh:1114:    GRAPH_DB_PATH           Êï∞ÊçÆÂ∫ìË∑ØÂæÑÔºàÈªòËÆ§: .devbooks/graph.dbÔºâ
scripts/graph-store.sh:1119:    graph-store.sh init
scripts/graph-store.sh:1122:    graph-store.sh add-node --id "sym:func:main" --symbol "main" --kind "function" --file "src/index.ts"
scripts/graph-store.sh:1125:    graph-store.sh add-edge --source "sym:func:main" --target "sym:func:helper" --type CALLS
scripts/graph-store.sh:1128:    graph-store.sh query-edges --from "sym:func:main" --type CALLS
scripts/graph-store.sh:1131:    graph-store.sh find-orphans
scripts/graph-store.sh:1134:    graph-store.sh stats
scripts/vendor-proto.sh:4:# Áî®ÈÄî: ÁÆ°ÁêÜ vendored/scip.proto ÁöÑÁâàÊú¨ÂíåÂÖºÂÆπÊÄßÊ£ÄÊü•
scripts/vendor-proto.sh:10:#   --check      Ê£ÄÊü• vendored proto ÁâàÊú¨‰∏é scip-typescript ÂÖºÂÆπÊÄß
scripts/vendor-proto.sh:11:#   --upgrade    ‰ªé GitHub ‰∏ãËΩΩÊúÄÊñ∞ proto Âπ∂Êõ¥Êñ∞ vendored/scip.proto
scripts/vendor-proto.sh:26:VENDORED_PROTO="$PROJECT_ROOT/vendored/scip.proto"
scripts/vendor-proto.sh:27:SCIP_PROTO_URL="https://raw.githubusercontent.com/sourcegraph/scip/main/scip.proto"
scripts/vendor-proto.sh:28:TEMP_PROTO="/tmp/scip-upgrade-$$.proto"
scripts/vendor-proto.sh:55:# Ê£ÄÊü• scip-typescript ÁâàÊú¨
scripts/vendor-proto.sh:56:get_scip_typescript_version() {
scripts/vendor-proto.sh:57:    if command -v scip-typescript &>/dev/null; then
scripts/vendor-proto.sh:58:        scip-typescript --version 2>/dev/null | head -1 | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown"
scripts/vendor-proto.sh:59:    elif [[ -f "$PROJECT_ROOT/node_modules/.bin/scip-typescript" ]]; then
scripts/vendor-proto.sh:60:        "$PROJECT_ROOT/node_modules/.bin/scip-typescript" --version 2>/dev/null | head -1 | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown"
scripts/vendor-proto.sh:70:    local scip_version="$2"
scripts/vendor-proto.sh:73:    if [[ "$proto_version" == "unknown" || "$scip_version" == "unknown" || "$scip_version" == "not-installed" ]]; then
scripts/vendor-proto.sh:78:    local proto_major scip_major
scripts/vendor-proto.sh:80:    scip_major=$(echo "$scip_version" | cut -d'.' -f1)
scripts/vendor-proto.sh:83:    if [[ "$proto_major" == "$scip_major" ]]; then
scripts/vendor-proto.sh:98:            echo '{"status":"error","message":"vendored/scip.proto not found","compatible":false}'
scripts/vendor-proto.sh:100:            log_error "vendored/scip.proto ‰∏çÂ≠òÂú®"
scripts/vendor-proto.sh:107:    local proto_version scip_version
scripts/vendor-proto.sh:109:    scip_version=$(get_scip_typescript_version)
scripts/vendor-proto.sh:115:    if check_version_compatibility "$proto_version" "$scip_version"; then
scripts/vendor-proto.sh:126:            --arg scip_version "$scip_version" \
scripts/vendor-proto.sh:132:                scip_typescript_version: $scip_version,
scripts/vendor-proto.sh:134:                proto_path: "vendored/scip.proto"
scripts/vendor-proto.sh:140:        echo "  scip-typescript ÁâàÊú¨: $scip_version"
scripts/vendor-proto.sh:142:        echo "  Ë∑ØÂæÑ: vendored/scip.proto"
scripts/vendor-proto.sh:204:        echo "// Compatible with: scip-typescript 0.x"
scripts/vendor-proto.sh:214:    log_ok "Â∑≤Êõ¥Êñ∞ vendored/scip.proto"
scripts/vendor-proto.sh:227:            echo '{"version":"not-found","path":"vendored/scip.proto"}'
scripts/vendor-proto.sh:229:            log_error "vendored/scip.proto ‰∏çÂ≠òÂú®"
scripts/vendor-proto.sh:238:        jq -n --arg version "$version" '{"version": $version, "path": "vendored/scip.proto"}'
scripts/vendor-proto.sh:257:    Ê≠§ËÑöÊú¨Áî®‰∫éÁÆ°ÁêÜ vendored/scip.proto Êñá‰ª∂ÔºåÁ°Æ‰øùÁ¶ªÁ∫øÁéØÂ¢É‰∏ã
scripts/vendor-proto.sh:261:    1. vendored/scip.proto ÊòØÂê¶Â≠òÂú®
scripts/vendor-proto.sh:262:    2. ÁâàÊú¨ÊòØÂê¶‰∏é scip-typescript ÂÖºÂÆπ
scripts/vendor-proto.sh:267:    3. Êõ¥Êñ∞ vendored/scip.proto
src/server.ts:5: * A thin MCP shell that delegates to shell scripts for code intelligence capabilities.
src/server.ts:41:    description: "Semantic code search using embeddings or keywords",
src/server.ts:49:          enum: ["semantic", "keyword"],
src/server.ts:50:          description: "Search mode (default: semantic)",
src/server.ts:74:    name: "ci_bug_locate",
src/server.ts:75:    description: "Locate potential bug locations based on error description",
src/server.ts:85:    name: "ci_complexity",
src/server.ts:86:    description: "Analyze code complexity metrics",
src/server.ts:101:    name: "ci_graph_rag",
src/server.ts:115:    description: "Check or manage Embedding index status (Note: This tool manages Embedding index, not SCIP index. For SCIP index, use ci_ast_delta)",
src/server.ts:122:          description: "Action to perform: status (check index status), build (rebuild embedding index), clear (clean embedding cache). Default: status",
src/server.ts:128:    name: "ci_hotspot",
src/server.ts:129:    description: "Analyze code hotspots based on change frequency and complexity",
src/server.ts:133:        top: { type: "number", description: "Number of top hotspots to return (default: 20)" },
src/server.ts:145:    name: "ci_boundary",
src/server.ts:146:    description: "Detect code boundary type (user/library/generated/vendor)",
src/server.ts:177:    name: "ci_federation",
src/server.ts:178:    description: "Cross-repo API contract tracking, symbol search, and virtual edges",
src/server.ts:200:    name: "ci_graph_store",
src/server.ts:223:    name: "ci_ast_delta",
src/server.ts:230:          enum: ["update", "batch", "status", "clear-cache"],
src/server.ts:231:          description: "Action to perform: update (single file), batch (multiple files), status, clear-cache",
src/server.ts:240:    name: "ci_impact",
src/server.ts:245:        symbol: { type: "string", description: "Symbol to analyze impact for" },
src/server.ts:248:        decay: { type: "number", description: "Decay factor for impact calculation (default: 0.8)" },
src/server.ts:249:        threshold: { type: "number", description: "Minimum impact threshold (default: 0.1)" },
src/server.ts:260:    name: "ci_cod",
src/server.ts:268:          description: "Action: generate (full codebase), module (specific module)",
src/server.ts:281:        include_hotspots: { type: "boolean", description: "Include hotspot coloring (default: true)" },
src/server.ts:282:        include_complexity: { type: "boolean", description: "Include complexity annotations (default: false)" },
src/server.ts:288:    name: "ci_intent",
src/server.ts:317:    name: "ci_vuln",
src/server.ts:325:          description: "Action: scan (vulnerability scan), trace (dependency trace)",
src/server.ts:349:  // Use execFile instead of exec to avoid shell injection vulnerabilities
src/server.ts:376:      const mode = (args.mode as string) || "semantic";
src/server.ts:377:      const { stdout, stderr } = await runScript("embedding.sh", [
src/server.ts:392:      const { stdout, stderr } = await runScript("call-chain.sh", [
src/server.ts:402:    case "ci_bug_locate": {
src/server.ts:404:      const { stdout, stderr } = await runScript("bug-locator.sh", [
src/server.ts:411:    case "ci_complexity": {
src/server.ts:414:      const { stdout, stderr } = await runScript("complexity.sh", [
src/server.ts:422:    case "ci_graph_rag": {
src/server.ts:426:      const { stdout, stderr } = await runScript("graph-rag.sh", [
src/server.ts:437:    // ci_index_status now routes to embedding.sh for Embedding index management
src/server.ts:451:      // "clear" maps to "clean" in embedding.sh
src/server.ts:452:      const embeddingAction = action === "clear" ? "clean" : action;
src/server.ts:454:      // Route to embedding.sh (AC-005: MP6.1)
src/server.ts:455:      const { stdout, stderr } = await runScript("embedding.sh", [embeddingAction]);
src/server.ts:459:    case "ci_hotspot": {
src/server.ts:464:      const { stdout, stderr } = await runScript("hotspot-analyzer.sh", [
src/server.ts:477:    case "ci_boundary": {
src/server.ts:480:      const { stdout, stderr } = await runScript("boundary-detector.sh", [
src/server.ts:496:      const { stdout, stderr } = await runScript("dependency-guard.sh", scriptArgs);
src/server.ts:500:    case "ci_federation": {
src/server.ts:544:      const { stdout, stderr } = await runScript("federation-lite.sh", scriptArgs);
src/server.ts:548:    case "ci_graph_store": {
src/server.ts:569:      const { stdout, stderr } = await runScript("graph-store.sh", scriptArgs);
src/server.ts:574:    case "ci_ast_delta": {
src/server.ts:596:        case "clear-cache":
src/server.ts:597:          scriptArgs.push("clear-cache");
src/server.ts:602:      const { stdout, stderr } = await runScript("ast-delta.sh", scriptArgs);
src/server.ts:607:    case "ci_impact": {
src/server.ts:631:      const { stdout, stderr } = await runScript("impact-analyzer.sh", scriptArgs);
src/server.ts:636:    case "ci_cod": {
src/server.ts:641:      const includeHotspots = args.include_hotspots !== false; // ÈªòËÆ§ true
src/server.ts:642:      const includeComplexity = args.include_complexity === true; // ÈªòËÆ§ false
src/server.ts:652:        scriptArgs.push("--include-hotspots");
src/server.ts:655:        scriptArgs.push("--include-complexity");
src/server.ts:657:      const { stdout, stderr } = await runScript("cod-visualizer.sh", scriptArgs);
src/server.ts:662:    case "ci_intent": {
src/server.ts:704:      const { stdout, stderr } = await runScript("intent-learner.sh", scriptArgs);
src/server.ts:709:    case "ci_vuln": {
src/server.ts:735:      const { stdout, stderr } = await runScript("vuln-tracker.sh", scriptArgs);
src/server.ts:748:    console.log(`code-intelligence-mcp v${VERSION}`);
src/server.ts:756:  code-intelligence-mcp [options]
src/server.ts:763:  MCP Server providing code intelligence capabilities:
src/server.ts:764:  - Semantic code search (embeddings)
src/server.ts:769:  - Hotspot analysis (frequency x complexity)
src/server.ts:770:  - Code boundary detection
src/server.ts:772:  - Transitive impact analysis
src/server.ts:775:  - Security vulnerability tracking
src/server.ts:776:  - Cross-repo federation with virtual edges
src/server.ts:778:For more information, see: https://github.com/user/code-intelligence-mcp
src/server.ts:785:      name: "code-intelligence-mcp",
scripts/test-embedding.sh:8:EMBEDDING_SCRIPT="$PROJECT_ROOT/tools/devbooks-embedding.sh"
scripts/test-embedding.sh:55:check_dependency() {
scripts/test-embedding.sh:67:check_dependency "curl"
scripts/test-embedding.sh:68:check_dependency "jq"
scripts/test-embedding.sh:69:check_dependency "bc"
scripts/test-embedding.sh:70:check_dependency "md5sum" || check_dependency "md5"
scripts/test-embedding.sh:81:run_test "Ê£ÄÊü•ÈÖçÁΩÆÊñá‰ª∂" "[ -f '$PROJECT_ROOT/.devbooks/embedding.yaml' ]"
scripts/test-embedding.sh:104:  TEST_DIR="$PROJECT_ROOT/.devbooks/embeddings-test"
scripts/test-embedding.sh:152:HOOK_SCRIPT="$PROJECT_ROOT/.claude/hooks/augment-context-with-embedding.sh"
scripts/test-embedding.sh:159:  TEST_INPUT='{"prompt": "‰øÆÂ§çÁî®Êà∑ÁôªÂΩïÁöÑ bug"}'
scripts/federation-lite.sh:2:# federation-lite.sh - Lightweight Federation Index (Cross-repo Contract Tracking)
scripts/federation-lite.sh:5:# Purpose: Discover and index API contracts across multiple repositories
scripts/federation-lite.sh:9:#   federation-lite.sh --status
scripts/federation-lite.sh:10:#   federation-lite.sh --update [--config config/federation.yaml]
scripts/federation-lite.sh:11:#   federation-lite.sh --search "<symbol>" [--format json]
scripts/federation-lite.sh:12:#   federation-lite.sh --list-contracts [--repo "<name>"]
scripts/federation-lite.sh:13:#   federation-lite.sh --help
scripts/federation-lite.sh:16:#   FEDERATION_CONFIG   - Config file path (default: config/federation.yaml)
scripts/federation-lite.sh:17:#   FEDERATION_INDEX    - Index file path (default: .devbooks/federation-index.json)
scripts/federation-lite.sh:18:#   DEBUG               - Enable debug output (default: false)
scripts/federation-lite.sh:25:# ==================== CT-VE-005: Fast Path for generate-virtual-edges ====================
scripts/federation-lite.sh:29:  _fast_local_repo="."
scripts/federation-lite.sh:30:  _fast_db_path=""
scripts/federation-lite.sh:31:  _fast_min_confidence="0.5"
scripts/federation-lite.sh:32:  _fast_config=""
scripts/federation-lite.sh:33:  _fast_index=""
scripts/federation-lite.sh:34:  _fast_sync_mode="false"
scripts/federation-lite.sh:39:      --local-repo) _fast_local_repo="$2"; shift 2 ;;
scripts/federation-lite.sh:40:      --db) _fast_db_path="$2"; shift 2 ;;
scripts/federation-lite.sh:41:      --min-confidence) _fast_min_confidence="$2"; shift 2 ;;
scripts/federation-lite.sh:42:      --config) _fast_config="$2"; shift 2 ;;
scripts/federation-lite.sh:43:      --sync) _fast_sync_mode="true"; shift ;;
scripts/federation-lite.sh:49:  _fast_local_repo=$(cd "$_fast_local_repo" && pwd)
scripts/federation-lite.sh:50:  [[ -z "$_fast_db_path" ]] && _fast_db_path="$_fast_local_repo/.devbooks/graph.db"
scripts/federation-lite.sh:51:  [[ -z "$_fast_index" ]] && _fast_index="${FEDERATION_INDEX:-$_fast_local_repo/.devbooks/federation-index.json}"
scripts/federation-lite.sh:53:  # Check federation index exists
scripts/federation-lite.sh:54:  if [[ ! -f "$_fast_index" ]]; then
scripts/federation-lite.sh:55:    echo "{\"error\": \"Federation index not found at $_fast_index\"}" >&2
scripts/federation-lite.sh:60:  mkdir -p "$(dirname "$_fast_db_path")" 2>/dev/null || true
scripts/federation-lite.sh:63:  sqlite3 "$_fast_db_path" 'CREATE TABLE IF NOT EXISTS virtual_edges (
scripts/federation-lite.sh:72:    contract_type TEXT DEFAULT '"'"'unknown'"'"',
scripts/federation-lite.sh:73:    contract_bonus REAL DEFAULT 0.0,
scripts/federation-lite.sh:82:  # Fast symbol extraction using grep + single jq transform
scripts/federation-lite.sh:83:  _fast_local_repo_name=$(basename "$_fast_local_repo")
scripts/federation-lite.sh:84:  _fast_symbols=$(grep -rhoE '(export\s+)?(async\s+)?function\s+[A-Za-z_][A-Za-z0-9_]*' "$_fast_local_repo" \
scripts/federation-lite.sh:90:  _fast_result=$(jq -c \
scripts/federation-lite.sh:91:    --argjson local_symbols "$_fast_symbols" \
scripts/federation-lite.sh:92:    --arg local_repo "$_fast_local_repo_name" \
scripts/federation-lite.sh:93:    --argjson min_conf "$_fast_min_confidence" \
scripts/federation-lite.sh:116:      .contracts[]? |
scripts/federation-lite.sh:117:      .type as $contract_type |
scripts/federation-lite.sh:123:        contract_type: $contract_type,
scripts/federation-lite.sh:124:        contract_bonus: (if $contract_type == "proto" then 0.2 elif $contract_type == "openapi" then 0.15 else 0.1 end)
scripts/federation-lite.sh:152:      # Calculate confidence: exact*0.6 + sig*0.3 + contract*0.1
scripts/federation-lite.sh:153:      (($exact_match * 0.6) + ($sig_sim * 0.3) + ($remote.contract_bonus * 0.1)) as $confidence |
scripts/federation-lite.sh:164:        contract_type: $remote.contract_type,
scripts/federation-lite.sh:169:        contract_bonus: $remote.contract_bonus
scripts/federation-lite.sh:172:    ' "$_fast_index")
scripts/federation-lite.sh:175:  _fast_edge_count=$(echo "$_fast_result" | jq 'length')
scripts/federation-lite.sh:176:  if [[ "$_fast_edge_count" -gt 0 ]]; then
scripts/federation-lite.sh:177:    _fast_sql=$(echo "$_fast_result" | jq -r '
scripts/federation-lite.sh:179:      (map("INSERT OR REPLACE INTO virtual_edges (source_repo, source_symbol, target_repo, target_symbol, edge_type, contract_type, confidence, confidence_level, exact_match, signature_similarity, contract_bonus) VALUES (" +
scripts/federation-lite.sh:185:        "'"'"'\(.contract_type)'"'"', " +
scripts/federation-lite.sh:190:        "\(.contract_bonus));") | join("\n")) +
scripts/federation-lite.sh:193:    echo "$_fast_sql" | sqlite3 "$_fast_db_path" 2>/dev/null || true
scripts/federation-lite.sh:197:  _fast_total=$(sqlite3 "$_fast_db_path" "SELECT COUNT(*) FROM virtual_edges" 2>/dev/null || echo "0")
scripts/federation-lite.sh:198:  jq -n --argjson created "$_fast_edge_count" --argjson total "$_fast_total" --arg db "$_fast_db_path" \
scripts/federation-lite.sh:203:# ==================== End Fast Path ====================
scripts/federation-lite.sh:217:: "${FEDERATION_CONFIG:=config/federation.yaml}"
scripts/federation-lite.sh:218:: "${FEDERATION_INDEX:=.devbooks/federation-index.json}"
scripts/federation-lite.sh:225:log_debug() {
scripts/federation-lite.sh:245:federation-lite.sh - Lightweight Federation Index (Cross-repo Contract Tracking)
scripts/federation-lite.sh:248:  federation-lite.sh --status
scripts/federation-lite.sh:249:  federation-lite.sh --update [--config config/federation.yaml]
scripts/federation-lite.sh:250:  federation-lite.sh --search "<symbol>" [--format json]
scripts/federation-lite.sh:251:  federation-lite.sh --list-contracts [--repo "<name>"]
scripts/federation-lite.sh:252:  federation-lite.sh generate-virtual-edges [--repo <name>] [--min-confidence <n>]
scripts/federation-lite.sh:253:  federation-lite.sh query-virtual <symbol> [--virtual-edges] [--confidence <n>]
scripts/federation-lite.sh:254:  federation-lite.sh --help
scripts/federation-lite.sh:258:  --update            Update the federation index
scripts/federation-lite.sh:259:  --config <file>     Configuration file (default: config/federation.yaml)
scripts/federation-lite.sh:261:  --list-contracts    List all indexed contracts
scripts/federation-lite.sh:264:  --debug             Enable debug output
scripts/federation-lite.sh:268:  generate-virtual-edges    Generate virtual edges from local calls to remote contracts
scripts/federation-lite.sh:270:    --db <path>             Path to graph.db (default: .devbooks/graph.db)
scripts/federation-lite.sh:279:  FEDERATION_CONFIG   Config file path (default: config/federation.yaml)
scripts/federation-lite.sh:280:  FEDERATION_INDEX    Index file path (default: .devbooks/federation-index.json)
scripts/federation-lite.sh:281:  GRAPH_DB            Graph database path (default: .devbooks/graph.db)
scripts/federation-lite.sh:284:  - Protocol Buffers (.proto)   - contract_bonus: 0.1
scripts/federation-lite.sh:285:  - OpenAPI (openapi.yaml)      - contract_bonus: 0.05
scripts/federation-lite.sh:286:  - GraphQL (.graphql)          - contract_bonus: 0.08
scripts/federation-lite.sh:287:  - TypeScript Types (.d.ts)    - contract_bonus: 0.0
scripts/federation-lite.sh:290:  confidence = exact_match * 0.6 + signature_similarity * 0.3 + contract_bonus * 0.1
scripts/federation-lite.sh:294:  federation-lite.sh --status
scripts/federation-lite.sh:297:  federation-lite.sh --update
scripts/federation-lite.sh:300:  federation-lite.sh --search "UserService"
scripts/federation-lite.sh:302:  # List all contracts
scripts/federation-lite.sh:303:  federation-lite.sh --list-contracts
scripts/federation-lite.sh:306:  federation-lite.sh generate-virtual-edges --local-repo ./my-app --min-confidence 0.5
scripts/federation-lite.sh:309:  federation-lite.sh query-virtual "getUserById" --confidence 0.5
scripts/federation-lite.sh:317:# Parse simple YAML config (no external dependency)
scripts/federation-lite.sh:319:load_federation_config() {
scripts/federation-lite.sh:329:    local auto_discover='{"enabled": false, "search_paths": [], "contract_patterns": []}'
scripts/federation-lite.sh:333:    local in_contracts=false
scripts/federation-lite.sh:334:    local contracts="[]"
scripts/federation-lite.sh:336:    local in_contract_patterns=false
scripts/federation-lite.sh:338:    local contract_patterns="[]"
scripts/federation-lite.sh:339:    local federation_root_seen=false
scripts/federation-lite.sh:340:    local in_federation=true
scripts/federation-lite.sh:348:        if [[ "$line" =~ ^[[:space:]]*federation:[[:space:]]*$ ]]; then
scripts/federation-lite.sh:349:            federation_root_seen=true
scripts/federation-lite.sh:350:            in_federation=true
scripts/federation-lite.sh:356:        if [[ "$federation_root_seen" == "true" && "$line" =~ ^[^[:space:]] ]]; then
scripts/federation-lite.sh:357:            if [[ ! "$line" =~ ^federation: ]]; then
scripts/federation-lite.sh:358:                in_federation=false
scripts/federation-lite.sh:363:            if [[ "$federation_root_seen" == "true" && "$in_federation" != "true" ]]; then
scripts/federation-lite.sh:372:            if [[ "$federation_root_seen" == "true" && "$in_federation" != "true" ]]; then
scripts/federation-lite.sh:385:                    current_repo=$(echo "$current_repo" | jq --argjson c "$contracts" '.contracts = $c')
scripts/federation-lite.sh:389:                contracts="[]"
scripts/federation-lite.sh:390:                in_contracts=false
scripts/federation-lite.sh:398:            if [[ "$line" =~ ^[[:space:]]+contracts:[[:space:]]*$ ]]; then
scripts/federation-lite.sh:399:                in_contracts=true
scripts/federation-lite.sh:403:            if [[ "$in_contracts" == "true" && "$line" =~ ^[[:space:]]+-[[:space:]]*\"?([^\"]+)\"?$ ]]; then
scripts/federation-lite.sh:404:                contracts=$(echo "$contracts" | jq --arg c "${BASH_REMATCH[1]}" '. + [$c]')
scripts/federation-lite.sh:415:                in_contract_patterns=false
scripts/federation-lite.sh:419:            if [[ "$line" =~ ^[[:space:]]+contract_patterns:[[:space:]]*$ ]]; then
scripts/federation-lite.sh:420:                in_contract_patterns=true
scripts/federation-lite.sh:429:            if [[ "$in_contract_patterns" == "true" && "$line" =~ ^[[:space:]]+-[[:space:]]*\"?([^\"]+)\"?$ ]]; then
scripts/federation-lite.sh:430:                contract_patterns=$(echo "$contract_patterns" | jq --arg p "${BASH_REMATCH[1]}" '. + [$p]')
scripts/federation-lite.sh:435:    # Save last repo
scripts/federation-lite.sh:437:        current_repo=$(echo "$current_repo" | jq --argjson c "$contracts" '.contracts = $c')
scripts/federation-lite.sh:442:    auto_discover=$(echo "$auto_discover" | jq --argjson sp "$search_paths" --argjson cp "$contract_patterns" \
scripts/federation-lite.sh:443:        '.search_paths = $sp | .contract_patterns = $cp')
scripts/federation-lite.sh:465:# Detect contract type from filename
scripts/federation-lite.sh:466:# REQ-FED-001: Cross-repo contract discovery
scripts/federation-lite.sh:467:detect_contract_type() {
scripts/federation-lite.sh:479:        *.graphql|*.gql)
scripts/federation-lite.sh:480:            echo "graphql"
scripts/federation-lite.sh:593:extract_graphql_symbols() {
scripts/federation-lite.sh:627:# Extract symbols from any contract file
scripts/federation-lite.sh:639:        graphql)
scripts/federation-lite.sh:640:            extract_graphql_symbols "$file"
scripts/federation-lite.sh:655:# Update federation index
scripts/federation-lite.sh:660:    log_info "Loading federation config: $config_file"
scripts/federation-lite.sh:662:    config=$(load_federation_config "$config_file")
scripts/federation-lite.sh:674:        local repo_name repo_path repo_contracts
scripts/federation-lite.sh:677:        repo_contracts=$(echo "$config" | jq -r ".repositories[$i].contracts")
scripts/federation-lite.sh:688:        local contracts="[]"
scripts/federation-lite.sh:690:        # Find contract files using patterns
scripts/federation-lite.sh:697:                local contract_type
scripts/federation-lite.sh:698:                contract_type=$(detect_contract_type "$file")
scripts/federation-lite.sh:699:                [[ "$contract_type" == "unknown" ]] && continue
scripts/federation-lite.sh:702:                symbols=$(extract_symbols "$file" "$contract_type")
scripts/federation-lite.sh:708:                contracts=$(echo "$contracts" | jq \
scripts/federation-lite.sh:710:                    --arg type "$contract_type" \
scripts/federation-lite.sh:715:                log_debug "  Indexed: $rel_path ($contract_type, $(echo "$symbols" | jq 'length') symbols)"
scripts/federation-lite.sh:716:            done < <(find "$repo_path" -type f \( -name "*.proto" -o -name "openapi.yaml" -o -name "openapi.yml" -o -name "swagger.json" -o -name "*.graphql" -o -name "*.d.ts" \) 2>/dev/null)
scripts/federation-lite.sh:717:        done < <(echo "$repo_contracts" | jq -r '.[]' 2>/dev/null)
scripts/federation-lite.sh:720:        if [[ "$repo_contracts" == "null" || "$repo_contracts" == "[]" ]]; then
scripts/federation-lite.sh:725:                local contract_type
scripts/federation-lite.sh:726:                contract_type=$(detect_contract_type "$file")
scripts/federation-lite.sh:727:                [[ "$contract_type" == "unknown" ]] && continue
scripts/federation-lite.sh:730:                symbols=$(extract_symbols "$file" "$contract_type")
scripts/federation-lite.sh:736:                contracts=$(echo "$contracts" | jq \
scripts/federation-lite.sh:738:                    --arg type "$contract_type" \
scripts/federation-lite.sh:743:                log_debug "  Indexed: $rel_path ($contract_type, $(echo "$symbols" | jq 'length') symbols)"
scripts/federation-lite.sh:744:            done < <(find "$repo_path" -type f \( -name "*.proto" -o -name "openapi.yaml" -o -name "openapi.yml" -o -name "swagger.json" -o -name "*.graphql" -o -name "*.d.ts" \) 2>/dev/null)
scripts/federation-lite.sh:750:            --argjson contracts "$contracts" \
scripts/federation-lite.sh:751:            '. + [{"name": $name, "path": $path, "contracts": $contracts}]')
scripts/federation-lite.sh:762:        local search_paths contract_patterns
scripts/federation-lite.sh:764:        contract_patterns=$(echo "$config" | jq -r '.auto_discover.contract_patterns // []')
scripts/federation-lite.sh:766:        # Expand search paths and find directories with contracts
scripts/federation-lite.sh:774:                # Check if this path has any contract files
scripts/federation-lite.sh:775:                local has_contracts=false
scripts/federation-lite.sh:778:                    -o -name "swagger.json" -o -name "*.graphql" -o -name "*.d.ts" \) \
scripts/federation-lite.sh:780:                    has_contracts=true
scripts/federation-lite.sh:783:                [[ "$has_contracts" != "true" ]] && continue
scripts/federation-lite.sh:797:                local contracts="[]"
scripts/federation-lite.sh:799:                # Find and index contract files
scripts/federation-lite.sh:804:                    local contract_type
scripts/federation-lite.sh:805:                    contract_type=$(detect_contract_type "$file")
scripts/federation-lite.sh:806:                    [[ "$contract_type" == "unknown" ]] && continue
scripts/federation-lite.sh:809:                    symbols=$(extract_symbols "$file" "$contract_type")
scripts/federation-lite.sh:815:                    contracts=$(echo "$contracts" | jq \
scripts/federation-lite.sh:817:                        --arg type "$contract_type" \
scripts/federation-lite.sh:822:                    log_debug "  Indexed: $rel_path ($contract_type)"
scripts/federation-lite.sh:825:                    -o -name "swagger.json" -o -name "*.graphql" -o -name "*.d.ts" \) 2>/dev/null)
scripts/federation-lite.sh:830:                    --argjson contracts "$contracts" \
scripts/federation-lite.sh:831:                    '. + [{"name": $name, "path": $path, "contracts": $contracts}]')
scripts/federation-lite.sh:859:    local total_contracts
scripts/federation-lite.sh:860:    total_contracts=$(echo "$index_json" | jq '[.repositories[].contracts | length] | add // 0')
scripts/federation-lite.sh:862:    total_symbols=$(echo "$index_json" | jq '[.repositories[].contracts[].symbols | length] | add // 0')
scripts/federation-lite.sh:864:    log_info "Index complete: $total_repos repositories, $total_contracts contracts, $total_symbols symbols"
scripts/federation-lite.sh:879:            echo "No federation index found at: $FEDERATION_INDEX"
scripts/federation-lite.sh:891:    local contract_count
scripts/federation-lite.sh:892:    contract_count=$(echo "$index" | jq '[.repositories[].contracts | length] | add // 0')
scripts/federation-lite.sh:894:    symbol_count=$(echo "$index" | jq '[.repositories[].contracts[].symbols | length] | add // 0')
scripts/federation-lite.sh:901:            --argjson contracts "$contract_count" \
scripts/federation-lite.sh:908:                contracts: $contracts,
scripts/federation-lite.sh:916:        echo "Contracts: $contract_count"
scripts/federation-lite.sh:923:# SC-FED-005: Search contract symbol
scripts/federation-lite.sh:938:    # Search through all repositories and contracts
scripts/federation-lite.sh:947:        local contract_count
scripts/federation-lite.sh:948:        contract_count=$(echo "$index" | jq ".repositories[$i].contracts | length")
scripts/federation-lite.sh:950:        for ((j=0; j<contract_count; j++)); do
scripts/federation-lite.sh:951:            local contract_path contract_type symbols
scripts/federation-lite.sh:952:            contract_path=$(echo "$index" | jq -r ".repositories[$i].contracts[$j].path")
scripts/federation-lite.sh:953:            contract_type=$(echo "$index" | jq -r ".repositories[$i].contracts[$j].type")
scripts/federation-lite.sh:954:            symbols=$(echo "$index" | jq ".repositories[$i].contracts[$j].symbols")
scripts/federation-lite.sh:968:                        --arg contract "$contract_path" \
scripts/federation-lite.sh:969:                        --arg type "$contract_type" \
scripts/federation-lite.sh:971:                        '. + [{"repository": $repo, "repository_path": $repo_path, "contract": $contract, "type": $type, "symbol": $symbol}]')
scripts/federation-lite.sh:996:            echo "$results" | jq -r '.[] | "    -> \(.repository): \(.contract)"'
scripts/federation-lite.sh:1001:# List all contracts
scripts/federation-lite.sh:1002:list_contracts() {
scripts/federation-lite.sh:1019:        echo "$index" | jq '{repositories: [.repositories[] | {name, path, contracts: [.contracts[] | {path, type, symbol_count: (.symbols | length)}]}]}'
scripts/federation-lite.sh:1021:        echo "$index" | jq -r '.repositories[] | "Repository: \(.name) (\(.path))\n" + (.contracts[] | "  - \(.path) [\(.type)] (\(.symbols | length) symbols)")'
scripts/federation-lite.sh:1030:# Default graph database path
scripts/federation-lite.sh:1031:: "${GRAPH_DB:=.devbooks/graph.db}"
scripts/federation-lite.sh:1038:get_contract_bonus() {
scripts/federation-lite.sh:1039:    local contract_type="$1"
scripts/federation-lite.sh:1040:    case "$contract_type" in
scripts/federation-lite.sh:1042:        graphql)    echo "0.08" ;;
scripts/federation-lite.sh:1218:# Formula: confidence = exact_match * 0.6 + signature_similarity * 0.3 + contract_bonus * 0.1
scripts/federation-lite.sh:1222:    local contract_bonus="$3"
scripts/federation-lite.sh:1226:        confidence=$(echo "scale=2; $exact_match * 0.6 + $signature_similarity * 0.3 + $contract_bonus * 0.1" | bc 2>/dev/null)
scripts/federation-lite.sh:1228:        confidence=$(awk "BEGIN {printf \"%.2f\", $exact_match * 0.6 + $signature_similarity * 0.3 + $contract_bonus * 0.1}" 2>/dev/null)
scripts/federation-lite.sh:1309:    # Ensure federation index exists
scripts/federation-lite.sh:1333:    contract_type TEXT DEFAULT 'unknown',
scripts/federation-lite.sh:1334:    contract_bonus REAL DEFAULT 0.0,
scripts/federation-lite.sh:1346:    log_debug "Database: $db_path"
scripts/federation-lite.sh:1347:    log_debug "Min confidence: $min_confidence"
scripts/federation-lite.sh:1348:    log_debug "Sync mode: $sync_mode"
scripts/federation-lite.sh:1350:    # Load federation index
scripts/federation-lite.sh:1385:        log_debug "Processing local symbol: $local_name"
scripts/federation-lite.sh:1399:            local contract_count
scripts/federation-lite.sh:1400:            contract_count=$(echo "$index" | jq ".repositories[$i].contracts | length")
scripts/federation-lite.sh:1402:            for ((j=0; j<contract_count; j++)); do
scripts/federation-lite.sh:1403:                local contract_type contract_path remote_symbols
scripts/federation-lite.sh:1404:                contract_type=$(echo "$index" | jq -r ".repositories[$i].contracts[$j].type")
scripts/federation-lite.sh:1405:                contract_path=$(echo "$index" | jq -r ".repositories[$i].contracts[$j].path")
scripts/federation-lite.sh:1406:                remote_symbols=$(echo "$index" | jq ".repositories[$i].contracts[$j].symbols")
scripts/federation-lite.sh:1408:                # Get contract bonus
scripts/federation-lite.sh:1409:                local contract_bonus
scripts/federation-lite.sh:1410:                contract_bonus=$(get_contract_bonus "$contract_type")
scripts/federation-lite.sh:1431:                    confidence=$(calculate_confidence "$exact_match" "$signature_similarity" "$contract_bonus")
scripts/federation-lite.sh:1442:                        log_debug "  Skipping $local_name -> $remote_symbol (confidence $confidence < $min_confidence)"
scripts/federation-lite.sh:1451:                    log_debug "  Match: $local_name -> $remote_symbol (confidence: $confidence, level: $confidence_level)"
scripts/federation-lite.sh:1459:                        sql="INSERT INTO virtual_edges (source_repo, source_symbol, target_repo, target_symbol, edge_type, contract_type, confidence, confidence_level, exact_match, signature_similarity, contract_bonus, updated_at) VALUES ('$local_repo_name', '$local_name', '$remote_repo_name', '$remote_symbol', 'VIRTUAL_CALLS', '$contract_type', $confidence, '$confidence_level', $exact_match, $signature_similarity, $contract_bonus, datetime('now'));"
scripts/federation-lite.sh:1466:                            log_debug "    Edge already exists, skipping"
scripts/federation-lite.sh:1469:                        sql="INSERT INTO virtual_edges (source_repo, source_symbol, target_repo, target_symbol, edge_type, contract_type, confidence, confidence_level, exact_match, signature_similarity, contract_bonus) VALUES ('$local_repo_name', '$local_name', '$remote_repo_name', '$remote_symbol', 'VIRTUAL_CALLS', '$contract_type', $confidence, '$confidence_level', $exact_match, $signature_similarity, $contract_bonus);"
scripts/federation-lite.sh:1520:    log_debug "Querying virtual edges for: $symbol"
scripts/federation-lite.sh:1521:    log_debug "Database: $db_path"
scripts/federation-lite.sh:1522:    log_debug "Min confidence: $min_confidence"
scripts/federation-lite.sh:1532:        'contract_type', contract_type,
scripts/federation-lite.sh:1537:        'contract_bonus', contract_bonus,
scripts/federation-lite.sh:1572:            echo "$results" | jq -r '.[] | "  \(.source_symbol) -> \(.target_symbol) [\(.contract_type)]"'
scripts/federation-lite.sh:1629:            --list-contracts)
scripts/federation-lite.sh:1645:            --debug)
scripts/federation-lite.sh:1698:                log_error "Usage: federation-lite.sh --search <symbol>"
scripts/federation-lite.sh:1704:            list_contracts "$repo_filter" "$format"
scripts/federation-lite.sh:1711:                log_error "Usage: federation-lite.sh query-virtual <symbol>"
scripts/pattern-learner.sh:25:  _fast_patterns_file=".devbooks/learned-patterns.json"
scripts/pattern-learner.sh:26:  _fast_elimination_threshold=0.3
scripts/pattern-learner.sh:27:  _fast_cwd="${PROJECT_ROOT:-$(pwd)}"
scripts/pattern-learner.sh:28:  _fast_format="json"
scripts/pattern-learner.sh:33:      --patterns-file|--patterns) _fast_patterns_file="$2"; shift 2 ;;
scripts/pattern-learner.sh:34:      --eliminate-threshold) _fast_elimination_threshold="$2"; shift 2 ;;
scripts/pattern-learner.sh:35:      --cwd) _fast_cwd="$2"; shift 2 ;;
scripts/pattern-learner.sh:36:      --format) _fast_format="$2"; shift 2 ;;
scripts/pattern-learner.sh:43:  [[ "$_fast_patterns_file" != /* ]] && _fast_patterns_file="$_fast_cwd/$_fast_patterns_file"
scripts/pattern-learner.sh:46:  if [[ ! -f "$_fast_patterns_file" ]]; then
scripts/pattern-learner.sh:59:    --argjson t "$_fast_elimination_threshold" \
scripts/pattern-learner.sh:70:      (.last_confirmed // "") as $lc |
scripts/pattern-learner.sh:76:    ' "$_fast_patterns_file")
scripts/pattern-learner.sh:79:  echo "$_result" > "$_fast_patterns_file"
scripts/pattern-learner.sh:186:  PatternScore = frequency √ó (decay_factor ^ days_since_last)
scripts/pattern-learner.sh:188:  Á§∫‰æã: frequency=10, decay_factor=0.9, days_since_last=5
scripts/pattern-learner.sh:971:        | map(last)
scripts/pattern-learner.sh:990:# ÂÖ¨Âºè: PatternScore = frequency √ó (decay_factor ^ days_since_last)
scripts/pattern-learner.sh:994:#   $2 - days_since_last: Ëá™‰∏äÊ¨°Á°ÆËÆ§‰ª•Êù•ÁöÑÂ§©Êï∞
scripts/pattern-learner.sh:1000:  local days_since_last="${2:-0}"
scripts/pattern-learner.sh:1004:  # PatternScore = frequency √ó (decay ^ days_since_last)
scripts/pattern-learner.sh:1006:  score=$(awk -v f="$frequency" -v d="$decay" -v days="$days_since_last" \
scripts/pattern-learner.sh:1093:# Ê†πÊçÆ last_confirmed Êó•ÊúüËÆ°ÁÆóÊØè‰∏™Ê®°ÂºèÁöÑÂΩìÂâçÁΩÆ‰ø°Â∫¶
scripts/pattern-learner.sh:1146:      (.last_confirmed // "") as $lc |
scripts/complexity.sh:15:check_complexity_tools() {
scripts/complexity.sh:49:get_python_complexity() {
scripts/complexity.sh:71:get_scc_complexity() {
scripts/complexity.sh:91:get_go_complexity() {
scripts/complexity.sh:115:get_complexity() {
scripts/complexity.sh:126:  local complexity=""
scripts/complexity.sh:131:      complexity=$(get_python_complexity "$file") || \
scripts/complexity.sh:132:      complexity=$(get_scc_complexity "$file") || \
scripts/complexity.sh:133:      complexity=""
scripts/complexity.sh:137:      complexity=$(get_go_complexity "$file") || \
scripts/complexity.sh:138:      complexity=$(get_scc_complexity "$file") || \
scripts/complexity.sh:139:      complexity=""
scripts/complexity.sh:143:      complexity=$(get_scc_complexity "$file") || complexity=""
scripts/complexity.sh:147:      complexity=$(get_scc_complexity "$file") || complexity=""
scripts/complexity.sh:152:  if [ -n "$complexity" ] && [ "$complexity" -gt 0 ]; then
scripts/complexity.sh:153:    echo "$complexity"
scripts/complexity.sh:165:    echo "Usage: devbooks-complexity.sh <file>" >&2
scripts/complexity.sh:166:    echo "Returns the cyclomatic complexity of a file (>=1)" >&2
scripts/complexity.sh:171:  check_complexity_tools || true
scripts/complexity.sh:174:  get_complexity "$file"
scripts/benchmark.sh:5:# Purpose: Validate P95 latency targets for cache operations and queries
scripts/benchmark.sh:6:# Depends: jq, cache-manager.sh
scripts/benchmark.sh:9:#   benchmark.sh --cache
scripts/benchmark.sh:26:: "${CACHE_SCRIPT:=$SCRIPT_DIR/cache-manager.sh}"
scripts/benchmark.sh:89:benchmark_cache() {
scripts/benchmark.sh:90:    log_info "Running cache benchmark ($BENCHMARK_ITERATIONS iterations)..."
scripts/benchmark.sh:102:    # Clear any existing cache for this test
scripts/benchmark.sh:103:    export CACHE_DIR="${TMPDIR:-/tmp}/.ci-cache-benchmark"
scripts/benchmark.sh:107:    # First, set a cache entry
scripts/benchmark.sh:110:    # Benchmark cache hits
scripts/benchmark.sh:142:    echo "cache_hit_p95_ms=$p95"
scripts/benchmark.sh:153:    # we simulate by timing cache-manager operations
scripts/benchmark.sh:158:    export CACHE_DIR="${TMPDIR:-/tmp}/.ci-cache-benchmark"
scripts/benchmark.sh:203:    local guard_script="$PROJECT_ROOT/scripts/dependency-guard.sh"
scripts/benchmark.sh:205:        log_fail "dependency-guard.sh not found or not executable"
scripts/benchmark.sh:270:            --cache)
scripts/benchmark.sh:271:                mode="cache"
scripts/benchmark.sh:291:                echo "Usage: benchmark.sh [--cache|--full|--precommit|--all] [--iterations N]"
scripts/benchmark.sh:303:    local log_file="$EVIDENCE_DIR/cache-benchmark.log"
scripts/benchmark.sh:314:            cache)
scripts/benchmark.sh:315:                benchmark_cache
scripts/benchmark.sh:325:                benchmark_cache
scripts/intent-learner.sh:2:# intent-learner.sh - ÊÑèÂõæÂÅèÂ•ΩÂ≠¶‰π†Ê®°Âùó
scripts/intent-learner.sh:23:#     - recency_weight = 1 / (1 + days_since_last_query)
scripts/intent-learner.sh:33:#   INTENT_HISTORY_PATH - ÂéÜÂè≤Êñá‰ª∂Ë∑ØÂæÑ (ÈªòËÆ§: .devbooks/intent-history.json)
scripts/intent-learner.sh:57:: "${INTENT_HISTORY_PATH:=${DEVBOOKS_DIR:-.devbooks}/intent-history.json}"
scripts/intent-learner.sh:557:        log_error "Áî®Ê≥ï: intent-learner session resume <session_id>"
scripts/intent-learner.sh:637:        log_error "Áî®Ê≥ï: intent-learner session <new|resume|list|clear>"
scripts/intent-learner.sh:668:        log_error "Áî®Ê≥ï: intent-learner context <save|load|apply-weight>"
scripts/intent-learner.sh:701:        log_error "Áî®Ê≥ï: intent-learner record <symbol> <symbol_id> [--action view|edit|ignore]"
scripts/intent-learner.sh:818:    # ÂÖ∂‰∏≠ recency_weight = 1 / (1 + days_since_last_query)
scripts/intent-learner.sh:844:            last_query: (map(.timestamp) | max),
scripts/intent-learner.sh:920:  intent-learner <command> [options]
scripts/intent-learner.sh:976:  INTENT_HISTORY_PATH           ÂéÜÂè≤Êñá‰ª∂Ë∑ØÂæÑ (ÈªòËÆ§: .devbooks/intent-history.json)
scripts/intent-learner.sh:992:    - recency_weight = 1 / (1 + days_since_last_query)
scripts/intent-learner.sh:1003:  intent-learner record handleToolCall src/server.ts::handleToolCall --action view
scripts/intent-learner.sh:1006:  intent-learner get-preferences --top 5
scripts/intent-learner.sh:1009:  intent-learner get-preferences --prefix src/
scripts/intent-learner.sh:1012:  intent-learner cleanup --days 30
scripts/intent-learner.sh:1015:  intent-learner context save --query "find auth module" --symbols "src/auth.ts,src/auth.ts::login"
scripts/intent-learner.sh:1018:  intent-learner context load
scripts/intent-learner.sh:1021:  intent-learner context apply-weight --results '[{"symbol":"src/auth.ts::login","score":0.8}]'
scripts/intent-learner.sh:1024:  intent-learner session new
scripts/intent-learner.sh:1027:  intent-learner session list
scripts/intent-learner.sh:1030:  intent-learner session clear
scripts/intent-learner.sh:1038:    if ! check_dependency "jq"; then
scripts/scip-to-graph.sh:2:# scip-to-graph.sh - SCIP Á¥¢ÂºïËß£ÊûêËΩ¨Êç¢ËÑöÊú¨
scripts/scip-to-graph.sh:10:#   SCIP_INDEX_PATH - SCIP Á¥¢ÂºïË∑ØÂæÑÔºåÈªòËÆ§ index.scip
scripts/scip-to-graph.sh:11:#   GRAPH_DB_PATH - Êï∞ÊçÆÂ∫ìË∑ØÂæÑÔºåÈªòËÆ§ .devbooks/graph.db
scripts/scip-to-graph.sh:21:LOG_PREFIX="scip-to-graph"
scripts/scip-to-graph.sh:26:: "${SCIP_INDEX_PATH:=index.scip}"
scripts/scip-to-graph.sh:27:: "${GRAPH_DB_PATH:=$DEVBOOKS_DIR/graph.db}"
scripts/scip-to-graph.sh:32:#   2. vendored/scip.protoÔºàVENDOREDÔºâ
scripts/scip-to-graph.sh:33:#   3. $CACHE_DIR/scip.protoÔºàCACHEDÔºâ
scripts/scip-to-graph.sh:36:SCIP_PROTO_URL="https://raw.githubusercontent.com/sourcegraph/scip/main/scip.proto"
scripts/scip-to-graph.sh:75:        BEGIN { in_indexer = 0 }
scripts/scip-to-graph.sh:76:        /^[[:space:]]*indexer:/ { in_indexer = 1; next }
scripts/scip-to-graph.sh:77:        /^[[:space:]]*[a-z]/ && !/^[[:space:]]*indexer:/ && in_indexer { in_indexer = 0 }
scripts/scip-to-graph.sh:78:        in_indexer && $1 ~ key {
scripts/scip-to-graph.sh:98:ensure_scip_proto() {
scripts/scip-to-graph.sh:116:    # ‰ºòÂÖàÁ∫ß 2: vendored/scip.protoÔºàVENDOREDÔºâ
scripts/scip-to-graph.sh:120:        local vendored_path="$project_root/vendored/scip.proto"
scripts/scip-to-graph.sh:139:    local cached_path="$SCIP_PROTO_CACHE_DIR/scip.proto"
scripts/scip-to-graph.sh:140:    if [[ -f "$cached_path" ]]; then
scripts/scip-to-graph.sh:141:        RESOLVED_PROTO_PATH="$cached_path"
scripts/scip-to-graph.sh:144:        log_info "Using cached proto: $RESOLVED_PROTO_PATH"
scripts/scip-to-graph.sh:152:            if curl -s --connect-timeout 10 "$SCIP_PROTO_URL" -o "$cached_path" 2>/dev/null; then
scripts/scip-to-graph.sh:153:                RESOLVED_PROTO_PATH="$cached_path"
scripts/scip-to-graph.sh:156:                log_ok "Downloaded proto to: $cached_path"
scripts/scip-to-graph.sh:160:            if wget -q --timeout=10 "$SCIP_PROTO_URL" -O "$cached_path" 2>/dev/null; then
scripts/scip-to-graph.sh:161:                RESOLVED_PROTO_PATH="$cached_path"
scripts/scip-to-graph.sh:164:                log_ok "Downloaded proto to: $cached_path"
scripts/scip-to-graph.sh:175:    log_error "  2. vendored/scip.proto"
scripts/scip-to-graph.sh:176:    log_error "  3. $SCIP_PROTO_CACHE_DIR/scip.proto"
scripts/scip-to-graph.sh:189:is_scip_fresh() {
scripts/scip-to-graph.sh:190:    local scip_path="$1"
scripts/scip-to-graph.sh:197:    if [[ ! -f "$scip_path" ]]; then
scripts/scip-to-graph.sh:202:    if [[ "$scip_path" -nt "$db_path" ]]; then
scripts/scip-to-graph.sh:232:    # ‰æãÂ¶Ç: scip-typescript npm @types/node 18.0.0 path/`join`().
scripts/scip-to-graph.sh:254:parse_scip_with_node() {
scripts/scip-to-graph.sh:255:    local scip_path="$1"
scripts/scip-to-graph.sh:259:    ensure_scip_proto || return 1
scripts/scip-to-graph.sh:266:    local node_script="$project_root/.devbooks/parse-scip-temp.cjs"
scripts/scip-to-graph.sh:277:    const scipPath = process.argv[2];
scripts/scip-to-graph.sh:284:        const Index = root.lookupType('scip.Index');
scripts/scip-to-graph.sh:286:        const buffer = fs.readFileSync(scipPath);
scripts/scip-to-graph.sh:287:        const index = Index.decode(buffer);
scripts/scip-to-graph.sh:442:    if result=$(cd "$project_root" && node "$node_script" "$scip_path" "$output_file" 2>&1); then
scripts/scip-to-graph.sh:613:        nodes_json=$(printf '%s\n' "${nodes[@]}" | paste -sd ',' -)
scripts/scip-to-graph.sh:619:        edges_json=$(printf '%s\n' "${edges[@]}" | paste -sd ',' -)
scripts/scip-to-graph.sh:667:        log_info "Generate with: npx scip-typescript index"
scripts/scip-to-graph.sh:675:    "$SCRIPT_DIR/graph-store.sh" init >/dev/null 2>&1
scripts/scip-to-graph.sh:679:        if ! is_scip_fresh "$SCIP_INDEX_PATH" "$GRAPH_DB_PATH"; then
scripts/scip-to-graph.sh:682:                echo '{"status":"up-to-date","symbols":0,"confidence":"high","source":"scip"}'
scripts/scip-to-graph.sh:703:    local source="scip"
scripts/scip-to-graph.sh:705:    if parse_result=$(parse_scip_with_node "$SCIP_INDEX_PATH" "$temp_json" 2>&1); then
scripts/scip-to-graph.sh:730:    import_result=$("$SCRIPT_DIR/graph-store.sh" batch-import --file "$temp_json" 2>&1)
scripts/scip-to-graph.sh:755:    local scip_path="${SCIP_INDEX_PATH}"
scripts/scip-to-graph.sh:757:    if [[ ! -f "$scip_path" ]]; then
scripts/scip-to-graph.sh:766:    if parse_scip_with_node "$scip_path" "$temp_json" >/dev/null 2>&1; then
scripts/scip-to-graph.sh:779:    local scip_path="${SCIP_INDEX_PATH}"
scripts/scip-to-graph.sh:782:    if is_scip_fresh "$scip_path" "$db_path"; then
scripts/scip-to-graph.sh:803:    # Ë∞ÉÁî® ensure_scip_proto Êù•ÂèëÁé∞ proto Êñá‰ª∂
scripts/scip-to-graph.sh:804:    if ensure_scip_proto; then
scripts/scip-to-graph.sh:846:scip-to-graph.sh - SCIP Á¥¢ÂºïËß£ÊûêËΩ¨Êç¢
scripts/scip-to-graph.sh:849:    scip-to-graph.sh <command> [options]
scripts/scip-to-graph.sh:867:    SCIP_INDEX_PATH     SCIP Á¥¢ÂºïË∑ØÂæÑÔºàÈªòËÆ§: index.scipÔºâ
scripts/scip-to-graph.sh:868:    GRAPH_DB_PATH       Êï∞ÊçÆÂ∫ìË∑ØÂæÑÔºàÈªòËÆ§: .devbooks/graph.dbÔºâ
scripts/scip-to-graph.sh:874:    2. vendored/scip.proto
scripts/scip-to-graph.sh:875:    3. $SCIP_PROTO_CACHE_DIR/scip.protoÔºàÈªòËÆ§ /tmpÔºâ
scripts/scip-to-graph.sh:886:    scip-to-graph.sh parse
scripts/scip-to-graph.sh:889:    scip-to-graph.sh parse --incremental
scripts/scip-to-graph.sh:892:    scip-to-graph.sh parse --force --format json
scripts/scip-to-graph.sh:895:    scip-to-graph.sh check-proto --format json
scripts/scip-to-graph.sh:960:        bash "$SCRIPT_DIR/graph-store.sh" init >/dev/null 2>&1 || true
scripts/scip-to-graph.sh:973:            bash "$SCRIPT_DIR/graph-store.sh" batch-import --file "$temp_json" >/dev/null 2>&1
scripts/adr-parser.sh:2:# adr-parser.sh - ADR (Architecture Decision Records) Ëß£Êûê‰∏éÂÖ≥ËÅîËÑöÊú¨
scripts/adr-parser.sh:11:#   GRAPH_DB_PATH - ÂõæÊï∞ÊçÆÂ∫ìË∑ØÂæÑÔºåÈªòËÆ§ .devbooks/graph.db
scripts/adr-parser.sh:12:#   ADR_INDEX_PATH - ADR Á¥¢ÂºïÊñá‰ª∂Ë∑ØÂæÑÔºåÈªòËÆ§ .devbooks/adr-index.json
scripts/adr-parser.sh:21:LOG_PREFIX="adr-parser"
scripts/adr-parser.sh:26:: "${GRAPH_DB_PATH:=$DEVBOOKS_DIR/graph.db}"
scripts/adr-parser.sh:27:: "${ADR_INDEX_PATH:=$DEVBOOKS_DIR/adr-index.json}"
scripts/adr-parser.sh:30:ADR_SEARCH_PATHS=("docs/adr" "doc/adr" "ADR" "adr")
scripts/adr-parser.sh:48:    "cache" "data" "system" "simple" "record" "mode" "need" "make"
scripts/adr-parser.sh:84:find_adr_dir() {
scripts/adr-parser.sh:104:detect_adr_format() {
scripts/adr-parser.sh:113:        echo "madr"
scripts/adr-parser.sh:160:parse_madr() {
scripts/adr-parser.sh:167:    local adr_id=""
scripts/adr-parser.sh:171:        adr_id="${BASH_REMATCH[1]}"
scripts/adr-parser.sh:191:        --arg id "$adr_id" \
scripts/adr-parser.sh:198:        --arg format "madr" \
scripts/adr-parser.sh:220:    local adr_id=""
scripts/adr-parser.sh:224:        adr_id="${BASH_REMATCH[1]}"
scripts/adr-parser.sh:229:    local adr_date=""
scripts/adr-parser.sh:230:    adr_date=$(grep -E "^Date:" "$file" 2>/dev/null | head -1 | sed 's/^Date:[[:space:]]*//' | tr -d '\r')
scripts/adr-parser.sh:248:        --arg id "$adr_id" \
scripts/adr-parser.sh:251:        --arg date "$adr_date" \
scripts/adr-parser.sh:271:parse_adr_file() {
scripts/adr-parser.sh:280:    format=$(detect_adr_format "$file")
scripts/adr-parser.sh:283:        madr)
scripts/adr-parser.sh:284:            parse_madr "$file"
scripts/adr-parser.sh:291:            parse_madr "$file"
scripts/adr-parser.sh:299:    local adr_json="$1"
scripts/adr-parser.sh:303:    decision=$(echo "$adr_json" | jq -r '.decision // ""')
scripts/adr-parser.sh:304:    context=$(echo "$adr_json" | jq -r '.context // ""')
scripts/adr-parser.sh:305:    title=$(echo "$adr_json" | jq -r '.title // ""')
scripts/adr-parser.sh:386:link_keywords_to_graph() {
scripts/adr-parser.sh:387:    local adr_id="$1"
scripts/adr-parser.sh:391:        log_info "graph.db ‰∏çÂ≠òÂú®ÔºåË∑≥ËøáÂÖ≥ËÅî"
scripts/adr-parser.sh:403:        # Âú® graph.db ‰∏≠ÊêúÁ¥¢ÂåπÈÖçÁöÑËäÇÁÇπ
scripts/adr-parser.sh:426:                edge_id="edge:adr:$(date +%s)-$RANDOM"
scripts/adr-parser.sh:427:                local source_id="adr:$adr_id"
scripts/adr-parser.sh:483:    local adr_json
scripts/adr-parser.sh:484:    adr_json=$(parse_adr_file "$file")
scripts/adr-parser.sh:488:    keywords_json=$(extract_keywords "$adr_json")
scripts/adr-parser.sh:492:    result=$(echo "$adr_json" | jq --argjson keywords "$keywords_json" '. + {keywords: $keywords}')
scripts/adr-parser.sh:494:    # ÂåÖË£Ö‰∏∫ adrs Êï∞ÁªÑÊ†ºÂºè
scripts/adr-parser.sh:495:    jq -n --argjson adr "$result" '{adrs: [$adr], edges_generated: 0}'
scripts/adr-parser.sh:516:    local adr_json
scripts/adr-parser.sh:517:    adr_json=$(parse_adr_file "$file")
scripts/adr-parser.sh:519:    extract_keywords "$adr_json"
scripts/adr-parser.sh:525:    local adr_dir=""
scripts/adr-parser.sh:531:            --adr-dir) adr_dir="$2"; shift 2 ;;
scripts/adr-parser.sh:541:    if [[ -z "$adr_dir" ]]; then
scripts/adr-parser.sh:542:        adr_dir=$(find_adr_dir "." 2>/dev/null) || true
scripts/adr-parser.sh:547:    if [[ -z "$adr_dir" || ! -d "$adr_dir" ]]; then
scripts/adr-parser.sh:549:        echo '{"adrs": [], "edges_generated": 0}'
scripts/adr-parser.sh:553:    local adrs=()
scripts/adr-parser.sh:557:    for adr_file in "$adr_dir"/*.md; do
scripts/adr-parser.sh:558:        [[ -f "$adr_file" ]] || continue
scripts/adr-parser.sh:560:        log_info "Ëß£Êûê: $adr_file"
scripts/adr-parser.sh:563:        local adr_json
scripts/adr-parser.sh:564:        adr_json=$(parse_adr_file "$adr_file")
scripts/adr-parser.sh:568:        keywords_json=$(extract_keywords "$adr_json")
scripts/adr-parser.sh:571:        adr_json=$(echo "$adr_json" | jq --argjson keywords "$keywords_json" '. + {keywords: $keywords}')
scripts/adr-parser.sh:575:            local adr_id
scripts/adr-parser.sh:576:            adr_id=$(echo "$adr_json" | jq -r '.id')
scripts/adr-parser.sh:579:            link_result=$(link_keywords_to_graph "$adr_id" "$keywords_json")
scripts/adr-parser.sh:588:            adr_json=$(echo "$adr_json" | jq --argjson related "$related_nodes" '. + {related_nodes: $related}')
scripts/adr-parser.sh:591:        adrs+=("$adr_json")
scripts/adr-parser.sh:595:    local adrs_array
scripts/adr-parser.sh:596:    if [[ ${#adrs[@]} -eq 0 ]]; then
scripts/adr-parser.sh:597:        adrs_array='[]'
scripts/adr-parser.sh:599:        adrs_array=$(printf '%s\n' "${adrs[@]}" | jq -s .)
scripts/adr-parser.sh:604:        --argjson adrs "$adrs_array" \
scripts/adr-parser.sh:606:        '{adrs: $adrs, edges_generated: $edges}')
scripts/adr-parser.sh:611:        update_adr_index "$result"
scripts/adr-parser.sh:622:    local adr_dir
scripts/adr-parser.sh:623:    adr_dir=$(find_adr_dir "." 2>/dev/null) || true
scripts/adr-parser.sh:625:    local adr_count=0
scripts/adr-parser.sh:630:    if [[ -n "$adr_dir" && -d "$adr_dir" ]]; then
scripts/adr-parser.sh:631:        adr_count=$(ls "$adr_dir"/*.md 2>/dev/null | wc -l | tr -d ' ')
scripts/adr-parser.sh:644:        --arg adr_dir "${adr_dir:-none}" \
scripts/adr-parser.sh:645:        --argjson adr_count "$adr_count" \
scripts/adr-parser.sh:650:            adr_directory: $adr_dir,
scripts/adr-parser.sh:651:            adr_file_count: $adr_count,
scripts/adr-parser.sh:653:            index_last_updated: $index_mtime,
scripts/adr-parser.sh:654:            adr_related_edges: $edge_count
scripts/adr-parser.sh:662:update_adr_index() {
scripts/adr-parser.sh:685:adr-parser.sh - ADR Ëß£Êûê‰∏éÂÖ≥ËÅî
scripts/adr-parser.sh:688:    adr-parser.sh <command> [options]
scripts/adr-parser.sh:700:    --adr-dir <path>    ÊåáÂÆö ADR ÁõÆÂΩïÔºàË¶ÜÁõñËá™Âä®ÂèëÁé∞Ôºâ
scripts/adr-parser.sh:701:    --link              ÁîüÊàêÂÖ≥ËÅîËæπÂÜôÂÖ• graph.db
scripts/adr-parser.sh:712:    1. docs/adr/
scripts/adr-parser.sh:713:    2. doc/adr/
scripts/adr-parser.sh:715:    4. adr/
scripts/adr-parser.sh:719:    GRAPH_DB_PATH       ÂõæÊï∞ÊçÆÂ∫ìË∑ØÂæÑÔºàÈªòËÆ§: .devbooks/graph.dbÔºâ
scripts/adr-parser.sh:720:    ADR_INDEX_PATH      Á¥¢ÂºïÊñá‰ª∂Ë∑ØÂæÑÔºàÈªòËÆ§: .devbooks/adr-index.jsonÔºâ
scripts/adr-parser.sh:724:    adr-parser.sh parse docs/adr/0001-use-sqlite.md
scripts/adr-parser.sh:727:    adr-parser.sh scan
scripts/adr-parser.sh:730:    adr-parser.sh scan --link
scripts/adr-parser.sh:733:    adr-parser.sh status
tests/graph-store.bats:2:# graph-store.bats - SQLite ÂõæÂ≠òÂÇ®ÊµãËØï
tests/graph-store.bats:24:GRAPH_STORE="$SCRIPT_DIR/graph-store.sh"
tests/graph-store.bats:25:SCIP_TO_GRAPH="$SCRIPT_DIR/scip-to-graph.sh"
tests/graph-store.bats:29:    export GRAPH_DB_PATH="$TEST_TEMP_DIR/graph.db"
tests/graph-store.bats:43:@test "SC-GS-001: graph-store init creates database with correct schema" {
tests/graph-store.bats:47:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:65:@test "SC-GS-010: graph-store init skips when database exists" {
tests/graph-store.bats:70:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:90:@test "SC-GS-002: graph-store add-node creates node successfully" {
tests/graph-store.bats:94:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:104:    skip_if_not_ready "$status" "$output" "graph-store.sh add-node"
tests/graph-store.bats:113:@test "SC-GS-003: graph-store add-edge creates edge with valid type" {
tests/graph-store.bats:117:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:131:    skip_if_not_ready "$status" "$output" "graph-store.sh add-edge"
tests/graph-store.bats:140:@test "SC-GS-004: graph-store add-edge rejects invalid edge type" {
tests/graph-store.bats:144:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:153:    skip_if_not_ready "$status" "$output" "graph-store.sh add-edge validation"
tests/graph-store.bats:160:@test "SC-GS-004b: graph-store supports all 4 edge types" {
tests/graph-store.bats:164:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:174:        skip_if_not_ready "$status" "$output" "graph-store.sh add-edge $edge_type"
tests/graph-store.bats:188:@test "SC-GS-005: graph-store query-edges filters by type" {
tests/graph-store.bats:192:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:207:    skip_if_not_ready "$status" "$output" "graph-store.sh query-edges"
tests/graph-store.bats:218:@test "SC-GS-006: graph-store find-orphans returns nodes with no incoming edges" {
tests/graph-store.bats:222:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:238:    skip_if_not_ready "$status" "$output" "graph-store.sh find-orphans"
tests/graph-store.bats:250:@test "SC-GS-011: graph-store find-orphans returns empty array for empty graph" {
tests/graph-store.bats:254:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:258:    skip_if_not_ready "$status" "$output" "graph-store.sh find-orphans"
tests/graph-store.bats:273:@test "SC-GS-007: graph-store batch-import writes all nodes in single transaction" {
tests/graph-store.bats:277:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:292:    skip_if_not_ready "$status" "$output" "graph-store.sh batch-import"
tests/graph-store.bats:301:@test "SC-GS-008: graph-store batch-import rolls back on error" {
tests/graph-store.bats:305:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:320:    skip_if_not_ready "$status" "$output" "graph-store.sh batch-import validation"
tests/graph-store.bats:329:@test "SC-GS-009: graph-store stats returns correct counts" {
tests/graph-store.bats:333:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:343:    skip_if_not_ready "$status" "$output" "graph-store.sh stats"
tests/graph-store.bats:363:@test "AC-N03a: graph-store batch-import succeeds for bulk data" {
tests/graph-store.bats:367:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:383:    skip_if_not_ready "$status" "$output" "graph-store.sh batch-import bulk"
tests/graph-store.bats:393:@test "AC-N03b: graph-store falls back to single operations when batch fails" {
tests/graph-store.bats:397:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:412:    skip_if_not_ready "$status" "$output" "graph-store.sh add-node fallback"
tests/graph-store.bats:422:@test "AC-N03c: graph database file size is reasonable" {
tests/graph-store.bats:426:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:482:@test "test_edge_types: graph-store add-edge supports IMPLEMENTS/EXTENDS/RETURNS_TYPE" {
tests/graph-store.bats:487:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:497:    skip_if_not_ready "$status" "$output" "graph-store.sh add-edge IMPLEMENTS"
tests/graph-store.bats:500:    skip_if_not_ready "$status" "$output" "graph-store.sh add-edge EXTENDS"
tests/graph-store.bats:503:    skip_if_not_ready "$status" "$output" "graph-store.sh add-edge RETURNS_TYPE"
tests/graph-store.bats:508:        skip_not_implemented "graph-store edge types"
tests/graph-store.bats:512:@test "test_edge_types_python: scip-to-graph produces IMPLEMENTS edges for annotated Python" {
tests/graph-store.bats:524:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh parse (python)"
tests/graph-store.bats:545:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh parse (fallback)"
tests/graph-store.bats:554:@test "test_migrate_check_old: graph-store migrate --check detects old schema" {
tests/graph-store.bats:558:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:563:        skip_if_not_ready "$status" "$output" "graph-store.sh migrate --check"
tests/graph-store.bats:569:@test "test_migrate_check_new: graph-store migrate --check returns UP_TO_DATE after apply" {
tests/graph-store.bats:573:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:577:        skip_if_not_ready "$status" "$output" "graph-store.sh migrate --apply"
tests/graph-store.bats:582:        skip_if_not_ready "$status" "$output" "graph-store.sh migrate --check"
tests/graph-store.bats:588:@test "test_migrate_apply: graph-store migrate preserves edges and data" {
tests/graph-store.bats:593:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:603:    skip_if_not_ready "$status" "$output" "graph-store.sh migrate --apply"
tests/graph-store.bats:612:@test "test_migrate_backup: graph-store migrate creates backup file" {
tests/graph-store.bats:616:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:619:    skip_if_not_ready "$status" "$output" "graph-store.sh migrate --apply"
tests/graph-store.bats:628:@test "test_find_path_basic: graph-store find-path returns shortest path" {
tests/graph-store.bats:633:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:642:    skip_if_not_ready "$status" "$output" "graph-store.sh find-path"
tests/graph-store.bats:652:@test "test_find_path_depth: graph-store find-path respects max depth" {
tests/graph-store.bats:657:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:668:    skip_if_not_ready "$status" "$output" "graph-store.sh find-path depth"
tests/graph-store.bats:678:@test "test_find_path_filter: graph-store find-path respects edge type filter" {
tests/graph-store.bats:683:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:692:    skip_if_not_ready "$status" "$output" "graph-store.sh find-path filter"
tests/graph-store.bats:702:@test "test_find_path_no_path: graph-store find-path returns empty when no path" {
tests/graph-store.bats:707:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:713:    skip_if_not_ready "$status" "$output" "graph-store.sh find-path no path"
tests/graph-store.bats:723:@test "test_find_path_output: graph-store find-path output includes path and length" {
tests/graph-store.bats:728:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/graph-store.bats:735:    skip_if_not_ready "$status" "$output" "graph-store.sh find-path output"
scripts/ast-diff.sh:11:#   ast-diff.sh update [ÈÄâÈ°π]
scripts/ast-diff.sh:12:#   ast-diff.sh status [ÈÄâÈ°π]
scripts/ast-diff.sh:63:SCIP_INDEX="$CWD/index.scip"
scripts/ast-diff.sh:64:CACHE_DIR="$CWD/.ci-cache"
scripts/ast-diff.sh:65:LAST_INDEX_TIME_FILE="$CACHE_DIR/last-index-time"
scripts/ast-diff.sh:81:  ast-diff.sh update [ÈÄâÈ°π]   Êõ¥Êñ∞Á¥¢ÂºïÔºàÂ¢ûÈáèÊàñÂÖ®ÈáèÔºâ
scripts/ast-diff.sh:82:  ast-diff.sh status [ÈÄâÈ°π]   ÊòæÁ§∫Á¥¢ÂºïÁä∂ÊÄÅ
scripts/ast-diff.sh:83:  ast-diff.sh diff [ÈÄâÈ°π]     ÊòæÁ§∫ÂèòÊõ¥Êñá‰ª∂ÂàóË°®
scripts/ast-diff.sh:110:  ast-diff.sh update
scripts/ast-diff.sh:113:  ast-diff.sh update --force-full
scripts/ast-diff.sh:116:  ast-diff.sh status
scripts/ast-diff.sh:122:  echo "ast-diff.sh version 1.0.0"
scripts/ast-diff.sh:143:        SCIP_INDEX="$CWD/index.scip"
scripts/ast-diff.sh:144:        CACHE_DIR="$CWD/.ci-cache"
scripts/ast-diff.sh:145:        LAST_INDEX_TIME_FILE="$CACHE_DIR/last-index-time"
scripts/ast-diff.sh:173:check_scip_index() {
scripts/ast-diff.sh:190:get_last_index_time() {
scripts/ast-diff.sh:242:# Êú™Êù•ÂÆûÁé∞Ë∑ØÂæÑÔºöscip-typescript index --incremental --files <changed-files>
scripts/ast-diff.sh:282:  # ‰æãÂ¶ÇÔºöscip-typescript index .
scripts/ast-diff.sh:285:  if command -v scip-typescript &>/dev/null; then
scripts/ast-diff.sh:286:    (cd "$CWD" && scip-typescript index . 2>/dev/null) || true
scripts/ast-diff.sh:288:    log_warn "scip-typescript ‰∏çÂèØÁî®ÔºåË∑≥ËøáÂÖ®ÈáèÁ¥¢Âºï"
scripts/ast-diff.sh:310:  if ! check_scip_index && [ "$FORCE_FULL" = false ]; then
scripts/ast-diff.sh:315:      --arg message "SCIP Á¥¢Âºï‰∏çÂ≠òÂú®ÔºåËØ∑ÂÖàËøêË°åÁ¥¢ÂºïÁîüÊàêÂ∑•ÂÖ∑ÔºàÂ¶Ç scip-typescript index .Ôºâ" \
scripts/ast-diff.sh:339:        --arg message "scip-typescript ‰∏çÂèØÁî®ÔºåÁ¥¢ÂºïÊú™Êõ¥Êñ∞" \
scripts/ast-diff.sh:365:  local last_time
scripts/ast-diff.sh:366:  last_time=$(get_last_index_time)
scripts/ast-diff.sh:370:  changed_files=$(get_changed_files "$last_time")
scripts/ast-diff.sh:436:  local last_update=0
scripts/ast-diff.sh:438:  if check_scip_index; then
scripts/ast-diff.sh:443:  last_update=$(get_last_index_time)
scripts/ast-diff.sh:449:    changed=$(get_changed_files "$last_update")
scripts/ast-diff.sh:463:    --argjson last_update "$last_update" \
scripts/ast-diff.sh:470:      last_update: $last_update,
scripts/ast-diff.sh:481:  local last_time
scripts/ast-diff.sh:482:  last_time=$(get_last_index_time)
scripts/ast-diff.sh:485:  changed_files=$(get_changed_files "$last_time")
scripts/ast-diff.sh:567:      local result exit_code=0
scripts/ast-diff.sh:568:      result=$(do_update) || exit_code=$?
scripts/ast-diff.sh:570:      exit "$exit_code"
scripts/context-layer.sh:2:# context-layer.sh - Context Layer Enhancement (Commit Classification + Bug History)
scripts/context-layer.sh:5:# Purpose: Semantic commit classification and bug fix history extraction
scripts/context-layer.sh:9:#   context-layer.sh --classify <sha>
scripts/context-layer.sh:10:#   context-layer.sh --classify-batch --since "90 days ago"
scripts/context-layer.sh:11:#   context-layer.sh --bug-history --file <path>
scripts/context-layer.sh:12:#   context-layer.sh --index [--days 90]
scripts/context-layer.sh:13:#   context-layer.sh --help
scripts/context-layer.sh:19:#   DEBUG                 - Enable debug output (default: false)
scripts/context-layer.sh:51:log_debug() {
scripts/context-layer.sh:78:context-layer.sh - Context Layer Enhancement (Commit Classification + Bug History)
scripts/context-layer.sh:81:  context-layer.sh --classify <sha>
scripts/context-layer.sh:82:  context-layer.sh --classify-batch --since "90 days ago"
scripts/context-layer.sh:83:  context-layer.sh --bug-history --file <path>
scripts/context-layer.sh:84:  context-layer.sh --index [--days 90]
scripts/context-layer.sh:85:  context-layer.sh --help
scripts/context-layer.sh:91:  --bug-history         Extract bug fix history
scripts/context-layer.sh:92:  --file <path>         Target file for bug history
scripts/context-layer.sh:96:  --debug               Enable debug output
scripts/context-layer.sh:106:  context-layer.sh --classify abc123
scripts/context-layer.sh:108:  # Classify all commits from last 30 days
scripts/context-layer.sh:109:  context-layer.sh --classify-batch --since "30 days ago"
scripts/context-layer.sh:111:  # Get bug history for a file
scripts/context-layer.sh:112:  context-layer.sh --bug-history --file src/server.ts
scripts/context-layer.sh:115:  context-layer.sh --index --days 90
scripts/context-layer.sh:124:# REQ-CTX-001: Commit semantic classification
scripts/context-layer.sh:133:    if declare -f is_bug_fix_message &>/dev/null; then
scripts/context-layer.sh:134:        if is_bug_fix_message "$message"; then
scripts/context-layer.sh:141:           [[ "$msg_lower" =~ (bug|issue|error|crash|broken|fail) ]]; then
scripts/context-layer.sh:273:# Get bug fix history for a specific file
scripts/context-layer.sh:276:get_bug_history_for_file() {
scripts/context-layer.sh:283:        log_debug "File not found: $file_path (may have been deleted)"
scripts/context-layer.sh:286:    local bug_fix_commits="[]"
scripts/context-layer.sh:287:    local bug_fix_count=0
scripts/context-layer.sh:288:    local last_bug_fix=""
scripts/context-layer.sh:308:            bug_fix_count=$((bug_fix_count + 1))
scripts/context-layer.sh:309:            bug_fix_commits=$(echo "$bug_fix_commits" | jq --arg sha "$sha" '. + [$sha]')
scripts/context-layer.sh:310:            if [[ -z "$last_bug_fix" ]]; then
scripts/context-layer.sh:311:                last_bug_fix="$date"
scripts/context-layer.sh:319:            --argjson bug_fix_count "$bug_fix_count" \
scripts/context-layer.sh:320:            --argjson bug_fix_commits "$bug_fix_commits" \
scripts/context-layer.sh:321:            --arg last_bug_fix "${last_bug_fix:-null}" \
scripts/context-layer.sh:325:                bug_fix_count: $bug_fix_count,
scripts/context-layer.sh:326:                bug_fix_commits: $bug_fix_commits,
scripts/context-layer.sh:327:                last_bug_fix: (if $last_bug_fix == "null" then null else $last_bug_fix end),
scripts/context-layer.sh:332:        echo "Bug Fix Count: $bug_fix_count"
scripts/context-layer.sh:333:        echo "Last Bug Fix: ${last_bug_fix:-N/A}"
scripts/context-layer.sh:334:        echo "Bug Fix Commits: $(echo "$bug_fix_commits" | jq -r 'join(", ")')"
scripts/context-layer.sh:348:    log_info "Generating context index (last $days days)..."
scripts/context-layer.sh:365:    log_info "Found $file_count files with commits in last $days days"
scripts/context-layer.sh:372:        local bug_fix_commits="[]"
scripts/context-layer.sh:373:        local bug_fix_count=0
scripts/context-layer.sh:374:        local last_bug_fix=""
scripts/context-layer.sh:397:            # Track bug fixes
scripts/context-layer.sh:399:                bug_fix_count=$((bug_fix_count + 1))
scripts/context-layer.sh:400:                bug_fix_commits=$(echo "$bug_fix_commits" | jq --arg sha "$sha" '. + [$sha]')
scripts/context-layer.sh:401:                if [[ -z "$last_bug_fix" ]]; then
scripts/context-layer.sh:402:                    last_bug_fix="$date"
scripts/context-layer.sh:410:            --argjson bug_fix_count "$bug_fix_count" \
scripts/context-layer.sh:411:            --argjson bug_fix_commits "$bug_fix_commits" \
scripts/context-layer.sh:412:            --arg last_bug_fix "${last_bug_fix:-null}" \
scripts/context-layer.sh:416:                bug_fix_count: $bug_fix_count,
scripts/context-layer.sh:417:                bug_fix_commits: $bug_fix_commits,
scripts/context-layer.sh:418:                last_bug_fix: (if $last_bug_fix == "null" then null else $last_bug_fix end),
scripts/context-layer.sh:422:        log_debug "Processed: $file_path"
scripts/context-layer.sh:455:# Get bug fix ratio for a file (for hotspot-analyzer.sh integration)
scripts/context-layer.sh:456:get_bug_fix_ratio() {
scripts/context-layer.sh:461:    local bug_fix_count=0
scripts/context-layer.sh:477:            bug_fix_count=$((bug_fix_count + 1))
scripts/context-layer.sh:485:        awk -v bug="$bug_fix_count" -v total="$total_commits" 'BEGIN { printf "%.4f", bug / total }'
scripts/context-layer.sh:515:            --bug-history)
scripts/context-layer.sh:516:                action="bug-history"
scripts/context-layer.sh:539:            --debug)
scripts/context-layer.sh:578:                log_error "Usage: context-layer.sh --classify <sha>"
scripts/context-layer.sh:586:        bug-history)
scripts/context-layer.sh:588:                log_error "Usage: context-layer.sh --bug-history --file <path>"
scripts/context-layer.sh:591:            get_bug_history_for_file "$file_path" "$days" "$format"
scripts/indexer.sh:19:LOG_PREFIX="indexer"
scripts/indexer.sh:27:read_indexer_config() {
scripts/indexer.sh:35:        "ast_delta_enabled")
scripts/indexer.sh:46:                value=$(awk '/^[[:space:]]*indexer:/{found=1} found && /debounce_seconds:/{gsub(/.*:/,""); gsub(/[[:space:]]/,""); print; exit}' "$CONFIG_FILE" 2>/dev/null)
scripts/indexer.sh:48:            "ast_delta_enabled")
scripts/indexer.sh:49:                value=$(awk '/^[[:space:]]*ast_delta:/{found=1} found && /enabled:/{gsub(/.*:/,""); gsub(/[[:space:]]/,""); print; exit}' "$CONFIG_FILE" 2>/dev/null)
scripts/indexer.sh:52:                value=$(awk '/^[[:space:]]*ast_delta:/{found=1} found && /file_threshold:/{gsub(/.*:/,""); gsub(/[[:space:]]/,""); print; exit}' "$CONFIG_FILE" 2>/dev/null)
scripts/indexer.sh:62:DEBOUNCE_SECONDS=$(read_indexer_config "debounce_seconds" "2")
scripts/indexer.sh:63:AST_DELTA_ENABLED=$(read_indexer_config "ast_delta_enabled" "true")
scripts/indexer.sh:64:FILE_THRESHOLD=$(read_indexer_config "file_threshold" "10")
scripts/indexer.sh:67:IGNORE_PATTERNS="node_modules|dist|build|\.git|__pycache__|\.lock"
scripts/indexer.sh:76:    # Ê£ÄÊü• ast-delta.sh ËÑöÊú¨ÊòØÂê¶Â≠òÂú®‰∏îÂèØÊâßË°å
scripts/indexer.sh:77:    if [[ -x "$SCRIPT_DIR/ast-delta.sh" ]]; then
scripts/indexer.sh:78:        # Ë∞ÉÁî® ast-delta.sh status Ê£ÄÊü•
scripts/indexer.sh:79:        if "$SCRIPT_DIR/ast-delta.sh" status --format json 2>/dev/null | grep -q '"status"'; then
scripts/indexer.sh:106:    local db_path="${GRAPH_DB_PATH:-.devbooks/graph.db}"
scripts/indexer.sh:115:clear_ast_cache() {
scripts/indexer.sh:116:    local cache_dir="${DEVBOOKS_DIR:-.devbooks}/ast-cache"
scripts/indexer.sh:117:    if [[ -d "$cache_dir" ]]; then
scripts/indexer.sh:118:        rm -rf "$cache_dir"/*
scripts/indexer.sh:161:    local cache_version db_version
scripts/indexer.sh:162:    cache_version=$(read_version_stamp "$VERSION_STAMP_FILE")
scripts/indexer.sh:165:    if [[ -n "$cache_version" && -n "$db_version" && "$cache_version" != "$db_version" ]]; then
scripts/indexer.sh:166:        clear_ast_cache
scripts/indexer.sh:168:        DISPATCH_REASON="cache_version_mismatch"
scripts/indexer.sh:225:        "$SCRIPT_DIR/ast-delta.sh" update "${files[0]}" 2>&1
scripts/indexer.sh:230:        "$SCRIPT_DIR/ast-delta.sh" batch --files "$files_csv" 2>&1
scripts/indexer.sh:233:    local exit_code=$?
scripts/indexer.sh:235:    if [[ $exit_code -eq 0 ]]; then
scripts/indexer.sh:242:    return $exit_code
scripts/indexer.sh:271:    if ! "$SCRIPT_DIR/scip-to-graph.sh" parse --incremental --format json 2>&1; then
scripts/indexer.sh:282:    clear_ast_cache
scripts/indexer.sh:325:        local since_last=$((now - LAST_CHANGE_TIME))
scripts/indexer.sh:327:        if [[ $since_last -ge $DEBOUNCE_SECONDS ]]; then
scripts/indexer.sh:384:            if command -v scip-typescript &>/dev/null; then
scripts/indexer.sh:385:                echo "scip-typescript index --output index.scip"
scripts/indexer.sh:391:            if command -v scip-python &>/dev/null; then
scripts/indexer.sh:392:                echo "scip-python index . --output index.scip"
scripts/indexer.sh:398:            if command -v scip-go &>/dev/null; then
scripts/indexer.sh:399:                echo "scip-go --output index.scip"
scripts/indexer.sh:432:    local last_index=0
scripts/indexer.sh:442:        local since_last=$((now - last_index))
scripts/indexer.sh:445:        if [[ $since_last -lt $INDEX_INTERVAL ]]; then
scripts/indexer.sh:454:            last_index=$(date +%s)
scripts/indexer.sh:462:    local last_index=0
scripts/indexer.sh:477:        local since_last=$((now - last_index))
scripts/indexer.sh:479:        if [[ $since_last -lt $INDEX_INTERVAL ]]; then
scripts/indexer.sh:488:            last_index=$(date +%s)
scripts/indexer.sh:501:        local index_file="$dir/index.scip"
scripts/indexer.sh:550:        --arg ast_delta_enabled "$AST_DELTA_ENABLED" \
scripts/indexer.sh:554:            ast_delta_enabled: ($ast_delta_enabled == "true"),
scripts/indexer.sh:560:    local daemon_running=false
scripts/indexer.sh:561:    if launchctl list 2>/dev/null | grep -q "com.devbooks.indexer"; then
scripts/indexer.sh:562:        daemon_running=true
scripts/indexer.sh:566:    local cache_version db_version version_match
scripts/indexer.sh:567:    cache_version=$(read_version_stamp "$VERSION_STAMP_FILE")
scripts/indexer.sh:569:    if [[ -n "$cache_version" && "$cache_version" == "$db_version" ]]; then
scripts/indexer.sh:571:    elif [[ -z "$cache_version" && -z "$db_version" ]]; then
scripts/indexer.sh:580:            --argjson daemon_running "$daemon_running" \
scripts/indexer.sh:582:            --arg cache_version "${cache_version:-null}" \
scripts/indexer.sh:586:                daemon_running: $daemon_running,
scripts/indexer.sh:588:                    cache: $cache_version,
scripts/indexer.sh:597:        echo "    ast_delta.enabled: $AST_DELTA_ENABLED"
scripts/indexer.sh:598:        echo "    ast_delta.file_threshold: $FILE_THRESHOLD"
scripts/indexer.sh:599:        echo "    indexer.debounce_seconds: $DEBOUNCE_SECONDS"
scripts/indexer.sh:601:        echo "  ÂÆàÊä§ËøõÁ®ã (daemon): $([ "$daemon_running" = true ] && echo "running ËøêË°å‰∏≠" || echo "not running Êú™ËøêË°å")"
scripts/indexer.sh:604:        echo "    ÁºìÂ≠ò: ${cache_version:-Êú™ËÆæÁΩÆ}"
scripts/indexer.sh:679:    if [[ ! -f "$project_dir/index.scip" ]]; then
scripts/indexer.sh:710:  indexer.sh [È°πÁõÆÁõÆÂΩï]                      ÂêØÂä®ÂÆàÊä§ËøõÁ®ã
scripts/indexer.sh:711:  indexer.sh --status [--format json]        Ê£ÄÊü•Áä∂ÊÄÅ
scripts/indexer.sh:712:  indexer.sh --dry-run --files <files>       Ê®°ÊãüË∞ÉÂ∫¶ÂÜ≥Á≠ñÔºà‰∏çÊâßË°åÔºâ
scripts/indexer.sh:713:  indexer.sh --once --files <files>          ‰∏ÄÊ¨°ÊÄßÊâßË°åÁ¥¢Âºï
scripts/indexer.sh:714:  indexer.sh --install [È°πÁõÆÁõÆÂΩï]            ÂÆâË£Ö‰∏∫ LaunchAgent (macOS)
scripts/indexer.sh:715:  indexer.sh --uninstall                     Âç∏ËΩΩ LaunchAgent
scripts/indexer.sh:716:  indexer.sh --help                          ÊòæÁ§∫Â∏ÆÂä©
scripts/indexer.sh:719:  1. ÂäüËÉΩÂºÄÂÖ≥Ê£ÄÊü•ÔºàCI_AST_DELTA_ENABLED Êàñ features.ast_delta.enabledÔºâ
scripts/indexer.sh:735:  - scip-typescript / scip-python / scip-go Áî®‰∫éÁîüÊàêÁ¥¢Âºï
scripts/indexer.sh:736:  - ast-delta.sh Áî®‰∫éÂ¢ûÈáèÁ¥¢Âºï
scripts/indexer.sh:743:    local plist_path="$HOME/Library/LaunchAgents/com.devbooks.indexer.plist"
scripts/indexer.sh:749:<?xml version="1.0" encoding="UTF-8"?>
scripts/indexer.sh:754:    <string>com.devbooks.indexer</string>
scripts/indexer.sh:765:    <string>/tmp/devbooks-indexer.log</string>
scripts/indexer.sh:767:    <string>/tmp/devbooks-indexer.log</string>
scripts/indexer.sh:774:    log_info "Êó•Âøó: /tmp/devbooks-indexer.log"
scripts/indexer.sh:830:            launchctl unload "$HOME/Library/LaunchAgents/com.devbooks.indexer.plist" 2>/dev/null || true
scripts/indexer.sh:831:            rm -f "$HOME/Library/LaunchAgents/com.devbooks.indexer.plist"
scripts/semantic-anomaly.sh:18:#   semantic-anomaly.sh <path>                 Ê£ÄÊµãÊåáÂÆöË∑ØÂæÑ
scripts/semantic-anomaly.sh:19:#   semantic-anomaly.sh --pattern <file> <path>  ‰ΩøÁî®Ëá™ÂÆö‰πâÊ®°ÂºèÊñá‰ª∂
scripts/semantic-anomaly.sh:20:#   semantic-anomaly.sh --output json <path>   ÊåáÂÆöËæìÂá∫Ê†ºÂºè
scripts/semantic-anomaly.sh:21:#   semantic-anomaly.sh --threshold 0.8 <path> ËÆæÁΩÆÁΩÆ‰ø°Â∫¶ÈòàÂÄº
scripts/semantic-anomaly.sh:63:  if ! is_feature_enabled "semantic_anomaly"; then
scripts/semantic-anomaly.sh:64:    log_warn "ËØ≠‰πâÂºÇÂ∏∏Ê£ÄÊµãÂäüËÉΩÂ∑≤Á¶ÅÁî® (features.semantic_anomaly: false)"
scripts/semantic-anomaly.sh:101:  semantic-anomaly.sh [ÈÄâÈ°π] <Ë∑ØÂæÑ>
scripts/semantic-anomaly.sh:141:  semantic-anomaly.sh src/api.ts
scripts/semantic-anomaly.sh:144:  semantic-anomaly.sh src/
scripts/semantic-anomaly.sh:147:  semantic-anomaly.sh --pattern my-patterns.json src/
scripts/semantic-anomaly.sh:150:  semantic-anomaly.sh --threshold 0.9 src/
scripts/semantic-anomaly.sh:156:  echo "semantic-anomaly.sh version 1.0.0"
scripts/semantic-anomaly.sh:653:    console_lines=$(echo "$content" | grep -n 'console\.\(log\|warn\|error\|info\|debug\)' 2>/dev/null | cut -d: -f1 || true)
scripts/semantic-anomaly.sh:822:      has_log=$(echo "$func_body" | grep -c '\(console\.\|logger\.\|log\.\)\(info\|warn\|error\|debug\|log\)' 2>/dev/null || true)
scripts/semantic-anomaly.sh:932:      local code_only
scripts/semantic-anomaly.sh:933:      code_only=$(echo "$remaining_content" | grep -v '^\s*//' | sed 's|//.*||g')
scripts/semantic-anomaly.sh:937:      usage_count=$(echo "$code_only" | grep -cw "${name}" 2>/dev/null || true)
scripts/impact-analyzer.sh:2:# impact-analyzer.sh - ‰º†ÈÄíÊÄßÂΩ±ÂìçÂàÜÊûêÊ®°Âùó
scripts/impact-analyzer.sh:10:#   Impact(node, depth) = base_impact √ó (decay_factor ^ depth)
scripts/impact-analyzer.sh:13:#   GRAPH_DB_PATH - Êï∞ÊçÆÂ∫ìË∑ØÂæÑÔºåÈªòËÆ§ .devbooks/graph.db
scripts/impact-analyzer.sh:23:LOG_PREFIX="impact-analyzer"
scripts/impact-analyzer.sh:29:: "${GRAPH_DB_PATH:=$DEVBOOKS_DIR/graph.db}"
scripts/impact-analyzer.sh:40:    _config_depth=$(awk '/impact_analyzer:/,/^[^ ]/ { if (/max_depth:/) { gsub(/.*:[ ]*/, ""); gsub(/[[:space:]]/, ""); print; exit } }' "$FEATURES_CONFIG" 2>/dev/null)
scripts/impact-analyzer.sh:41:    _config_decay=$(awk '/impact_analyzer:/,/^[^ ]/ { if (/decay_factor:/) { gsub(/.*:[ ]*/, ""); gsub(/[[:space:]]/, ""); print; exit } }' "$FEATURES_CONFIG" 2>/dev/null)
scripts/impact-analyzer.sh:42:    _config_threshold=$(awk '/impact_analyzer:/,/^[^ ]/ { if (/threshold:/) { gsub(/.*:[ ]*/, ""); gsub(/[[:space:]]/, ""); print; exit } }' "$FEATURES_CONFIG" 2>/dev/null)
scripts/impact-analyzer.sh:55:        log_info "Run 'graph-store.sh init' to initialize the database"
scripts/impact-analyzer.sh:145:bfs_impact_analysis() {
scripts/impact-analyzer.sh:159:    # Ê†ºÂºè: symbol_id|depth|impact
scripts/impact-analyzer.sh:173:        local node_id depth impact
scripts/impact-analyzer.sh:176:        impact=$(echo "$current" | cut -d'|' -f3)
scripts/impact-analyzer.sh:187:        if [[ "$node_id" != "$start_symbol" ]] && float_gte "$impact" "$threshold"; then
scripts/impact-analyzer.sh:206:                                            --argjson impact "$impact" \
scripts/impact-analyzer.sh:207:                    '. + [{"id": $id, "symbol": $sym, "kind": $kind, "file_path": $file, "depth": $depth, "confidence": ($impact | tonumber | . * 1000 | round / 1000)}]' \
scripts/impact-analyzer.sh:216:            local new_impact
scripts/impact-analyzer.sh:217:            new_impact=$(float_mul "$impact" "$decay_factor")
scripts/impact-analyzer.sh:220:            if float_gte "$new_impact" "$threshold"; then
scripts/impact-analyzer.sh:230:                        echo "${ds_id}|${new_depth}|${new_impact}" >> "$queue_file"
scripts/impact-analyzer.sh:249:bfs_reverse_impact_analysis() {
scripts/impact-analyzer.sh:274:        local node_id depth impact
scripts/impact-analyzer.sh:277:        impact=$(echo "$current" | cut -d'|' -f3)
scripts/impact-analyzer.sh:285:        if [[ "$node_id" != "$start_symbol" ]] && float_gte "$impact" "$threshold"; then
scripts/impact-analyzer.sh:301:                                            --argjson impact "$impact" \
scripts/impact-analyzer.sh:302:                    '. + [{"id": $id, "symbol": $sym, "kind": $kind, "file_path": $file, "depth": $depth, "confidence": ($impact | tonumber | . * 1000 | round / 1000)}]' \
scripts/impact-analyzer.sh:310:            local new_impact
scripts/impact-analyzer.sh:311:            new_impact=$(float_mul "$impact" "$decay_factor")
scripts/impact-analyzer.sh:313:            if float_gte "$new_impact" "$threshold"; then
scripts/impact-analyzer.sh:322:                        echo "${us_id}|${new_depth}|${new_impact}" >> "$queue_file"
scripts/impact-analyzer.sh:377:        echo "Usage: impact-analyzer.sh analyze <symbol> [--depth <n>] [--threshold <t>] [--format json|md|mermaid]" >&2
scripts/impact-analyzer.sh:398:    affected_nodes=$(bfs_impact_analysis "$symbol" "$depth" "$decay" "$threshold")
scripts/impact-analyzer.sh:480:        echo "Usage: impact-analyzer.sh file <file-path> [--depth <n>] [--threshold <t>]" >&2
scripts/impact-analyzer.sh:515:        affected=$(bfs_reverse_impact_analysis "$sym_id" "$depth" "$decay" "$threshold")
scripts/impact-analyzer.sh:580:    echo "graph TD"
scripts/impact-analyzer.sh:637:impact-analyzer.sh - ‰º†ÈÄíÊÄßÂΩ±ÂìçÂàÜÊûê
scripts/impact-analyzer.sh:640:    impact-analyzer.sh <command> [options]
scripts/impact-analyzer.sh:659:    Impact(node, depth) = base_impact √ó (decay_factor ^ depth)
scripts/impact-analyzer.sh:670:    GRAPH_DB_PATH           Êï∞ÊçÆÂ∫ìË∑ØÂæÑÔºàÈªòËÆ§: .devbooks/graph.dbÔºâ
scripts/impact-analyzer.sh:674:    impact-analyzer.sh analyze "sym:func:handleToolCall" --depth 3
scripts/impact-analyzer.sh:677:    impact-analyzer.sh file "src/server.ts" --depth 3
scripts/impact-analyzer.sh:680:    impact-analyzer.sh analyze "sym:func:main" --threshold 0.5
scripts/impact-analyzer.sh:683:    impact-analyzer.sh analyze "sym:func:main" --format mermaid
scripts/common.sh:36:# Áî®Ê≥ï: check_dependency "jq" || exit 2
scripts/common.sh:37:check_dependency() {
scripts/common.sh:52:    if ! check_dependency "$cmd"; then
scripts/common.sh:66:# Áî®Ê≥ï: check_optional_dependency "bc" "ÊµÆÁÇπËøêÁÆóÂ∞Ü‰ΩøÁî® awk Êõø‰ª£"
scripts/common.sh:67:check_optional_dependency() {
scripts/common.sh:71:  if ! check_dependency "$cmd"; then
scripts/common.sh:87:  if check_dependency "bc"; then
scripts/common.sh:114:is_bug_fix_message() {
scripts/common.sh:120:     [[ "$msg_lower" =~ (bug|issue|error|crash|broken|fail) ]]; then
scripts/common.sh:140:CODE_INTENT_PATTERN='‰øÆÂ§ç|fix|bug|ÈîôËØØ|ÈáçÊûÑ|refactor|‰ºòÂåñ|Ê∑ªÂä†|Êñ∞Â¢û|ÂÆûÁé∞|implement|Âà†Èô§|remove|‰øÆÊîπ|update|change|ÂàÜÊûê|analyze|ÂΩ±Âìç|impact|ÂºïÁî®|reference|Ë∞ÉÁî®|call|‰æùËµñ|depend|ÂáΩÊï∞|function|ÊñπÊ≥ï|method|Á±ª|class|Ê®°Âùó|module|\.ts|\.tsx|\.js|\.py|\.go|src/|lib/'
scripts/common.sh:146:# ‰ºòÂÖàÁ∫ß: debug > refactor > docs > feature (default)
scripts/common.sh:147:INTENT_DEBUG_PATTERN='fix|debug|bug|crash|fail|error|issue|resolve|problem|broken'
scripts/common.sh:154:is_code_intent() {
scripts/common.sh:159:  # ‰ΩøÁî®Êñ∞ÁöÑÂõõÂàÜÁ±ªÔºödebug/refactor/feature ÈÉΩÊòØ‰ª£Á†ÅÊÑèÂõæÔºådocs ‰∏çÊòØ
scripts/common.sh:160:  local intent_type
scripts/common.sh:161:  intent_type=$(get_intent_type "$input")
scripts/common.sh:163:  case "$intent_type" in
scripts/common.sh:164:    debug|refactor|feature)
scripts/common.sh:180:is_non_code() {
scripts/common.sh:186:# ËøîÂõû: debug | refactor | docs | feature
scripts/common.sh:187:# ‰ºòÂÖàÁ∫ß: debug > refactor > docs > feature (default)
scripts/common.sh:188:get_intent_type() {
scripts/common.sh:202:  # 1. Debug Á±ªÔºàÊúÄÈ´ò‰ºòÂÖàÁ∫ßÔºâ
scripts/common.sh:204:    echo "debug"
scripts/common.sh:237:    "embedding.provider") echo "auto" ;;
scripts/common.sh:238:    "embedding.enabled") echo "true" ;;
scripts/common.sh:239:    "embedding.auto_build") echo "true" ;;
scripts/common.sh:240:    "embedding.fallback_to_keyword") echo "true" ;;
scripts/common.sh:241:    "embedding.ollama.model") echo "nomic-embed-text" ;;
scripts/common.sh:242:    "embedding.ollama.endpoint") echo "http://localhost:11434" ;;
scripts/common.sh:243:    "embedding.ollama.timeout") echo "30" ;;
scripts/common.sh:244:    "graph_rag.enabled") echo "true" ;;
scripts/common.sh:245:    "graph_rag.max_depth") echo "2" ;;
scripts/common.sh:246:    "graph_rag.token_budget") echo "8000" ;;
scripts/common.sh:247:    "graph_rag.top_k") echo "10" ;;
scripts/common.sh:248:    "graph_rag.ckb.enabled") echo "true" ;;
scripts/common.sh:249:    "graph_rag.ckb.fallback_to_import") echo "true" ;;
scripts/common.sh:250:    "features.complexity_weighted_hotspot") echo "true" ;;
scripts/common.sh:251:    "features.hotspot_limit") echo "5" ;;
scripts/common.sh:269:  # ‰æãÂ¶Ç "embedding.provider" -> Êü•Êâæ embedding: ‰∏ãÁöÑ provider:
scripts/common.sh:344:# ÂèÇÊï∞: $1 - ÈÖçÁΩÆË∑ØÂæÑÔºàÂ¶Ç "embedding.provider"Ôºâ
scripts/common.sh:383:get_hotspot_files() {
scripts/common.sh:409:# ÂèÇÊï∞: $1 - ÂäüËÉΩÂêçÁß∞ (Â¶Ç hotspot_analyzer)
scripts/common.sh:447:# ÂèÇÊï∞: $1 - ÂäüËÉΩÂêçÁß∞ (Â¶Ç hotspot_limit)
scripts/common.sh:501:  # Ëß£Êûê features.llm_rerank.<key> ÈÖçÁΩÆ
scripts/common.sh:504:    BEGIN { in_features = 0; in_llm_rerank = 0 }
scripts/common.sh:506:    /^[a-zA-Z]/ && !/^features:/ { in_features = 0; in_llm_rerank = 0 }
scripts/common.sh:507:    in_features && /llm_rerank:/ { in_llm_rerank = 1; next }
scripts/common.sh:508:    in_features && /^[[:space:]][[:space:]][a-zA-Z]/ && !/llm_rerank/ { in_llm_rerank = 0 }
scripts/common.sh:509:    in_llm_rerank && $0 ~ key {
scripts/common.sh:591:        return 124  # timeout exit code
scripts/common.sh:639:  local exit_code=$?
scripts/common.sh:640:  if [[ $exit_code -ne 0 ]]; then
scripts/common.sh:642:    return $exit_code
scripts/common.sh:678:  local exit_code=$?
scripts/common.sh:679:  if [[ $exit_code -eq 124 ]]; then
scripts/common.sh:720:  local exit_code=$?
scripts/common.sh:721:  if [[ $exit_code -eq 124 ]]; then
scripts/common.sh:758:  local exit_code=$?
scripts/common.sh:759:  if [[ $exit_code -eq 124 ]]; then
scripts/common.sh:986:        # ‰ΩøÁî® sed Â§ÑÁêÜÔºåÂõ†‰∏∫ bash Ê≠£ÂàôÂØπ Unicode ÊîØÊåÅ‰∏çÂ•Ω
scripts/graph-rag.sh:12:#   graph-rag-context.sh --query "Êü•ËØ¢ÂÜÖÂÆπ" [ÈÄâÈ°π]
scripts/graph-rag.sh:16:#   AC-007: graph_rag.enabled: false Êó∂Ë∑≥Ëøá
scripts/graph-rag.sh:70:BOUNDARY_DETECTOR="${SCRIPT_DIR}/boundary-detector.sh"
scripts/graph-rag.sh:73:CACHE_MANAGER="${SCRIPT_DIR}/cache-manager.sh"
scripts/graph-rag.sh:79:CACHE_DIR="${TMPDIR:-/tmp}/.devbooks-cache/graph-rag"
scripts/graph-rag.sh:129:  local exit_code=0
scripts/graph-rag.sh:131:    exit_code=$(cat "$pid_file.exit")
scripts/graph-rag.sh:137:  return $exit_code
scripts/graph-rag.sh:171:      local last_failure_time
scripts/graph-rag.sh:172:      last_failure_time=$(cat "$cooldown_file" 2>/dev/null || echo "0")
scripts/graph-rag.sh:175:      local elapsed=$((current_time - last_failure_time))
scripts/graph-rag.sh:215:  graph-rag-context.sh --query "Êü•ËØ¢ÂÜÖÂÆπ" [ÈÄâÈ°π]
scripts/graph-rag.sh:227:  --rerank              ÂêØÁî® LLM ÈáçÊéíÂ∫èÔºàÈªòËÆ§ÂÖ≥Èó≠Ôºâ
scripts/graph-rag.sh:230:  --mock-embedding      ‰ΩøÁî®Ê®°Êãü Embedding Êï∞ÊçÆÔºàÊµãËØïÁî®Ôºâ
scripts/graph-rag.sh:237:  graph-rag-context.sh --query "Áî®Êà∑ËÆ§ËØÅÁõ∏ÂÖ≥ÁöÑÂáΩÊï∞"
scripts/graph-rag.sh:240:  graph-rag-context.sh --query "Â§ÑÁêÜÊîØ‰ªòÁöÑ‰ª£Á†Å" --top-k 20 --max-depth 3
scripts/graph-rag.sh:243:  graph-rag-context.sh --query "ÈîôËØØÂ§ÑÁêÜ" --format json
scripts/graph-rag.sh:248:    "source": "graph-rag",
scripts/graph-rag.sh:250:    "subgraph": {
scripts/graph-rag.sh:276:      "graph_depth": 3,
scripts/graph-rag.sh:277:      "boundary_filtered": 5
scripts/graph-rag.sh:285:  echo "graph-rag-context.sh version 1.0.0"
scripts/graph-rag.sh:333:      --rerank)
scripts/graph-rag.sh:342:      --mock-embedding)
scripts/graph-rag.sh:386:    echo '{"error":"invalid fusion-depth: must be integer 0-5","code":"INVALID_PARAM"}'
scripts/graph-rag.sh:390:    echo '{"error":"invalid fusion-depth: cannot be negative","code":"INVALID_PARAM"}'
scripts/graph-rag.sh:457:        elif [[ "$line" =~ ^[[:space:]]+hotspot:[[:space:]]*([0-9.]+) ]]; then
scripts/graph-rag.sh:469:get_cache_key() {
scripts/graph-rag.sh:485:# ÈÄâÊã©‰∏Ä‰∏™Â≠òÂú®ÁöÑÁºìÂ≠òÈîöÁÇπÊñá‰ª∂ÔºàÁî®‰∫é cache-manager Ê†°È™åÔºâ
scripts/graph-rag.sh:486:resolve_graph_rag_cache_anchor() {
scripts/graph-rag.sh:493:    "$SCRIPT_DIR/graph-rag.sh"
scripts/graph-rag.sh:506:# ‰ΩøÁî® cache-manager.sh Ëé∑ÂèñÁºìÂ≠òÔºà‰ºòÂÖàÔºâ
scripts/graph-rag.sh:507:get_cached() {
scripts/graph-rag.sh:508:  local cache_key="$1"
scripts/graph-rag.sh:510:  query_hash=$(get_cache_key "$cache_key")
scripts/graph-rag.sh:512:  # MP5.2: ‰ºòÂÖà‰ΩøÁî® cache-manager.sh
scripts/graph-rag.sh:514:    local cache_anchor
scripts/graph-rag.sh:515:    cache_anchor=$(resolve_graph_rag_cache_anchor "$CWD") || cache_anchor=""
scripts/graph-rag.sh:517:    local cache_result
scripts/graph-rag.sh:518:    if [[ -n "$cache_anchor" ]]; then
scripts/graph-rag.sh:519:      cache_result=$("$CACHE_MANAGER" --get "$cache_anchor" --query "$query_hash" 2>/dev/null)
scripts/graph-rag.sh:521:      cache_result=""
scripts/graph-rag.sh:524:    if [[ -n "$cache_result" ]] && echo "$cache_result" | jq -e '.schema_version' &>/dev/null; then
scripts/graph-rag.sh:525:      log_info "ÁºìÂ≠òÂëΩ‰∏≠ (cache-manager, key: ${query_hash:0:8}...)"
scripts/graph-rag.sh:526:      echo "$cache_result"
scripts/graph-rag.sh:532:  local cache_file="$CACHE_DIR/$query_hash"
scripts/graph-rag.sh:534:  if [ -f "$cache_file" ]; then
scripts/graph-rag.sh:537:    mtime=$(stat -f %m "$cache_file" 2>/dev/null || stat -c %Y "$cache_file" 2>/dev/null || echo 0)
scripts/graph-rag.sh:540:      cat "$cache_file"
scripts/graph-rag.sh:547:# ‰ΩøÁî® cache-manager.sh ËÆæÁΩÆÁºìÂ≠òÔºà‰ºòÂÖàÔºâ
scripts/graph-rag.sh:548:set_cache() {
scripts/graph-rag.sh:549:  local cache_key="$1"
scripts/graph-rag.sh:552:  query_hash=$(get_cache_key "$cache_key")
scripts/graph-rag.sh:554:  # MP5.2: ‰ºòÂÖà‰ΩøÁî® cache-manager.sh
scripts/graph-rag.sh:556:    local cache_anchor
scripts/graph-rag.sh:557:    cache_anchor=$(resolve_graph_rag_cache_anchor "$CWD") || cache_anchor=""
scripts/graph-rag.sh:558:    if [[ -n "$cache_anchor" ]]; then
scripts/graph-rag.sh:559:      "$CACHE_MANAGER" --set "$cache_anchor" --query "$query_hash" --value "$value" 2>/dev/null || true
scripts/graph-rag.sh:576:is_library_code() {
scripts/graph-rag.sh:594:      local boundary_type
scripts/graph-rag.sh:595:      boundary_type=$(echo "$result" | jq -r '.type // "user"' 2>/dev/null)
scripts/graph-rag.sh:597:      case "$boundary_type" in
scripts/graph-rag.sh:609:filter_library_code() {
scripts/graph-rag.sh:622:    if ! is_library_code "$file_path"; then
scripts/graph-rag.sh:641:_is_llm_rerank_enabled() {
scripts/graph-rag.sh:649:  # Ëß£Êûê features.llm_rerank.enabled
scripts/graph-rag.sh:652:    BEGIN { in_features = 0; in_llm_rerank = 0 }
scripts/graph-rag.sh:654:    /^[a-zA-Z]/ && !/^features:/ { in_features = 0; in_llm_rerank = 0 }
scripts/graph-rag.sh:655:    in_features && /llm_rerank:/ { in_llm_rerank = 1; next }
scripts/graph-rag.sh:656:    in_features && /^[[:space:]][[:space:]][a-zA-Z]/ && !/llm_rerank/ { in_llm_rerank = 0 }
scripts/graph-rag.sh:657:    in_llm_rerank && /enabled:/ {
scripts/graph-rag.sh:677:_build_rerank_prompt() {
scripts/graph-rag.sh:699:You are a code relevance ranker. Given a query and a list of code file candidates,
scripts/graph-rag.sh:716:_parse_rerank_response() {
scripts/graph-rag.sh:737:  local reranked='[]'
scripts/graph-rag.sh:761:      reranked=$(echo "$reranked" | jq --argjson c "$candidate" '. + [$c]')
scripts/graph-rag.sh:765:  echo "$reranked"
scripts/graph-rag.sh:770:llm_rerank_candidates() {
scripts/graph-rag.sh:782:  _write_rerank_state() {
scripts/graph-rag.sh:794:  if ! _is_llm_rerank_enabled; then
scripts/graph-rag.sh:796:    _write_rerank_state
scripts/graph-rag.sh:809:    _write_rerank_state
scripts/graph-rag.sh:820:    _write_rerank_state
scripts/graph-rag.sh:827:  prompt=$(_build_rerank_prompt "$query" "$candidates_json")
scripts/graph-rag.sh:837:  local exit_code
scripts/graph-rag.sh:838:  local last_error=""
scripts/graph-rag.sh:848:    exit_code=$?
scripts/graph-rag.sh:851:    if [[ $exit_code -eq 124 ]]; then
scripts/graph-rag.sh:852:      last_error="timeout"
scripts/graph-rag.sh:858:    if [[ $exit_code -ne 0 ]] || echo "$response" | jq -e '.error' &>/dev/null; then
scripts/graph-rag.sh:859:      last_error=$(echo "$response" | jq -r '.error // "unknown error"' 2>/dev/null || echo "llm_error")
scripts/graph-rag.sh:866:      last_error="invalid_json"
scripts/graph-rag.sh:873:      last_error="invalid_format"
scripts/graph-rag.sh:879:    local reranked
scripts/graph-rag.sh:880:    reranked=$(_parse_rerank_response "$response" "$candidates_json")
scripts/graph-rag.sh:882:    if [[ $? -eq 0 ]] && [[ -n "$reranked" ]] && [[ "$reranked" != "[]" ]]; then
scripts/graph-rag.sh:884:      _write_rerank_state
scripts/graph-rag.sh:885:      echo "$reranked"
scripts/graph-rag.sh:888:      last_error="parse_error"
scripts/graph-rag.sh:895:  RERANK_RESULT_FALLBACK_REASON="${last_error:-max_retries_exhausted}"
scripts/graph-rag.sh:896:  _write_rerank_state
scripts/graph-rag.sh:905:embedding_search() {
scripts/graph-rag.sh:908:  local embedding_tool="${SCRIPT_DIR}/devbooks-embedding.sh"
scripts/graph-rag.sh:909:  local index_path="$CWD/.devbooks/embeddings/index.tsv"
scripts/graph-rag.sh:914:    echo '[{"file_path":"src/auth.ts","relevance_score":0.8,"hotspot":0.6,"distance":2},{"file_path":"src/user.ts","relevance_score":0.6,"hotspot":0.4,"distance":3}]'
scripts/graph-rag.sh:930:  # Ë∞ÉÁî® embedding Â∑•ÂÖ∑ËøõË°åÊêúÁ¥¢
scripts/graph-rag.sh:931:  if [ -x "$embedding_tool" ]; then
scripts/graph-rag.sh:933:    result=$(cd "$CWD" && PROJECT_ROOT="$CWD" "$embedding_tool" search "$query" --top-k "$top_k" 2>/dev/null)
scripts/graph-rag.sh:1000:ckb_graph_traverse() {
scripts/graph-rag.sh:1039:  local graph_results='[]'
scripts/graph-rag.sh:1053:        graph_results=$(echo "$graph_results" | jq --arg path "$import_path" '. + [{file_path: $path, depth: 1, source: "import"}]')
scripts/graph-rag.sh:1058:  echo "$graph_results"
scripts/graph-rag.sh:1104:# ÂÖ¨ÂºèÔºöPriority = relevance √ó 0.4 + hotspot √ó 0.3 + (1/distance) √ó 0.3
scripts/graph-rag.sh:1111:  local relevance hotspot distance
scripts/graph-rag.sh:1115:  hotspot=$(echo "$candidate_json" | jq -r '.hotspot // 0')
scripts/graph-rag.sh:1123:  # ËÆ°ÁÆó‰ºòÂÖàÁ∫ßÔºöPriority = relevance √ó 0.4 + hotspot √ó 0.3 + (1/distance) √ó 0.3
scripts/graph-rag.sh:1124:  awk -v r="$relevance" -v h="$hotspot" -v d="$distance" \
scripts/graph-rag.sh:1151:        hotspot: (.hotspot // 0),
scripts/graph-rag.sh:1272:build_subgraph() {
scripts/graph-rag.sh:1352:  local cache_key="graph-rag:$CWD:$query:$TOP_K:$MAX_DEPTH:$TOKEN_BUDGET:$CKB_AVAILABLE:$LEGACY_MODE:$RERANK_ENABLED"
scripts/graph-rag.sh:1353:  local cached
scripts/graph-rag.sh:1354:  cached=$(get_cached "$cache_key")
scripts/graph-rag.sh:1355:  if [ -n "$cached" ]; then
scripts/graph-rag.sh:1356:    echo "$cached"
scripts/graph-rag.sh:1378:      anchors=$(embedding_search "$query" "$TOP_K")
scripts/graph-rag.sh:1401:    anchors=$(embedding_search "$query" "$TOP_K")
scripts/graph-rag.sh:1426:  candidates=$(filter_library_code "$candidates")
scripts/graph-rag.sh:1443:    local rerank_state_file
scripts/graph-rag.sh:1444:    rerank_state_file=$(mktemp)
scripts/graph-rag.sh:1445:    candidates=$(llm_rerank_candidates "$query" "$candidates" "$rerank_state_file")
scripts/graph-rag.sh:1446:    if [[ -f "$rerank_state_file" ]]; then
scripts/graph-rag.sh:1448:      source "$rerank_state_file"
scripts/graph-rag.sh:1449:      rm -f "$rerank_state_file"
scripts/graph-rag.sh:1483:  local subgraph='{"nodes":[],"edges":[]}'
scripts/graph-rag.sh:1485:    subgraph=$(build_subgraph "$trimmed")
scripts/graph-rag.sh:1488:  # ÊûÑÂª∫ÁªìÊûúÔºàÂåÖÂê´ metadata Âíå subgraphÔºâ
scripts/graph-rag.sh:1497:    --arg source "graph-rag" \
scripts/graph-rag.sh:1499:    --argjson subgraph "$subgraph" \
scripts/graph-rag.sh:1502:    --argjson graph_depth "$MAX_DEPTH" \
scripts/graph-rag.sh:1504:    --argjson boundary_filtered "$BOUNDARY_FILTERED_COUNT" \
scripts/graph-rag.sh:1506:    --argjson reranked "$RERANK_RESULT_RERANKED" \
scripts/graph-rag.sh:1515:      subgraph: $subgraph,
scripts/graph-rag.sh:1521:        graph_depth: $graph_depth,
scripts/graph-rag.sh:1523:        boundary_filtered: $boundary_filtered,
scripts/graph-rag.sh:1525:        reranked: $reranked,
scripts/graph-rag.sh:1533:  set_cache "$cache_key" "$result"
scripts/graph-rag.sh:1621:  # ÈôçÁ∫ßÔºöËøîÂõûÁ©∫Êï∞ÁªÑÔºåËÆ©Ë∞ÉÁî®ËÄÖ‰ΩøÁî® embedding ÊêúÁ¥¢
scripts/graph-rag.sh:1662:      local graph_nodes
scripts/graph-rag.sh:1663:      graph_nodes=$(ckb_graph_traverse "$file_path" "$max_depth")
scripts/graph-rag.sh:1665:      if [ -n "$graph_nodes" ] && [ "$graph_nodes" != "[]" ]; then
scripts/graph-rag.sh:1667:        graph_nodes=$(echo "$graph_nodes" | jq '[.[] | . + {source: "import"}]')
scripts/graph-rag.sh:1668:        all_candidates=$(echo "$all_candidates" "$graph_nodes" | jq -s 'add | unique_by(.file_path)')
scripts/graph-rag.sh:1692:    edge_count=$(echo "$result" | jq '.subgraph.edges | length')
scripts/graph-rag.sh:1694:    filtered_count=$(echo "$result" | jq '.metadata.boundary_filtered // 0')
scripts/graph-rag.sh:1706:        edge=$(echo "$result" | jq ".subgraph.edges[$j]")
scripts/ast-delta.sh:2:# ast-delta.sh - AST Delta Â¢ûÈáèÁ¥¢ÂºïÂçèË∞ÉËÑöÊú¨
scripts/ast-delta.sh:12:#   clear-cache            - Ê∏ÖÁêÜ AST ÁºìÂ≠ò
scripts/ast-delta.sh:31:LOG_PREFIX="ast-delta"
scripts/ast-delta.sh:37:: "${GRAPH_DB_PATH:=$DEVBOOKS_DIR/graph.db}"
scripts/ast-delta.sh:38:: "${AST_CACHE_DIR:=$DEVBOOKS_DIR/ast-cache}"
scripts/ast-delta.sh:82:        temp_files=$(find "$DEVBOOKS_DIR" -name ".ast-delta-temp-*.tmp" 2>/dev/null || true)
scripts/ast-delta.sh:84:        temp_files=$(find "$DEVBOOKS_DIR" -name ".ast-delta-temp-*.tmp" -mmin +"$min_age" 2>/dev/null || true)
scripts/ast-delta.sh:93:get_cache_path() {
scripts/ast-delta.sh:98:    echo "$AST_CACHE_DIR/${safe_path}.ast"
scripts/ast-delta.sh:175:    sqlite3 "$GRAPH_DB_PATH" "SELECT value FROM metadata WHERE key = 'ast_cache_version';" 2>/dev/null || echo ""
scripts/ast-delta.sh:189:    sqlite3 "$GRAPH_DB_PATH" "INSERT OR REPLACE INTO metadata (key, value) VALUES ('ast_cache_version', '$version');" 2>/dev/null
scripts/ast-delta.sh:197:    local scip_mtime=""
scripts/ast-delta.sh:198:    if [[ -f "index.scip" ]]; then
scripts/ast-delta.sh:199:        scip_mtime=$(stat -c %Y "index.scip" 2>/dev/null || stat -f %m "index.scip" 2>/dev/null || echo "")
scripts/ast-delta.sh:203:    file_count=$(find "$AST_CACHE_DIR" -name "*.ast" 2>/dev/null | wc -l | tr -d ' ')
scripts/ast-delta.sh:209:    "scip_mtime": "$scip_mtime",
scripts/ast-delta.sh:216:atomic_write_cache() {
scripts/ast-delta.sh:219:    local cache_file
scripts/ast-delta.sh:220:    cache_file=$(get_cache_path "$file_path")
scripts/ast-delta.sh:221:    local tmp_file="$cache_file.tmp.$$"
scripts/ast-delta.sh:224:    mkdir -p "$(dirname "$cache_file")"
scripts/ast-delta.sh:236:    mv "$tmp_file" "$cache_file"
scripts/ast-delta.sh:266:        const code = fs.readFileSync(filePath, "utf-8");
scripts/ast-delta.sh:268:        // Â∞ùËØïÂä†ËΩΩ ast-delta Ê®°Âùó
scripts/ast-delta.sh:269:        let astDelta;
scripts/ast-delta.sh:272:            astDelta = require(path.join(process.cwd(), "dist", "ast-delta.js"));
scripts/ast-delta.sh:277:                astDelta = require(path.join(process.cwd(), "src", "ast-delta.ts"));
scripts/ast-delta.sh:284:                const tree = parser.parse(code);
scripts/ast-delta.sh:289:                    endLine: code.split("\n").length,
scripts/ast-delta.sh:297:        const result = astDelta.parseTypeScript(code, { filePath: filePath });
scripts/ast-delta.sh:362:    local cache_version
scripts/ast-delta.sh:363:    cache_version=$(cat "$VERSION_STAMP_FILE" 2>/dev/null | grep -o '"timestamp"[[:space:]]*:[[:space:]]*"[^"]*"' | head -1 | sed 's/.*"\([^"]*\)"$/\1/')
scripts/ast-delta.sh:368:    if [[ -n "$db_version" ]] && [[ "$cache_version" != "$db_version" ]]; then
scripts/ast-delta.sh:384:update_graph_db() {
scripts/ast-delta.sh:386:    local ast_json="$2"
scripts/ast-delta.sh:388:    # Ê£ÄÊü• graph-store.sh ÊòØÂê¶Â≠òÂú®
scripts/ast-delta.sh:389:    if [[ ! -x "$SCRIPT_DIR/graph-store.sh" ]]; then
scripts/ast-delta.sh:390:        log_warn "graph-store.sh not found, skipping graph update"
scripts/ast-delta.sh:395:    "$SCRIPT_DIR/graph-store.sh" init >/dev/null 2>&1 || true
scripts/ast-delta.sh:401:        nodes=$(echo "$ast_json" | jq -c 'recurse(.children[]?) | select(.name != null)' 2>/dev/null || true)
scripts/ast-delta.sh:414:                    "$SCRIPT_DIR/graph-store.sh" add-node \
scripts/ast-delta.sh:460:    # ÂÜÖËÅî cache_path ËÆ°ÁÆó‰ª•ÈÅøÂÖçÂ≠êËøõÁ®ãÂºÄÈîÄ
scripts/ast-delta.sh:462:    local cache_file="$AST_CACHE_DIR/${file_path//\//_}.ast"
scripts/ast-delta.sh:463:    if [[ "$force_rebuild" != "true" ]] && [[ -f "$cache_file" ]] && ! [[ "$file_path" -nt "$cache_file" ]]; then
scripts/ast-delta.sh:465:        local cache_version_ok=true
scripts/ast-delta.sh:467:            local cache_ts db_ts
scripts/ast-delta.sh:468:            cache_ts=$(grep -o '"timestamp"[[:space:]]*:[[:space:]]*"[^"]*"' "$VERSION_STAMP_FILE" 2>/dev/null | head -1 | sed 's/.*"\([^"]*\)"$/\1/')
scripts/ast-delta.sh:469:            db_ts=$(sqlite3 "$GRAPH_DB_PATH" "SELECT value FROM metadata WHERE key = 'ast_cache_version';" 2>/dev/null || echo "")
scripts/ast-delta.sh:470:            if [[ -n "$db_ts" ]] && [[ "$cache_ts" != "$db_ts" ]]; then
scripts/ast-delta.sh:471:                cache_version_ok=false
scripts/ast-delta.sh:475:        if [[ "$cache_version_ok" == "true" ]]; then
scripts/ast-delta.sh:477:            echo "{\"status\":\"success\",\"mode\":\"cache_hit\",\"state\":\"CACHE_HIT\",\"file\":\"$file_path\"}"
scripts/ast-delta.sh:495:    local ast_json=""
scripts/ast-delta.sh:503:            local cache_file
scripts/ast-delta.sh:504:            cache_file=$(get_cache_path "$file_path")
scripts/ast-delta.sh:505:            if [[ -f "$cache_file" ]] && ! [[ "$file_path" -nt "$cache_file" ]]; then
scripts/ast-delta.sh:507:                mode="cache_hit"
scripts/ast-delta.sh:508:                ast_json=$(cat "$cache_file")
scripts/ast-delta.sh:511:                ast_json=$(parse_file_with_node "$file_path" 2>/dev/null) || true
scripts/ast-delta.sh:512:                if [[ -z "$ast_json" || "$ast_json" == *'"error"'* ]]; then
scripts/ast-delta.sh:514:                    ast_json=$(parse_file_with_regex "$file_path")
scripts/ast-delta.sh:522:            log_info "FULL_REBUILD: cache invalidated or missing"
scripts/ast-delta.sh:526:            ast_json=$(parse_file_with_node "$file_path" 2>/dev/null) || true
scripts/ast-delta.sh:527:            if [[ -z "$ast_json" || "$ast_json" == *'"error"'* ]]; then
scripts/ast-delta.sh:528:                ast_json=$(parse_file_with_regex "$file_path")
scripts/ast-delta.sh:536:            local cache_file_fb
scripts/ast-delta.sh:537:            cache_file_fb=$(get_cache_path "$file_path")
scripts/ast-delta.sh:538:            if [[ -f "$cache_file_fb" ]] && ! [[ "$file_path" -nt "$cache_file_fb" ]]; then
scripts/ast-delta.sh:540:                mode="cache_hit"
scripts/ast-delta.sh:541:                ast_json=$(cat "$cache_file_fb")
scripts/ast-delta.sh:544:                ast_json=$(parse_file_with_regex "$file_path")
scripts/ast-delta.sh:550:    if [[ "$mode" != "cache_hit" ]]; then
scripts/ast-delta.sh:552:        if [[ -n "$ast_json" ]]; then
scripts/ast-delta.sh:553:            atomic_write_cache "$file_path" "$ast_json"
scripts/ast-delta.sh:557:        update_graph_db "$file_path" "$ast_json"
scripts/ast-delta.sh:651:            # Â¶ÇÊûúÊúâ scip-to-graph.shÔºå‰ΩøÁî®ÂÆÉËøõË°åÂÖ®ÈáèÈáçÂª∫
scripts/ast-delta.sh:652:            if [[ -x "$SCRIPT_DIR/scip-to-graph.sh" ]]; then
scripts/ast-delta.sh:653:                log_info "Using scip-to-graph.sh for full rebuild"
scripts/ast-delta.sh:654:                "$SCRIPT_DIR/scip-to-graph.sh" 2>/dev/null || true
scripts/ast-delta.sh:671:            # Â∞ùËØï‰ΩøÁî® scip-to-graph.sh
scripts/ast-delta.sh:672:            if [[ -x "$SCRIPT_DIR/scip-to-graph.sh" ]]; then
scripts/ast-delta.sh:673:                "$SCRIPT_DIR/scip-to-graph.sh" 2>/dev/null || true
scripts/ast-delta.sh:704:    local cache_file_count
scripts/ast-delta.sh:705:    cache_file_count=$(find "$AST_CACHE_DIR" -name "*.ast" 2>/dev/null | wc -l | tr -d ' ')
scripts/ast-delta.sh:707:    local cache_size_kb
scripts/ast-delta.sh:708:    cache_size_kb=$(du -sk "$AST_CACHE_DIR" 2>/dev/null | cut -f1 || echo "0")
scripts/ast-delta.sh:724:    "cache_dir": "$AST_CACHE_DIR",
scripts/ast-delta.sh:725:    "cache_file_count": $cache_file_count,
scripts/ast-delta.sh:726:    "cache_size_kb": $cache_size_kb,
scripts/ast-delta.sh:727:    "cache_max_size_mb": $AST_CACHE_MAX_SIZE_MB,
scripts/ast-delta.sh:737:# ==================== ÂëΩ‰ª§: clear-cache ====================
scripts/ast-delta.sh:739:cmd_clear_cache() {
scripts/ast-delta.sh:742:    log_info "Clearing AST cache..."
scripts/ast-delta.sh:745:    file_count=$(find "$AST_CACHE_DIR" -name "*.ast" 2>/dev/null | wc -l | tr -d ' ')
scripts/ast-delta.sh:752:    log_ok "Cleared $file_count cache files"
scripts/ast-delta.sh:760:ast-delta.sh - AST Delta Â¢ûÈáèÁ¥¢Âºï
scripts/ast-delta.sh:763:    ast-delta.sh <command> [options]
scripts/ast-delta.sh:769:    clear-cache            Ê∏ÖÁêÜ AST ÁºìÂ≠ò
scripts/ast-delta.sh:778:    GRAPH_DB_PATH          ÂõæÊï∞ÊçÆÂ∫ìË∑ØÂæÑÔºàÈªòËÆ§: .devbooks/graph.dbÔºâ
scripts/ast-delta.sh:779:    AST_CACHE_DIR          AST ÁºìÂ≠òÁõÆÂΩïÔºàÈªòËÆ§: .devbooks/ast-cacheÔºâ
scripts/ast-delta.sh:794:    ast-delta.sh update src/index.ts
scripts/ast-delta.sh:797:    ast-delta.sh batch --since HEAD~1
scripts/ast-delta.sh:800:    ast-delta.sh status
scripts/ast-delta.sh:803:    ast-delta.sh clear-cache
scripts/ast-delta.sh:823:        clear-cache)
scripts/ast-delta.sh:824:            cmd_clear_cache "$@"
tests/cache-manager.bats:2:# cache-manager.bats - Cache Manager Contract Tests
tests/cache-manager.bats:4:# Purpose: Verify multi-level cache (L1/L2) with mtime + blob hash invalidation
tests/cache-manager.bats:6:# Run: bats tests/cache-manager.bats
tests/cache-manager.bats:17:CACHE_MANAGER="${PROJECT_ROOT}/scripts/cache-manager.sh"
tests/cache-manager.bats:25:    # Create isolated cache directory for each test
tests/cache-manager.bats:31:    export SUBGRAPH_CACHE_DB="$DEVBOOKS_DIR/subgraph-cache.db"
tests/cache-manager.bats:44:@test "CT-CACHE-BASE-001: cache-manager.sh exists and is executable" {
tests/cache-manager.bats:45:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:49:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:52:    [[ "$output" == *"cache"* ]] || [[ "$output" == *"Cache"* ]]
tests/cache-manager.bats:57:# AC-002: L1 memory cache hit
tests/cache-manager.bats:60:@test "CT-CACHE-001: L1 cache hit returns result in < 10ms" {
tests/cache-manager.bats:61:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:67:    # First query - populate cache
tests/cache-manager.bats:69:    [ "$status" -eq 0 ] || skip "cache-manager.sh get not yet implemented"
tests/cache-manager.bats:86:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:93:    [ "$status" -eq 0 ] || skip "cache-manager.sh get not yet implemented"
tests/cache-manager.bats:115:# AC-003: L2 file cache hit
tests/cache-manager.bats:118:@test "CT-CACHE-002: L2 cache hit returns result in < 100ms" {
tests/cache-manager.bats:119:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:124:    # First query - populate L2 cache
tests/cache-manager.bats:126:    [ "$status" -eq 0 ] || skip "cache-manager.sh get not yet implemented"
tests/cache-manager.bats:128:    # Clear L1 cache (simulate new session)
tests/cache-manager.bats:134:    local exit_code=$?
tests/cache-manager.bats:136:    [ "$exit_code" -eq 0 ]
tests/cache-manager.bats:141:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:147:    run "$CACHE_MANAGER" --get "$test_file" --query "test_query" --debug
tests/cache-manager.bats:148:    [ "$status" -eq 0 ] || skip "cache-manager.sh get not yet implemented"
tests/cache-manager.bats:150:    # Check debug output mentions validation
tests/cache-manager.bats:152:    skip "Debug output does not show validation"
tests/cache-manager.bats:160:@test "CT-CACHE-003a: mtime change invalidates cache" {
tests/cache-manager.bats:161:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:166:    # First query - set cache with known value
tests/cache-manager.bats:167:    run "$CACHE_MANAGER" --set "$test_file" --query "test_query" --value "cached_value_1"
tests/cache-manager.bats:168:    [ "$status" -eq 0 ] || skip "cache-manager.sh set not yet implemented"
tests/cache-manager.bats:170:    # Verify cache hit returns the cached value
tests/cache-manager.bats:172:    [ "$status" -eq 0 ] || skip "cache-manager.sh get not yet implemented"
tests/cache-manager.bats:179:    # Second query - cache should be invalidated
tests/cache-manager.bats:183:    # 1. Return cache miss (status != 0 or empty output)
tests/cache-manager.bats:184:    # 2. Return different result if cache-manager recalculates
tests/cache-manager.bats:185:    # 3. Debug output shows "miss" or "invalidate"
tests/cache-manager.bats:191:    # If status != 0 or empty output, cache was correctly invalidated
tests/cache-manager.bats:194:@test "CT-CACHE-003b: blob hash change invalidates cache (mtime spoofed)" {
tests/cache-manager.bats:195:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:209:    [ "$status" -eq 0 ] || { cd - > /dev/null; skip "cache-manager.sh get not yet implemented"; }
tests/cache-manager.bats:226:    # If blob hash is checked, cache should be invalidated
tests/cache-manager.bats:233:@test "CT-CACHE-004: mtime change < 1s skips cache" {
tests/cache-manager.bats:234:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:241:    [ "$status" -eq 0 ] || skip "cache-manager.sh get not yet implemented"
tests/cache-manager.bats:246:    # Second query - should skip cache (file may be writing)
tests/cache-manager.bats:247:    run "$CACHE_MANAGER" --get "$test_file" --query "test_query" --debug
tests/cache-manager.bats:250:    # Debug output should mention skip or write-in-progress
tests/cache-manager.bats:260:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:272:    # Wait and capture exit codes
tests/cache-manager.bats:282:    # Verify at least one cache entry was created
tests/cache-manager.bats:283:    local cache_files
tests/cache-manager.bats:284:    cache_files=$(find "$TEST_CACHE_DIR/l2" -name "*.json" 2>/dev/null | wc -l | tr -d ' ')
tests/cache-manager.bats:285:    [ "${cache_files:-0}" -ge 1 ] || skip "No cache files created"
tests/cache-manager.bats:292:            echo "Corrupted cache file: $f" >&2
tests/cache-manager.bats:298:    [ "$corrupted" -eq 0 ] || { echo "Found $corrupted corrupted cache files" >&2; return 1; }
tests/cache-manager.bats:307:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:309:    # Set very low cache limit
tests/cache-manager.bats:312:    # Create many small cache entries
tests/cache-manager.bats:320:    local cache_size_kb=$(du -sk "$TEST_CACHE_DIR/l2" 2>/dev/null | cut -f1 || echo "0")
tests/cache-manager.bats:323:    [ "$cache_size_kb" -lt 1024 ] || skip "LRU eviction did not occur (${cache_size_kb}KB > 1024KB)"
tests/cache-manager.bats:326:@test "CT-CACHE-006b: cache size stays under CACHE_MAX_SIZE_MB" {
tests/cache-manager.bats:327:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:331:    # The cache should never exceed the limit
tests/cache-manager.bats:347:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:367:@test "CT-CACHE-008: incompatible schema version invalidates cache" {
tests/cache-manager.bats:368:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:376:    [ "$status" -eq 0 ] || skip "cache-manager.sh get not yet implemented"
tests/cache-manager.bats:378:    # Find and modify cache entry schema version
tests/cache-manager.bats:379:    local cache_file=$(find "$TEST_CACHE_DIR/l2" -name "*.json" -print -quit 2>/dev/null)
tests/cache-manager.bats:380:    if [ -f "$cache_file" ]; then
tests/cache-manager.bats:382:        jq '.schema_version = "0.0.1"' "$cache_file" > "${cache_file}.tmp" && mv "${cache_file}.tmp" "$cache_file"
tests/cache-manager.bats:385:        run "$CACHE_MANAGER" --get "$test_file" --query "test_query" --debug
tests/cache-manager.bats:392:        skip "No cache file found"
tests/cache-manager.bats:400:@test "CT-CACHE-FORMAT-001: cache entry JSON has required fields" {
tests/cache-manager.bats:401:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:410:    local cache_file=$(find "$TEST_CACHE_DIR/l2" -name "*.json" -print -quit 2>/dev/null)
tests/cache-manager.bats:411:    [ -f "$cache_file" ] || skip "No cache file created"
tests/cache-manager.bats:414:    local content=$(cat "$cache_file")
tests/cache-manager.bats:426:# Subgraph LRU Cache Tests (AC-G07)
tests/cache-manager.bats:429:@test "test_lru_persistence: cache-manager uses sqlite persistence for subgraph cache" {
tests/cache-manager.bats:430:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:433:    run "$CACHE_MANAGER" cache-set "key1" "value1"
tests/cache-manager.bats:434:    skip_if_not_ready "$status" "$output" "cache-manager.sh cache-set"
tests/cache-manager.bats:437:        skip_not_implemented "subgraph cache database not created"
tests/cache-manager.bats:443:        skip_not_implemented "subgraph cache WAL mode"
tests/cache-manager.bats:447:    run sqlite3 "$SUBGRAPH_CACHE_DB" ".schema subgraph_cache"
tests/cache-manager.bats:449:        skip_not_implemented "subgraph_cache table not found"
tests/cache-manager.bats:456:        skip_not_implemented "subgraph_cache table: key column missing"
tests/cache-manager.bats:460:        skip_not_implemented "subgraph_cache table: value column missing"
tests/cache-manager.bats:464:        skip_not_implemented "subgraph_cache table: access_time column missing"
tests/cache-manager.bats:468:        skip_not_implemented "subgraph_cache table: created_time column missing"
tests/cache-manager.bats:473:        skip_not_implemented "subgraph_cache table: PRIMARY KEY constraint missing"
tests/cache-manager.bats:477:    run sqlite3 "$SUBGRAPH_CACHE_DB" ".indices subgraph_cache"
tests/cache-manager.bats:482:@test "test_lru_hit_rate: cache-manager stats reports hit rate > 0.8 for repeated queries" {
tests/cache-manager.bats:483:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:487:        "$CACHE_MANAGER" cache-set "hit-key" "hit-value" >/dev/null 2>&1 || true
tests/cache-manager.bats:488:        "$CACHE_MANAGER" cache-get "hit-key" >/dev/null 2>&1 || true
tests/cache-manager.bats:492:    skip_if_not_ready "$status" "$output" "cache-manager.sh stats"
tests/cache-manager.bats:509:@test "test_lru_cross_process: cache entries are readable across processes" {
tests/cache-manager.bats:510:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:514:    run "$CACHE_MANAGER" cache-set "cross-key" "cross-value"
tests/cache-manager.bats:515:    skip_if_not_ready "$status" "$output" "cache-manager.sh cache-set"
tests/cache-manager.bats:518:    run "$CACHE_MANAGER" cache-get "cross-key"
tests/cache-manager.bats:519:    skip_if_not_ready "$status" "$output" "cache-manager.sh cache-get"
tests/cache-manager.bats:522:        skip_not_implemented "cross-process cache get: value mismatch"
tests/cache-manager.bats:529:        db_value=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT value FROM subgraph_cache WHERE key='cross-key';" 2>/dev/null || echo "")
tests/cache-manager.bats:533:            db_value=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT value FROM cache WHERE key='cross-key';" 2>/dev/null || echo "")
tests/cache-manager.bats:537:            skip_not_implemented "cross-process cache get: SQLite persistence mismatch"
tests/cache-manager.bats:542:        access_time=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT access_time FROM subgraph_cache WHERE key='cross-key';" 2>/dev/null || \
tests/cache-manager.bats:543:                     sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT accessed_at FROM subgraph_cache WHERE key='cross-key';" 2>/dev/null || \
tests/cache-manager.bats:544:                     sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT last_access FROM cache WHERE key='cross-key';" 2>/dev/null || echo "")
tests/cache-manager.bats:549:    # Test true cross-process by running cache-get in a subshell
tests/cache-manager.bats:551:    subshell_result=$(bash -c "export DEVBOOKS_DIR='$DEVBOOKS_DIR'; export SUBGRAPH_CACHE_DB='$SUBGRAPH_CACHE_DB'; '$CACHE_MANAGER' cache-get 'cross-key' 2>/dev/null" || echo "")
tests/cache-manager.bats:554:        skip_not_implemented "cross-process cache get: subshell read failed"
tests/cache-manager.bats:558:@test "test_lru_eviction: cache evicts least recently used entries" {
tests/cache-manager.bats:559:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:564:    "$CACHE_MANAGER" cache-set "k1" "v1" >/dev/null 2>&1 || true
tests/cache-manager.bats:565:    "$CACHE_MANAGER" cache-set "k2" "v2" >/dev/null 2>&1 || true
tests/cache-manager.bats:566:    "$CACHE_MANAGER" cache-set "k3" "v3" >/dev/null 2>&1 || true
tests/cache-manager.bats:567:    "$CACHE_MANAGER" cache-set "k4" "v4" >/dev/null 2>&1 || true
tests/cache-manager.bats:570:        skip_not_implemented "subgraph cache database not created"
tests/cache-manager.bats:574:    count=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM subgraph_cache;" 2>/dev/null || echo "0")
tests/cache-manager.bats:580:@test "test_lru_stats: cache-manager stats reports total entries and hit rate" {
tests/cache-manager.bats:581:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:585:    skip_if_not_ready "$status" "$output" "cache-manager.sh stats"
tests/cache-manager.bats:604:# Spec: dev-playbooks/changes/algorithm-optimization-parity/specs/cache-lru/spec.md
tests/cache-manager.bats:608:@test "CT-CL-001: LRU eviction - evicts least recently used first" {
tests/cache-manager.bats:610:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:617:    "$CACHE_MANAGER" cache-set "k1" "v1" >/dev/null 2>&1 || skip "cache-set not implemented"
tests/cache-manager.bats:619:    "$CACHE_MANAGER" cache-set "k2" "v2" >/dev/null 2>&1 || true
tests/cache-manager.bats:621:    "$CACHE_MANAGER" cache-set "k3" "v3" >/dev/null 2>&1 || true
tests/cache-manager.bats:624:    "$CACHE_MANAGER" cache-get "k1" >/dev/null 2>&1 || true
tests/cache-manager.bats:627:    "$CACHE_MANAGER" cache-set "k4" "v4" >/dev/null 2>&1 || true
tests/cache-manager.bats:630:    run "$CACHE_MANAGER" cache-get "k2"
tests/cache-manager.bats:636:    run "$CACHE_MANAGER" cache-get "k1"
tests/cache-manager.bats:639:    run "$CACHE_MANAGER" cache-get "k4"
tests/cache-manager.bats:645:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:653:        "$CACHE_MANAGER" cache-set "cap-k${i}" "cap-v${i}" >/dev/null 2>&1 || {
tests/cache-manager.bats:655:                skip "cache-set not implemented"
tests/cache-manager.bats:663:        count=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM subgraph_cache;" 2>/dev/null || \
tests/cache-manager.bats:664:                sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM cache;" 2>/dev/null || echo "0")
tests/cache-manager.bats:670:        skip_not_implemented "subgraph cache database not created"
tests/cache-manager.bats:676:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:680:    "$CACHE_MANAGER" cache-set "access-key" "access-value" >/dev/null 2>&1 || skip "cache-set not implemented"
tests/cache-manager.bats:683:        skip_not_implemented "subgraph cache database not created"
tests/cache-manager.bats:689:        "SELECT access_time FROM subgraph_cache WHERE key='access-key';" 2>/dev/null || \
tests/cache-manager.bats:691:        "SELECT accessed_at FROM subgraph_cache WHERE key='access-key';" 2>/dev/null || \
tests/cache-manager.bats:693:        "SELECT last_access FROM cache WHERE key='access-key';" 2>/dev/null || echo "")
tests/cache-manager.bats:703:    "$CACHE_MANAGER" cache-get "access-key" >/dev/null 2>&1 || true
tests/cache-manager.bats:708:        "SELECT access_time FROM subgraph_cache WHERE key='access-key';" 2>/dev/null || \
tests/cache-manager.bats:710:        "SELECT accessed_at FROM subgraph_cache WHERE key='access-key';" 2>/dev/null || \
tests/cache-manager.bats:712:        "SELECT last_access FROM cache WHERE key='access-key';" 2>/dev/null || echo "")
tests/cache-manager.bats:722:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:730:        "$CACHE_MANAGER" cache-set "batch-k${i}" "batch-v${i}" >/dev/null 2>&1 || {
tests/cache-manager.bats:732:                skip "cache-set not implemented"
tests/cache-manager.bats:738:        skip_not_implemented "subgraph cache database not created"
tests/cache-manager.bats:743:    count_before=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM subgraph_cache;" 2>/dev/null || \
tests/cache-manager.bats:744:                   sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM cache;" 2>/dev/null || echo "0")
tests/cache-manager.bats:747:    "$CACHE_MANAGER" cache-set "batch-k11" "batch-v11" >/dev/null 2>&1 || true
tests/cache-manager.bats:751:    count_after=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM subgraph_cache;" 2>/dev/null || \
tests/cache-manager.bats:752:                  sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM cache;" 2>/dev/null || echo "0")
tests/cache-manager.bats:765:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:771:    local log_file="$TEST_CACHE_DIR/cache-eviction.log"
tests/cache-manager.bats:779:        "$CACHE_MANAGER" cache-set "evict-k${i}" "evict-v${i}" >/dev/null 2>&1 || {
tests/cache-manager.bats:781:                skip "cache-set not implemented"
tests/cache-manager.bats:788:        # Â∞ùËØï‰ªé debug ËæìÂá∫Ëé∑ÂèñÊ∑òÊ±∞‰ø°ÊÅØ
tests/cache-manager.bats:789:        run "$CACHE_MANAGER" cache-set "evict-k6" "evict-v6" --debug
tests/cache-manager.bats:791:            skip_not_implemented "Ê∑òÊ±∞Êó•Âøó - Êó•ÂøóÊñá‰ª∂Êú™ÂàõÂª∫‰∏îÊó† debug ËæìÂá∫"
tests/cache-manager.bats:805:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/cache-manager.bats:813:        "$CACHE_MANAGER" cache-set "perf-init-${i}" "perf-value-${i}" >/dev/null 2>&1 || {
tests/cache-manager.bats:815:                skip "cache-set not implemented"
tests/cache-manager.bats:825:        "$CACHE_MANAGER" cache-set "perf-k${i}" "perf-v${i}" >/dev/null 2>&1 || true
tests/cache-manager.bats:848:        count=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM subgraph_cache;" 2>/dev/null || \
tests/cache-manager.bats:849:                sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM cache;" 2>/dev/null || echo "0")
scripts/call-chain.sh:11:#   call-chain-tracer.sh --symbol "funcName" [ÈÄâÈ°π]
scripts/call-chain.sh:118:  call-chain-tracer.sh --symbol "funcName" [ÈÄâÈ°π]
scripts/call-chain.sh:174:  call-chain-tracer.sh --symbol "getUserById" --direction callers
scripts/call-chain.sh:177:  call-chain-tracer.sh --symbol "processPayment" --direction callees --depth 3
scripts/call-chain.sh:180:  call-chain-tracer.sh --symbol "handleError" --trace-usage
scripts/call-chain.sh:183:  call-chain-tracer.sh --symbol "userInput" --data-flow --data-flow-direction forward
scripts/call-chain.sh:186:  call-chain-tracer.sh --symbol "errorData" --data-flow --data-flow-direction backward --max-depth 10
scripts/call-chain.sh:192:  echo "call-chain-tracer.sh version 1.0.0"
scripts/hotspot-analyzer.sh:24:# CT-HW-001: Âä†ÊùÉÂàÜÊï∞ÈªòËÆ§ÊùÉÈáç - score = churn*0.4 + complexity*0.3 + coupling*0.2 + age*0.1
scripts/hotspot-analyzer.sh:42:if ! is_feature_enabled "hotspot_analyzer"; then
scripts/hotspot-analyzer.sh:43:  log_warn "ÁÉ≠ÁÇπÂàÜÊûêÂô®ÂäüËÉΩÂ∑≤Á¶ÅÁî® (features.hotspot_analyzer: false)"
scripts/hotspot-analyzer.sh:51:Usage: hotspot-analyzer.sh [OPTIONS]
scripts/hotspot-analyzer.sh:60:  --with-bug-history    ÂêØÁî® Bug ‰øÆÂ§çÂéÜÂè≤ÊùÉÈáçÂ¢ûÂº∫
scripts/hotspot-analyzer.sh:61:  --bug-weight FLOAT    Bug ‰øÆÂ§çÊùÉÈáçÁ≥ªÊï∞ÔºàÈªòËÆ§: 1.0Ôºâ
scripts/hotspot-analyzer.sh:66:  --weights W1,W2,W3,W4 Ëá™ÂÆö‰πâÊùÉÈáç (churn,complexity,coupling,age)
scripts/hotspot-analyzer.sh:75:    score = frequency √ó complexity
scripts/hotspot-analyzer.sh:78:    score = churn√ó0.4 + complexity√ó0.3 + coupling√ó0.2 + age√ó0.1
scripts/hotspot-analyzer.sh:81:  With --with-bug-history:
scripts/hotspot-analyzer.sh:82:    bug_fix_ratio = bug_fix_count / frequency
scripts/hotspot-analyzer.sh:83:    score = frequency √ó complexity √ó (1 + bug_weight √ó bug_fix_ratio)
scripts/hotspot-analyzer.sh:92:  {"hotspots": [{"rank": 1, "file": "...", "score": N, ...}, ...]}
scripts/hotspot-analyzer.sh:95:  hotspot-analyzer.sh --top 10 --days 7
scripts/hotspot-analyzer.sh:96:  hotspot-analyzer.sh --format json --path ./src
scripts/hotspot-analyzer.sh:97:  hotspot-analyzer.sh --with-bug-history --bug-weight 1.5
scripts/hotspot-analyzer.sh:98:  hotspot-analyzer.sh --weighted --normalized
scripts/hotspot-analyzer.sh:99:  hotspot-analyzer.sh --weighted --weights 0.5,0.2,0.2,0.1
scripts/hotspot-analyzer.sh:100:  hotspot-analyzer.sh --weighted --recency-boost --coupling
scripts/hotspot-analyzer.sh:141:    --with-bug-history)
scripts/hotspot-analyzer.sh:145:    --bug-weight)
scripts/hotspot-analyzer.sh:175:      echo "hotspot-analyzer.sh version $VERSION"
scripts/hotspot-analyzer.sh:215:get_file_complexity() {
scripts/hotspot-analyzer.sh:238:# ‰ΩøÁî® context-layer.sh ÁöÑÂàÜÁ±ªÈÄªËæë
scripts/hotspot-analyzer.sh:239:get_bug_fix_count() {
scripts/hotspot-analyzer.sh:242:  local bug_count=0
scripts/hotspot-analyzer.sh:248:    if declare -f is_bug_fix_message &>/dev/null; then
scripts/hotspot-analyzer.sh:249:      if is_bug_fix_message "$message"; then
scripts/hotspot-analyzer.sh:250:        bug_count=$((bug_count + 1))
scripts/hotspot-analyzer.sh:256:      # ‰ΩøÁî®‰∏é context-layer.sh Áõ∏ÂêåÁöÑ fix ÂàÜÁ±ªËßÑÂàô
scripts/hotspot-analyzer.sh:258:         [[ "$msg_lower" =~ (bug|issue|error|crash|broken|fail) ]]; then
scripts/hotspot-analyzer.sh:259:        bug_count=$((bug_count + 1))
scripts/hotspot-analyzer.sh:264:  echo "$bug_count"
scripts/hotspot-analyzer.sh:299:init_file_age_cache() {
scripts/hotspot-analyzer.sh:329:cleanup_file_age_cache() {
scripts/hotspot-analyzer.sh:373:# CT-HW-001: score = churn*0.4 + complexity*0.3 + coupling*0.2 + age*0.1
scripts/hotspot-analyzer.sh:376:  local complexity_norm="$2"
scripts/hotspot-analyzer.sh:382:      -v complexity="$complexity_norm" \
scripts/hotspot-analyzer.sh:386:      -v w_complexity="$WEIGHT_COMPLEXITY" \
scripts/hotspot-analyzer.sh:391:        score = (churn * w_churn + complexity * w_complexity + coupling * w_coupling + age * w_age) * recency
scripts/hotspot-analyzer.sh:421:  trap "rm -f '$tmp_freq' '$tmp_result' '$tmp_raw'; cleanup_file_age_cache" EXIT
scripts/hotspot-analyzer.sh:444:      echo '{"schema_version": "1.3", "hotspots": []}'
scripts/hotspot-analyzer.sh:454:    init_file_age_cache "$TARGET_PATH" "$since_date"
scripts/hotspot-analyzer.sh:458:    local tmp_complexity
scripts/hotspot-analyzer.sh:459:    tmp_complexity=$(mktemp)
scripts/hotspot-analyzer.sh:466:      awk '{print $2, 1}' "$tmp_freq" > "$tmp_complexity"
scripts/hotspot-analyzer.sh:478:        }' > "$tmp_complexity"
scripts/hotspot-analyzer.sh:490:        }' > "$tmp_complexity"
scripts/hotspot-analyzer.sh:495:    # ÂÖ≥ËÅî: FILE_AGE_CACHE_FILE (file age_days), tmp_complexity (file lines)
scripts/hotspot-analyzer.sh:499:        -v w_complexity="$WEIGHT_COMPLEXITY" \
scripts/hotspot-analyzer.sh:513:        age_cache[$1] = $2
scripts/hotspot-analyzer.sh:520:        complexity_cache[$1] = $2
scripts/hotspot-analyzer.sh:527:      max_complexity = 0
scripts/hotspot-analyzer.sh:537:      complexity = (file in complexity_cache) ? complexity_cache[file] : 1
scripts/hotspot-analyzer.sh:538:      age_days = (file in age_cache) ? age_cache[file] : default_days
scripts/hotspot-analyzer.sh:543:      complexities[n] = complexity
scripts/hotspot-analyzer.sh:548:      if (complexity > max_complexity) max_complexity = complexity
scripts/hotspot-analyzer.sh:558:        complexity = complexities[i]
scripts/hotspot-analyzer.sh:563:          complexity_n = (max_complexity > 0) ? complexity / max_complexity : 0
scripts/hotspot-analyzer.sh:568:          complexity_n = complexity
scripts/hotspot-analyzer.sh:580:        score = (churn_n * w_churn + complexity_n * w_complexity + coupling_n * w_coupling + age_n * w_age) * rec_factor
scripts/hotspot-analyzer.sh:584:        complexity_norms[i] = complexity_n
scripts/hotspot-analyzer.sh:608:        printf "\"weights\":{\"churn\":%s,\"complexity\":%s,\"coupling\":%s,\"age\":%s},", w_churn, w_complexity, w_coupling, w_age
scripts/hotspot-analyzer.sh:610:        printf "\"hotspots\":["
scripts/hotspot-analyzer.sh:616:          printf "{\"rank\":%d,\"file\":\"%s\",\"score\":%.4f,\"churn\":%d,\"complexity\":%d,\"coupling\":0,\"age\":%d}", \
scripts/hotspot-analyzer.sh:632:    ' "$FILE_AGE_CACHE_FILE" "$tmp_complexity" "$tmp_freq"
scripts/hotspot-analyzer.sh:634:    rm -f "$tmp_complexity"
scripts/hotspot-analyzer.sh:640:        local complexity
scripts/hotspot-analyzer.sh:641:        complexity=$(get_file_complexity "$file")
scripts/hotspot-analyzer.sh:643:        local bug_fix_count=0
scripts/hotspot-analyzer.sh:644:        local bug_fix_ratio="0"
scripts/hotspot-analyzer.sh:646:        # Ëé∑Âèñ bug fix Ê¨°Êï∞
scripts/hotspot-analyzer.sh:647:        bug_fix_count=$(get_bug_fix_count "$file" "$since_date")
scripts/hotspot-analyzer.sh:649:        # ËÆ°ÁÆó bug_fix_ratio = bug_fix_count / freq
scripts/hotspot-analyzer.sh:651:          bug_fix_ratio=$(awk -v bug="$bug_fix_count" -v f="$freq" 'BEGIN { printf "%.4f", bug / f }')
scripts/hotspot-analyzer.sh:654:        # ËÆ°ÁÆóÂ¢ûÂº∫ÂàÜÊï∞: score = freq √ó complexity √ó (1 + bug_weight √ó bug_fix_ratio)
scripts/hotspot-analyzer.sh:656:        score=$(awk -v f="$freq" -v c="$complexity" -v bw="$BUG_WEIGHT" -v br="$bug_fix_ratio" \
scripts/hotspot-analyzer.sh:659:        echo "$score|$freq|$complexity|$bug_fix_count|$bug_fix_ratio|$file" >> "$tmp_result"
scripts/hotspot-analyzer.sh:671:      echo "  \"with_bug_history\": true,"
scripts/hotspot-analyzer.sh:672:      echo "  \"bug_weight\": $BUG_WEIGHT,"
scripts/hotspot-analyzer.sh:673:      echo '  "hotspots": ['
scripts/hotspot-analyzer.sh:677:      while IFS='|' read -r score freq complexity bug_count bug_ratio file; do
scripts/hotspot-analyzer.sh:683:        printf '    {"rank": %d, "file": "%s", "score": %s, "frequency": %d, "complexity": %d, "bug_fix_count": %d, "bug_fix_ratio": %s}' \
scripts/hotspot-analyzer.sh:684:          "$rank" "$file" "$score" "$freq" "$complexity" "$bug_count" "$bug_ratio"
scripts/hotspot-analyzer.sh:694:      while IFS='|' read -r score freq complexity bug_count bug_ratio file; do
scripts/hotspot-analyzer.sh:695:        printf "%-4d  %-6s  %-4d  %-10d  %-8d  %-10s  %s\n" "$rank" "$score" "$freq" "$complexity" "$bug_count" "$bug_ratio" "$file"
scripts/hotspot-analyzer.sh:704:        local complexity
scripts/hotspot-analyzer.sh:705:        complexity=$(get_file_complexity "$file")
scripts/hotspot-analyzer.sh:708:        local score=$((freq * complexity))
scripts/hotspot-analyzer.sh:709:        echo "$score|$freq|$complexity|$file" >> "$tmp_result"
scripts/hotspot-analyzer.sh:721:      echo '  "hotspots": ['
scripts/hotspot-analyzer.sh:725:      while IFS='|' read -r score freq complexity file; do
scripts/hotspot-analyzer.sh:731:        printf '    {"rank": %d, "file": "%s", "score": %d, "frequency": %d, "complexity": %d}' \
scripts/hotspot-analyzer.sh:732:          "$rank" "$file" "$score" "$freq" "$complexity"
scripts/hotspot-analyzer.sh:742:      while IFS='|' read -r score freq complexity file; do
scripts/hotspot-analyzer.sh:743:        printf "%-4d  %-6d  %-4d  %-10d  %s\n" "$rank" "$score" "$freq" "$complexity" "$file"
scripts/boundary-detector.sh:20:if ! is_feature_enabled "boundary_detector"; then
scripts/boundary-detector.sh:21:  log_warn "ËæπÁïåÊ£ÄÊµãÂô®ÂäüËÉΩÂ∑≤Á¶ÅÁî® (features.boundary_detector: false)"
scripts/boundary-detector.sh:29:Usage: boundary-detector.sh [OPTIONS] <file-or-pattern>
scripts/boundary-detector.sh:53:  boundary-detector.sh src/index.ts
scripts/boundary-detector.sh:54:  boundary-detector.sh --path node_modules/lodash/index.js --format json
scripts/boundary-detector.sh:55:  boundary-detector.sh --format json dist/bundle.js
scripts/boundary-detector.sh:56:  boundary-detector.sh --config ./my-boundaries.yaml src/
scripts/boundary-detector.sh:125:  "__pycache__/**|generated|0.99|PythonÁºìÂ≠ò"
scripts/boundary-detector.sh:237:detect_boundary() {
scripts/boundary-detector.sh:289:    result=$(detect_boundary "$TARGET" "$CONFIG_FILE")
scripts/boundary-detector.sh:300:      result=$(detect_boundary "$file" "$CONFIG_FILE")
scripts/boundary-detector.sh:306:    result=$(detect_boundary "$TARGET" "$CONFIG_FILE")
scripts/vuln-tracker.sh:2:# vuln-tracker.sh - ÂÆâÂÖ®ÊºèÊ¥ûÂü∫Á°ÄËøΩË∏™Ê®°Âùó
scripts/vuln-tracker.sh:24:LOG_PREFIX="vuln-tracker"
scripts/vuln-tracker.sh:39:# npm 7+ ‰ΩøÁî®Êñ∞Ê†ºÂºè (.vulnerabilities)
scripts/vuln-tracker.sh:129:        # Â§ÑÁêÜ vulnerabilities ÂØπË±°
scripts/vuln-tracker.sh:130:        (.vulnerabilities // {}) | to_entries | map(
scripts/vuln-tracker.sh:204:    local vulnerabilities="$1"
scripts/vuln-tracker.sh:213:    total=$(echo "$vulnerabilities" | jq 'length')
scripts/vuln-tracker.sh:215:    by_severity=$(echo "$vulnerabilities" | jq '
scripts/vuln-tracker.sh:233:        --argjson vulnerabilities "$vulnerabilities" \
scripts/vuln-tracker.sh:238:            vulnerabilities: $vulnerabilities
scripts/vuln-tracker.sh:244:    local vulnerabilities="$1"
scripts/vuln-tracker.sh:249:    total=$(echo "$vulnerabilities" | jq 'length')
scripts/vuln-tracker.sh:268:    echo "$vulnerabilities" | jq -r '.[] | [
scripts/vuln-tracker.sh:321:    if ! check_dependency npm; then
scripts/vuln-tracker.sh:357:    local vulnerabilities
scripts/vuln-tracker.sh:359:        vulnerabilities=$(parse_npm7_format "$audit_json" "$severity" "$include_dev")
scripts/vuln-tracker.sh:361:        vulnerabilities=$(parse_npm6_format "$audit_json" "$severity" "$include_dev")
scripts/vuln-tracker.sh:365:    if [[ -z "$vulnerabilities" || "$vulnerabilities" == "null" ]]; then
scripts/vuln-tracker.sh:366:        vulnerabilities="[]"
scripts/vuln-tracker.sh:372:            format_json_output "$vulnerabilities"
scripts/vuln-tracker.sh:375:            format_md_output "$vulnerabilities"
scripts/vuln-tracker.sh:432:    # Ëá™Âä®Ê£ÄÊµãÊ†ºÂºèÔºàÂ¶ÇÊûúÂåÖÂê´ .vulnerabilities Âàô‰∏∫ npm7Ôºâ
scripts/vuln-tracker.sh:433:    if echo "$audit_json" | jq -e '.vulnerabilities' > /dev/null 2>&1; then
scripts/vuln-tracker.sh:440:    local vulnerabilities
scripts/vuln-tracker.sh:442:        vulnerabilities=$(parse_npm7_format "$audit_json" "$severity" "$include_dev")
scripts/vuln-tracker.sh:444:        vulnerabilities=$(parse_npm6_format "$audit_json" "$severity" "$include_dev")
scripts/vuln-tracker.sh:448:    if [[ -z "$vulnerabilities" || "$vulnerabilities" == "null" ]]; then
scripts/vuln-tracker.sh:449:        vulnerabilities="[]"
scripts/vuln-tracker.sh:454:    total=$(echo "$vulnerabilities" | jq 'length')
scripts/vuln-tracker.sh:459:            format_json_output "$vulnerabilities"
scripts/vuln-tracker.sh:462:            format_md_output "$vulnerabilities"
scripts/vuln-tracker.sh:467:                echo "No vulnerabilities found."
scripts/vuln-tracker.sh:469:                echo "$vulnerabilities" | jq -r '.[] | "\(.name): \(.severity)"'
scripts/vuln-tracker.sh:496:        log_error "ËØ∑ÊåáÂÆöÂåÖÂêç: vuln-tracker trace <package-name>"
scripts/vuln-tracker.sh:587:vuln-tracker.sh - ÂÆâÂÖ®ÊºèÊ¥ûÂü∫Á°ÄËøΩË∏™
scripts/vuln-tracker.sh:590:    vuln-tracker.sh <command> [options]
scripts/vuln-tracker.sh:617:    json - JSON Ê†ºÂºèÔºåÂåÖÂê´ scan_time, total, by_severity, vulnerabilities
scripts/vuln-tracker.sh:626:    vuln-tracker.sh scan
scripts/vuln-tracker.sh:629:    vuln-tracker.sh scan --severity high --format md
scripts/vuln-tracker.sh:632:    vuln-tracker.sh scan --include-dev
scripts/vuln-tracker.sh:635:    vuln-tracker.sh parse --input audit.json --format json
scripts/vuln-tracker.sh:638:    vuln-tracker.sh trace lodash
scripts/vuln-tracker.sh:641:    vuln-tracker.sh scan --dir /path/to/project
scripts/dependency-guard.sh:2:# dependency-guard.sh - Architecture Guard (Cycle Detection + Rule Validation)
scripts/dependency-guard.sh:9:#   dependency-guard.sh --cycles --scope "src/" --format json
scripts/dependency-guard.sh:10:#   dependency-guard.sh --rules <rules.yaml> --format json
scripts/dependency-guard.sh:11:#   dependency-guard.sh --all --scope "src/" --rules <rules.yaml>
scripts/dependency-guard.sh:12:#   dependency-guard.sh --pre-commit [--with-deps]
scripts/dependency-guard.sh:13:#   dependency-guard.sh --help
scripts/dependency-guard.sh:17:#   DEBUG               - Enable debug output (default: false)
scripts/dependency-guard.sh:36:log_debug() {
scripts/dependency-guard.sh:56:dependency-guard.sh - Architecture Guard (Cycle Detection + Rule Validation)
scripts/dependency-guard.sh:59:  dependency-guard.sh --cycles --scope "src/" --format json
scripts/dependency-guard.sh:60:  dependency-guard.sh --rules <rules.yaml> --format json
scripts/dependency-guard.sh:61:  dependency-guard.sh --all --scope "src/" --rules <rules.yaml>
scripts/dependency-guard.sh:62:  dependency-guard.sh --orphan-check --scope "src/" --format json
scripts/dependency-guard.sh:63:  dependency-guard.sh --pre-commit [--with-deps]
scripts/dependency-guard.sh:64:  dependency-guard.sh --help
scripts/dependency-guard.sh:83:  dependency-guard.sh --cycles --scope "src/" --format json
scripts/dependency-guard.sh:86:  dependency-guard.sh --rules config/arch-rules.yaml
scripts/dependency-guard.sh:89:  dependency-guard.sh --pre-commit --with-deps
scripts/dependency-guard.sh:265:# Build dependency graph from files
scripts/dependency-guard.sh:266:build_dependency_graph() {
scripts/dependency-guard.sh:275:    local graph="{}"
scripts/dependency-guard.sh:293:        graph=$(echo "$graph" | jq --arg file "$file" --argjson deps "$(echo "$imports" | jq '[.[] | .target]')" \
scripts/dependency-guard.sh:297:    echo "$graph"
scripts/dependency-guard.sh:481:with open(edges_file, "r", encoding="utf-8") as handle:
scripts/dependency-guard.sh:713:    # Save last rule
scripts/dependency-guard.sh:1115:    git diff --cached --name-only --diff-filter=ACMR 2>/dev/null | grep -E '\.(ts|tsx|js|jsx|sh)$' || true
scripts/dependency-guard.sh:1269:            --debug)
scripts/reranker.sh:11:#   reranker.sh --query "Êü•ËØ¢ÂÜÖÂÆπ" [ÈÄâÈ°π] < candidates.json
scripts/reranker.sh:12:#   echo '{"candidates":[...]}' | reranker.sh --query "Êü•ËØ¢"
scripts/reranker.sh:17:#   AC-003: reranker.sh ÊàêÂäüÊâßË°åÔºåËæìÂá∫ÂåÖÂê´ ranked_results Â≠óÊÆµ
scripts/reranker.sh:18:#   AC-007: reranker.enabled: falseÔºàÈªòËÆ§ÔºâÊó∂Ë∑≥Ëøá
scripts/reranker.sh:71:  reranker.sh --query "Êü•ËØ¢ÂÜÖÂÆπ" [ÈÄâÈ°π] < candidates.json
scripts/reranker.sh:72:  echo '{"candidates":[...]}' | reranker.sh --query "Êü•ËØ¢"
scripts/reranker.sh:96:      {"file_path": "src/auth.ts", "rank": 1, "rerank_score": 0.95},
scripts/reranker.sh:97:      {"file_path": "src/user.ts", "rank": 2, "rerank_score": 0.80}
scripts/reranker.sh:120:    reranker.sh --query "ËÆ§ËØÅÂáΩÊï∞"
scripts/reranker.sh:123:  reranker.sh --query "test" --provider openai < candidates.json
scripts/reranker.sh:126:  reranker.sh --query "test" --mock-llm < candidates.json
scripts/reranker.sh:132:  echo "reranker.sh version 2.0.0 (with LLM Provider abstraction)"
scripts/reranker.sh:198:      log_error "Á§∫‰æã: echo '{\"candidates\":[]}' | reranker.sh --query \"Êü•ËØ¢\""
scripts/reranker.sh:208:# ‰øùÁïôÂêëÂêéÂÖºÂÆπÁöÑ mock_rerank ÂáΩÊï∞
scripts/reranker.sh:209:legacy_mock_rerank() {
scripts/reranker.sh:217:  # ÁÆÄÂçïÊ®°ÊãüÔºöÊåâÂéüÂßãÈ°∫Â∫èËøîÂõûÔºåÊ∑ªÂä†ÈÄíÂáèÁöÑ rerank_score
scripts/reranker.sh:231:      '. + [{file_path: $fp, rank: $rank, rerank_score: ($score | tonumber)}]')
scripts/reranker.sh:239:rerank() {
scripts/reranker.sh:281:      # Ë∞ÉÁî® rerank
scripts/reranker.sh:283:      if result=$(llm_rerank "$query" "$candidates" 2>/dev/null); then
scripts/reranker.sh:290:          # ËΩ¨Êç¢Ê†ºÂºèÔºöindex/score/reason -> file_path/rank/rerank_score
scripts/reranker.sh:297:            local row orig_index score file_path rerank_score
scripts/reranker.sh:305:            rerank_score=$(echo "scale=2; $score / 10" | bc 2>/dev/null || echo "0.5")
scripts/reranker.sh:310:              --argjson score "$rerank_score" \
scripts/reranker.sh:311:              '. + [{file_path: $fp, rank: $rank, rerank_score: ($score | tonumber)}]')
scripts/reranker.sh:315:          ranked=$(legacy_mock_rerank "$input" "$query")
scripts/reranker.sh:318:        log_warn "LLM rerank Â§±Ë¥•Ôºå‰ΩøÁî®Â§áÁî®ÊéíÂ∫è"
scripts/reranker.sh:319:        ranked=$(legacy_mock_rerank "$input" "$query")
scripts/reranker.sh:323:      ranked=$(legacy_mock_rerank "$input" "$query")
scripts/reranker.sh:328:    ranked=$(legacy_mock_rerank "$input" "$query")
scripts/reranker.sh:358:  rerank "$input" "$QUERY"
tests/intent-analysis.bats:2:# intent-analysis.bats - AC-002 Intent Analysis Acceptance Tests
tests/intent-analysis.bats:4:# Purpose: Verify augment-context-global.sh 4-dimensional intent analysis
tests/intent-analysis.bats:6:# Run: bats tests/intent-analysis.bats
tests/intent-analysis.bats:9:# Change: enhance-code-intelligence
tests/intent-analysis.bats:32:    run "$HOOK_SCRIPT" --analyze-intent --prompt "fix authentication bug" 2>&1
tests/intent-analysis.bats:38:    run "$HOOK_SCRIPT" --analyze-intent --file "src/auth/login.ts" --line 42 2>&1
tests/intent-analysis.bats:44:    run "$HOOK_SCRIPT" --analyze-intent --with-history 2>&1
tests/intent-analysis.bats:49:@test "IA-004: code signal extraction" {
tests/intent-analysis.bats:50:    run "$HOOK_SCRIPT" --analyze-intent --file "src/auth.ts" --function "validateToken" 2>&1
tests/intent-analysis.bats:52:    [[ "$output" == *"code"* ]]
tests/intent-analysis.bats:60:    run "$HOOK_SCRIPT" --analyze-intent --prompt "fix bug" --file "src/test.ts" 2>&1
tests/intent-analysis.bats:66:    run "$HOOK_SCRIPT" --analyze-intent --prompt "test" 2>&1
tests/intent-analysis.bats:74:@test "IA-OUTPUT-001: intent analysis output includes signal labels" {
tests/intent-analysis.bats:75:    run "$HOOK_SCRIPT" --analyze-intent --prompt "test" 2>&1
tests/intent-analysis.bats:81:    [[ "$output" == *"code"* ]] || \
tests/intent-analysis.bats:90:    run "$HOOK_SCRIPT" --analyze-intent --prompt "test" --format json 2>&1
tests/intent-analysis.bats:100:@test "IA-PARAM-001: --analyze-intent parameter support" {
tests/intent-analysis.bats:103:    [[ "$output" == *"intent"* ]] || \
tests/intent-analysis.bats:113:@test "IA-COMPAT-001: without --analyze-intent maintains original behavior" {
scripts/cache-manager.sh:2:# cache-manager.sh - Multi-level Cache Manager (L1 Memory + L2 File)
scripts/cache-manager.sh:9:#   cache-manager.sh --get <file_path> --query <query_hash>
scripts/cache-manager.sh:10:#   cache-manager.sh --set <file_path> --query <query_hash> --value <value>
scripts/cache-manager.sh:11:#   cache-manager.sh --clear-l1
scripts/cache-manager.sh:12:#   cache-manager.sh --stats
scripts/cache-manager.sh:13:#   cache-manager.sh --help
scripts/cache-manager.sh:16:#   CACHE_DIR           - Cache directory (default: ${TMPDIR:-/tmp}/.ci-cache)
scripts/cache-manager.sh:17:#   CACHE_MAX_SIZE_MB   - Maximum cache size in MB (default: 50)
scripts/cache-manager.sh:19:#   DEBUG               - Enable debug output (default: false)
scripts/cache-manager.sh:31:: "${CACHE_DIR:=${TMPDIR:-/tmp}/.ci-cache}"
scripts/cache-manager.sh:36:# Subgraph LRU Cache Configuration (REQ-SLC-001 ~ REQ-SLC-009)
scripts/cache-manager.sh:38:: "${SUBGRAPH_CACHE_DB:=${DEVBOOKS_DIR}/subgraph-cache.db}"
scripts/cache-manager.sh:39:: "${CACHE_MAX_SIZE:=100}"  # Maximum number of cache entries
scripts/cache-manager.sh:48:# L1 cache (memory) - using associative arrays
scripts/cache-manager.sh:56:    # Bash 3 fallback - arrays won't work, disable L1 cache
scripts/cache-manager.sh:65:log_debug() {
scripts/cache-manager.sh:96:        local counter_file="${CACHE_DIR:-/tmp}/.cache_ts_counter"
scripts/cache-manager.sh:108:# Subgraph LRU Cache Functions (REQ-SLC-001 ~ REQ-SLC-009)
scripts/cache-manager.sh:111:# Initialize SQLite database for subgraph cache (REQ-SLC-001, REQ-SLC-002)
scripts/cache-manager.sh:113:init_subgraph_cache_db() {
scripts/cache-manager.sh:119:        log_error "sqlite3 is required for subgraph cache"
scripts/cache-manager.sh:129:-- Create cache table (REQ-SLC-002)
scripts/cache-manager.sh:131:CREATE TABLE IF NOT EXISTS subgraph_cache (
scripts/cache-manager.sh:140:CREATE INDEX IF NOT EXISTS idx_access_time ON subgraph_cache(access_time);
scripts/cache-manager.sh:143:CREATE INDEX IF NOT EXISTS idx_ttl_expires ON subgraph_cache(ttl_expires);
scripts/cache-manager.sh:146:CREATE TABLE IF NOT EXISTS cache_stats (
scripts/cache-manager.sh:152:INSERT OR IGNORE INTO cache_stats (stat_key, stat_value) VALUES ('hits', 0);
scripts/cache-manager.sh:153:INSERT OR IGNORE INTO cache_stats (stat_key, stat_value) VALUES ('misses', 0);
scripts/cache-manager.sh:160:ensure_subgraph_cache_db() {
scripts/cache-manager.sh:162:        init_subgraph_cache_db
scripts/cache-manager.sh:166:# Get value from subgraph cache (REQ-SLC-005)
scripts/cache-manager.sh:170:subgraph_cache_get() {
scripts/cache-manager.sh:171:    local cache_key="$1"
scripts/cache-manager.sh:173:    ensure_subgraph_cache_db || return 2
scripts/cache-manager.sh:181:    sqlite3 "$SUBGRAPH_CACHE_DB" "DELETE FROM subgraph_cache WHERE ttl_expires IS NOT NULL AND ttl_expires < $now_seconds;" 2>/dev/null || true
scripts/cache-manager.sh:189:UPDATE subgraph_cache SET access_time = $now WHERE key = '$cache_key';
scripts/cache-manager.sh:191:SELECT value FROM subgraph_cache WHERE key = '$cache_key';
scripts/cache-manager.sh:193:UPDATE cache_stats SET stat_value = stat_value + 1
scripts/cache-manager.sh:194:WHERE stat_key = CASE WHEN (SELECT COUNT(*) FROM subgraph_cache WHERE key = '$cache_key') > 0 THEN 'hits' ELSE 'misses' END;
scripts/cache-manager.sh:206:        sqlite3 "$SUBGRAPH_CACHE_DB" "UPDATE cache_stats SET stat_value = stat_value + 1 WHERE stat_key = 'misses';" 2>/dev/null || true
scripts/cache-manager.sh:211:# Set value in subgraph cache with LRU eviction (REQ-SLC-006)
scripts/cache-manager.sh:216:subgraph_cache_set() {
scripts/cache-manager.sh:217:    local cache_key="$1"
scripts/cache-manager.sh:218:    local cache_value="$2"
scripts/cache-manager.sh:220:    ensure_subgraph_cache_db || return 2
scripts/cache-manager.sh:233:    escaped_value="${cache_value//\'/\'\'}"
scripts/cache-manager.sh:241:    current_count=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM subgraph_cache;" 2>/dev/null || echo "0")
scripts/cache-manager.sh:249:                "SELECT key FROM subgraph_cache ORDER BY access_time ASC LIMIT $evict_count;" 2>/dev/null || echo "")
scripts/cache-manager.sh:255:        # Log to debug output
scripts/cache-manager.sh:256:        log_debug "LRU eviction: removing $evict_count oldest entries (current=$current_count, max=$max_size)"
scripts/cache-manager.sh:266:DELETE FROM subgraph_cache WHERE ttl_expires IS NOT NULL AND ttl_expires < $now_seconds;
scripts/cache-manager.sh:270:DELETE FROM subgraph_cache
scripts/cache-manager.sh:272:    SELECT key FROM subgraph_cache
scripts/cache-manager.sh:275:        WHEN (SELECT COUNT(*) FROM subgraph_cache) >= $max_size THEN $evict_count
scripts/cache-manager.sh:280:INSERT OR REPLACE INTO subgraph_cache (key, value, access_time, created_time, ttl_expires)
scripts/cache-manager.sh:281:VALUES ('$cache_key', '$escaped_value', $now, COALESCE(
scripts/cache-manager.sh:282:    (SELECT created_time FROM subgraph_cache WHERE key = '$cache_key'), $now
scripts/cache-manager.sh:290:# Delete entry from subgraph cache
scripts/cache-manager.sh:291:subgraph_cache_delete() {
scripts/cache-manager.sh:292:    local cache_key="$1"
scripts/cache-manager.sh:294:    ensure_subgraph_cache_db || return 2
scripts/cache-manager.sh:296:    sqlite3 "$SUBGRAPH_CACHE_DB" "DELETE FROM subgraph_cache WHERE key = '$cache_key';"
scripts/cache-manager.sh:300:# Clear all entries from subgraph cache
scripts/cache-manager.sh:301:subgraph_cache_clear() {
scripts/cache-manager.sh:302:    ensure_subgraph_cache_db || return 2
scripts/cache-manager.sh:305:DELETE FROM subgraph_cache;
scripts/cache-manager.sh:306:UPDATE cache_stats SET stat_value = 0 WHERE stat_key IN ('hits', 'misses');
scripts/cache-manager.sh:312:# Get subgraph cache statistics (REQ-SLC-007, REQ-SLC-009)
scripts/cache-manager.sh:313:subgraph_cache_stats() {
scripts/cache-manager.sh:316:    ensure_subgraph_cache_db || return 2
scripts/cache-manager.sh:319:    local total_entries oldest_access newest_access hits misses cache_size_bytes
scripts/cache-manager.sh:321:    total_entries=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT COUNT(*) FROM subgraph_cache;" 2>/dev/null || echo "0")
scripts/cache-manager.sh:322:    oldest_access=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT MIN(access_time) FROM subgraph_cache;" 2>/dev/null || echo "0")
scripts/cache-manager.sh:323:    newest_access=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT MAX(access_time) FROM subgraph_cache;" 2>/dev/null || echo "0")
scripts/cache-manager.sh:324:    hits=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT stat_value FROM cache_stats WHERE stat_key = 'hits';" 2>/dev/null || echo "0")
scripts/cache-manager.sh:325:    misses=$(sqlite3 "$SUBGRAPH_CACHE_DB" "SELECT stat_value FROM cache_stats WHERE stat_key = 'misses';" 2>/dev/null || echo "0")
scripts/cache-manager.sh:342:        cache_size_bytes=$(stat -c %s "$SUBGRAPH_CACHE_DB" 2>/dev/null || stat -f %z "$SUBGRAPH_CACHE_DB" 2>/dev/null || echo "0")
scripts/cache-manager.sh:344:        cache_size_bytes=0
scripts/cache-manager.sh:355:            --argjson cache_size_bytes "$cache_size_bytes" \
scripts/cache-manager.sh:363:                cache_size_bytes: $cache_size_bytes
scripts/cache-manager.sh:372:        echo "Cache size (bytes): $cache_size_bytes"
scripts/cache-manager.sh:379:cache-manager.sh - Multi-level Cache Manager (L1 Memory + L2 File + SQLite LRU)
scripts/cache-manager.sh:382:  cache-manager.sh --get <file_path> --query <query_hash> [--debug]
scripts/cache-manager.sh:383:  cache-manager.sh --set <file_path> --query <query_hash> --value <value>
scripts/cache-manager.sh:384:  cache-manager.sh --clear-l1
scripts/cache-manager.sh:385:  cache-manager.sh --stats
scripts/cache-manager.sh:386:  cache-manager.sh --help
scripts/cache-manager.sh:388:  # Subgraph LRU Cache Commands
scripts/cache-manager.sh:389:  cache-manager.sh cache-get <key>
scripts/cache-manager.sh:390:  cache-manager.sh cache-set <key> <value>
scripts/cache-manager.sh:391:  cache-manager.sh cache-delete <key>
scripts/cache-manager.sh:392:  cache-manager.sh cache-clear
scripts/cache-manager.sh:393:  cache-manager.sh stats [--format json|text]
scripts/cache-manager.sh:396:  --get         Get cached value for file and query
scripts/cache-manager.sh:397:  --set         Set cache value for file and query
scripts/cache-manager.sh:398:  --clear-l1    Clear L1 (memory) cache
scripts/cache-manager.sh:399:  --stats       Show cache statistics (L1/L2)
scripts/cache-manager.sh:400:  --debug       Enable debug output
scripts/cache-manager.sh:404:Subgraph Cache Commands:
scripts/cache-manager.sh:405:  cache-get     Get value from SQLite LRU cache
scripts/cache-manager.sh:406:  cache-set     Set value in SQLite LRU cache
scripts/cache-manager.sh:407:  cache-delete  Delete entry from cache
scripts/cache-manager.sh:408:  cache-clear   Clear all cache entries
scripts/cache-manager.sh:409:  stats         Show subgraph cache statistics
scripts/cache-manager.sh:412:  CACHE_DIR           Cache directory (default: ${TMPDIR:-/tmp}/.ci-cache)
scripts/cache-manager.sh:413:  CACHE_MAX_SIZE_MB   Maximum L2 cache size in MB (default: 50)
scripts/cache-manager.sh:414:  CACHE_MAX_SIZE      Maximum subgraph cache entries (default: 100)
scripts/cache-manager.sh:415:  SUBGRAPH_CACHE_DB   SQLite database path (default: .devbooks/subgraph-cache.db)
scripts/cache-manager.sh:419:  # Get cached value
scripts/cache-manager.sh:420:  cache-manager.sh --get src/server.ts --query abc123
scripts/cache-manager.sh:422:  # Set cache value
scripts/cache-manager.sh:423:  cache-manager.sh --set src/server.ts --query abc123 --value "result data"
scripts/cache-manager.sh:425:  # Clear memory cache
scripts/cache-manager.sh:426:  cache-manager.sh --clear-l1
scripts/cache-manager.sh:429:  cache-manager.sh --stats
scripts/cache-manager.sh:431:  # Subgraph cache operations
scripts/cache-manager.sh:432:  cache-manager.sh cache-set "key1" "value1"
scripts/cache-manager.sh:433:  cache-manager.sh cache-get "key1"
scripts/cache-manager.sh:434:  cache-manager.sh stats --format json
scripts/cache-manager.sh:490:        # Last resort: use cksum
scripts/cache-manager.sh:495:# Compute cache key from file path, mtime, blob hash, and query hash
scripts/cache-manager.sh:497:compute_cache_key() {
scripts/cache-manager.sh:531:    local mtime_cache_dir="${CACHE_DIR}/mtime"
scripts/cache-manager.sh:532:    mkdir -p "$mtime_cache_dir" 2>/dev/null
scripts/cache-manager.sh:537:    local mtime_file="${mtime_cache_dir}/${safe_name}.mtime"
scripts/cache-manager.sh:539:    local last_mtime="0"
scripts/cache-manager.sh:541:        last_mtime=$(cat "$mtime_file" 2>/dev/null || echo "0")
scripts/cache-manager.sh:544:    # Update mtime cache file
scripts/cache-manager.sh:547:    # Also update in-memory cache if available (bash 4+)
scripts/cache-manager.sh:553:    # If mtime changed within last 1 second, file may be writing
scripts/cache-manager.sh:554:    if [[ "$last_mtime" != "0" ]]; then
scripts/cache-manager.sh:555:        local delta=$((current_mtime - last_mtime))
scripts/cache-manager.sh:557:            log_debug "File may be in write progress, skipping cache: $file_path (delta=${delta}s)"
scripts/cache-manager.sh:565:# Validate cache entry against current file state
scripts/cache-manager.sh:567:validate_cache_entry() {
scripts/cache-manager.sh:568:    local cache_file="$1"
scripts/cache-manager.sh:572:    if [[ ! -f "$cache_file" ]]; then
scripts/cache-manager.sh:578:    schema_version=$(jq -r '.schema_version // ""' "$cache_file" 2>/dev/null)
scripts/cache-manager.sh:580:        log_debug "Schema version mismatch: $schema_version != $CACHE_SCHEMA_VERSION"
scripts/cache-manager.sh:585:    local cached_mtime
scripts/cache-manager.sh:586:    cached_mtime=$(jq -r '.mtime // 0' "$cache_file" 2>/dev/null)
scripts/cache-manager.sh:587:    if [[ "$cached_mtime" != "$current_mtime" ]]; then
scripts/cache-manager.sh:588:        log_debug "mtime mismatch: $cached_mtime != $current_mtime"
scripts/cache-manager.sh:593:    local cached_blob_hash
scripts/cache-manager.sh:594:    cached_blob_hash=$(jq -r '.blob_hash // ""' "$cache_file" 2>/dev/null)
scripts/cache-manager.sh:595:    if [[ "$cached_blob_hash" != "$current_blob_hash" ]]; then
scripts/cache-manager.sh:596:        log_debug "blob hash mismatch: $cached_blob_hash != $current_blob_hash"
scripts/cache-manager.sh:607:# Check cache size and evict if needed
scripts/cache-manager.sh:611:    local cache_l2_dir="${CACHE_DIR}/l2"
scripts/cache-manager.sh:613:    if [[ ! -d "$cache_l2_dir" ]]; then
scripts/cache-manager.sh:617:    # Get current cache size in KB for precise comparison
scripts/cache-manager.sh:619:    current_size_kb=$(du -sk "$cache_l2_dir" 2>/dev/null | cut -f1 || echo "0")
scripts/cache-manager.sh:624:    # Loop until cache is below target size
scripts/cache-manager.sh:628:        # Count total cache files
scripts/cache-manager.sh:630:        total_files=$(find "$cache_l2_dir" -type f -name "*.json" 2>/dev/null | wc -l | tr -d ' ')
scripts/cache-manager.sh:644:        find "$cache_l2_dir" -type f -name "*.json" -exec sh -c '
scripts/cache-manager.sh:662:        log_info "Evicted $deleted cache entries"
scripts/cache-manager.sh:665:        current_size_kb=$(du -sk "$cache_l2_dir" 2>/dev/null | cut -f1 || echo "0")
scripts/cache-manager.sh:678:# Get cached value with validation
scripts/cache-manager.sh:681:get_cached_with_validation() {
scripts/cache-manager.sh:690:    # 1. Check L1 cache (memory) - only if bash 4+
scripts/cache-manager.sh:695:        local cached_mtime="${L1_META_MTIME[$l1_key]:-}"
scripts/cache-manager.sh:696:        local cached_blob_hash="${L1_META_BLOB_HASH[$l1_key]:-}"
scripts/cache-manager.sh:697:        if [[ -n "$cached_mtime" && -n "$cached_blob_hash" ]]; then
scripts/cache-manager.sh:701:                if [[ "$current_mtime" == "$cached_mtime" && "$current_blob_hash" == "$cached_blob_hash" ]]; then
scripts/cache-manager.sh:702:                    log_debug "L1 cache hit for $file_path"
scripts/cache-manager.sh:716:        log_debug "Could not get mtime for $file_path"
scripts/cache-manager.sh:722:        log_debug "File may be in write progress, skipping cache"
scripts/cache-manager.sh:723:        return 0  # Return success but no output (cache skip)
scripts/cache-manager.sh:731:    # 5. Compute cache key
scripts/cache-manager.sh:732:    local cache_key
scripts/cache-manager.sh:733:    cache_key=$(compute_cache_key "$file_path" "$current_mtime" "$current_blob_hash" "$query_hash")
scripts/cache-manager.sh:734:    local cache_file="${CACHE_DIR}/l2/${cache_key}.json"
scripts/cache-manager.sh:736:    # 6. Check L2 cache (file)
scripts/cache-manager.sh:737:    if [[ -f "$cache_file" ]]; then
scripts/cache-manager.sh:739:        if validate_cache_entry "$cache_file" "$current_mtime" "$current_blob_hash"; then
scripts/cache-manager.sh:741:            value=$(jq -r '.value // ""' "$cache_file" 2>/dev/null)
scripts/cache-manager.sh:744:            local tmp_file="${cache_file}.tmp.$$"
scripts/cache-manager.sh:747:            if jq --arg accessed_at "$current_time" '.accessed_at = ($accessed_at | tonumber)' "$cache_file" > "$tmp_file" 2>/dev/null; then
scripts/cache-manager.sh:748:                mv "$tmp_file" "$cache_file" 2>/dev/null || rm -f "$tmp_file"
scripts/cache-manager.sh:760:            log_debug "L2 cache hit for $file_path (mtime=$current_mtime, blob_hash=$current_blob_hash)"
scripts/cache-manager.sh:764:            log_debug "L2 cache validation failed for $file_path"
scripts/cache-manager.sh:768:    log_debug "Cache miss for $file_path (mtime=$current_mtime, blob_hash=$current_blob_hash)"
scripts/cache-manager.sh:769:    # Return success (0) even on cache miss - caller checks output
scripts/cache-manager.sh:770:    # This allows tests to distinguish between "not implemented" and "cache miss"
scripts/cache-manager.sh:774:# Set cache value with lock protection
scripts/cache-manager.sh:776:set_cache_with_lock() {
scripts/cache-manager.sh:786:    # Ensure cache directory exists
scripts/cache-manager.sh:794:    # Compute cache key
scripts/cache-manager.sh:795:    local cache_key
scripts/cache-manager.sh:796:    cache_key=$(compute_cache_key "$file_path" "$mtime" "$blob_hash" "$query_hash")
scripts/cache-manager.sh:797:    local cache_file="${CACHE_DIR}/l2/${cache_key}.json"
scripts/cache-manager.sh:798:    local tmp_file="${cache_file}.tmp.$$"
scripts/cache-manager.sh:799:    local lock_file="${cache_file}.lock"
scripts/cache-manager.sh:816:            --arg key "$cache_key" \
scripts/cache-manager.sh:837:        mv "$tmp_file" "$cache_file" 2>/dev/null || rm -f "$tmp_file"
scripts/cache-manager.sh:844:    # Write to L1 cache (if available)
scripts/cache-manager.sh:852:    log_debug "Cache set for $file_path"
scripts/cache-manager.sh:855:# Clear L1 (memory) cache
scripts/cache-manager.sh:856:clear_l1_cache() {
scripts/cache-manager.sh:863:    log_debug "L1 cache cleared"
scripts/cache-manager.sh:866:# Show cache statistics
scripts/cache-manager.sh:868:    local cache_l2_dir="${CACHE_DIR}/l2"
scripts/cache-manager.sh:878:    if [[ -d "$cache_l2_dir" ]]; then
scripts/cache-manager.sh:879:        l2_entries=$(find "$cache_l2_dir" -type f -name "*.json" 2>/dev/null | wc -l | tr -d ' ')
scripts/cache-manager.sh:880:        size_kb=$(du -sk "$cache_l2_dir" 2>/dev/null | cut -f1 || echo "0")
scripts/cache-manager.sh:890:        --arg cache_dir "$CACHE_DIR" \
scripts/cache-manager.sh:898:            cache_dir: $cache_dir,
scripts/cache-manager.sh:914:    # Check for subgraph cache commands first (positional arguments)
scripts/cache-manager.sh:917:            cache-get)
scripts/cache-manager.sh:920:                    log_error "Usage: cache-manager.sh cache-get <key>"
scripts/cache-manager.sh:923:                subgraph_cache_get "$1"
scripts/cache-manager.sh:926:            cache-set)
scripts/cache-manager.sh:929:                    log_error "Usage: cache-manager.sh cache-set <key> <value>"
scripts/cache-manager.sh:932:                subgraph_cache_set "$1" "$2"
scripts/cache-manager.sh:935:            cache-delete)
scripts/cache-manager.sh:938:                    log_error "Usage: cache-manager.sh cache-delete <key>"
scripts/cache-manager.sh:941:                subgraph_cache_delete "$1"
scripts/cache-manager.sh:944:            cache-clear)
scripts/cache-manager.sh:945:                subgraph_cache_clear
scripts/cache-manager.sh:946:                echo "Subgraph cache cleared"
scripts/cache-manager.sh:963:                subgraph_cache_stats "$format"
scripts/cache-manager.sh:992:            --debug)
scripts/cache-manager.sh:1031:                log_error "Usage: cache-manager.sh --get <file_path> --query <query_hash>"
scripts/cache-manager.sh:1034:            get_cached_with_validation "$file_path" "$query_hash"
scripts/cache-manager.sh:1038:                log_error "Usage: cache-manager.sh --set <file_path> --query <query_hash> --value <value>"
scripts/cache-manager.sh:1041:            set_cache_with_lock "$file_path" "$query_hash" "$value"
scripts/cache-manager.sh:1044:            clear_l1_cache
scripts/cache-manager.sh:1045:            echo "L1 cache cleared"
scripts/entropy-viz.sh:7:#   2. ÁÉ≠ÁÇπÊñá‰ª∂Âõæ (graph TD/LR)
scripts/entropy-viz.sh:185:    "dependency_entropy": 0.31
scripts/entropy-viz.sh:188:  "hotspots": [
scripts/entropy-viz.sh:189:    {"file": "src/order/process.ts", "complexity": 45, "churn": 32},
scripts/entropy-viz.sh:190:    {"file": "src/auth/login.ts", "complexity": 38, "churn": 28},
scripts/entropy-viz.sh:191:    {"file": "src/payment/handler.ts", "complexity": 35, "churn": 24}
scripts/entropy-viz.sh:263:generate_mermaid_hotspot_chart() {
scripts/entropy-viz.sh:269:graph TD
scripts/entropy-viz.sh:270:    subgraph ÁÉ≠ÁÇπÊñá‰ª∂ÂàÜÊûê
scripts/entropy-viz.sh:274:  echo "$metrics_json" | jq -r '.hotspots[] | "\(.file)|\(.complexity)|\(.churn)"' 2>/dev/null | while IFS='|' read -r file complexity churn; do
scripts/entropy-viz.sh:277:    echo "        H${i}[\"$label<br/>Â§çÊùÇÂ∫¶: $complexity | ÂèòÊõ¥: $churn\"]"
scripts/entropy-viz.sh:303:  dep_entropy=$(echo "$metrics_json" | jq -r '.metrics.dependency_entropy' 2>/dev/null || echo "0.31")
scripts/entropy-viz.sh:357:    generate_mermaid_hotspot_chart "$metrics_json"
scripts/embedding.sh:22:TEMP_DIR="/tmp/devbooks-embedding-$$"
scripts/embedding.sh:64:log_debug() {
scripts/embedding.sh:78:    log_debug "Ollama ‰∏çÂèØÁî®ÔºàMOCKÔºâ"
scripts/embedding.sh:83:    log_debug "Ollama ÂèØÁî®ÔºàMOCKÔºâ"
scripts/embedding.sh:89:    log_debug "Ollama ÂëΩ‰ª§‰∏çÂ≠òÂú®"
scripts/embedding.sh:96:    log_debug "Ollama ÊúçÂä°ÂèØÁî®: $endpoint"
scripts/embedding.sh:100:  log_debug "Ollama ÊúçÂä°Êó†ÂìçÂ∫î: $endpoint"
scripts/embedding.sh:109:    log_debug "OpenAI API Key Â∑≤ËÆæÁΩÆ"
scripts/embedding.sh:113:  log_debug "OpenAI API Key Êú™ËÆæÁΩÆ"
scripts/embedding.sh:151:        log_debug "Ëá™Âä®Ê£ÄÊµãÔºö‰ΩøÁî® Ollama"
scripts/embedding.sh:182:    log_debug "Ollama Mock: ÁîüÊàêÊµãËØïÂêëÈáè"
scripts/embedding.sh:188:  log_debug "Ë∞ÉÁî® Ollama API: $endpoint/api/embeddings (model: $model)"
scripts/embedding.sh:199:  local http_code
scripts/embedding.sh:200:  response=$(curl -s -w "\n%{http_code}" -X POST "${endpoint}/api/embeddings" \
scripts/embedding.sh:205:  http_code=$(echo "$response" | tail -n1)
scripts/embedding.sh:209:  if [[ "$http_code" != "200" ]]; then
scripts/embedding.sh:210:    log_error "Ollama API ÈîôËØØ: HTTP $http_code"
scripts/embedding.sh:229:  echo "$response" | jq -r '.embedding | @json' > "$output_file"
scripts/embedding.sh:236:  log_debug "Ollama ÂêëÈáèÂ∑≤‰øùÂ≠ò: $output_file"
scripts/embedding.sh:254:  log_debug "‰ΩøÁî®ÂÖ≥ÈîÆËØçÊêúÁ¥¢: $query (top_k=$top_k, head_limit=$head_limit)"
scripts/embedding.sh:268:    done < <(rg -l --type-add 'code:*.{ts,tsx,js,jsx,py,go,rs,java,sh}' -t code -i "$query" "$PROJECT_ROOT" 2>/dev/null | head -n "$head_limit" | sed "s|^$PROJECT_ROOT/||")
scripts/embedding.sh:325:  API_MODEL="text-embedding-3-small"
scripts/embedding.sh:330:  VECTOR_DB_DIR="$PROJECT_ROOT/.devbooks/embeddings"
scripts/embedding.sh:348:  log_debug "Âä†ËΩΩÈÖçÁΩÆ: $CONFIG_FILE"
scripts/embedding.sh:359:  # ÊèêÂèñ embedding ÈÖçÁΩÆ
scripts/embedding.sh:410:  VECTOR_DB_DIR="$PROJECT_ROOT/${storage_path:-.devbooks/embeddings}"
scripts/embedding.sh:438:  log_debug "ÈÖçÁΩÆÂ∑≤Âä†ËΩΩ: provider=$EMBEDDING_PROVIDER, ollama_model=$OLLAMA_MODEL, api_model=$API_MODEL"
scripts/embedding.sh:444:call_embedding_api() {
scripts/embedding.sh:455:    log_debug "OpenAI Mock: ÁîüÊàêÊµãËØïÂêëÈáè"
scripts/embedding.sh:461:  local api_endpoint="${API_BASE_URL}/embeddings"
scripts/embedding.sh:463:  log_debug "Ë∞ÉÁî® API: $api_endpoint"
scripts/embedding.sh:486:  echo "$response" | jq -r '.data[0].embedding | @json' > "$output_file"
scripts/embedding.sh:493:  log_debug "ÂêëÈáèÂ∑≤‰øùÂ≠ò: $output_file"
scripts/embedding.sh:531:      local response=$(curl -s -X POST "${API_BASE_URL}/embeddings" \
scripts/embedding.sh:548:        local vector=$(echo "$response" | jq -r ".data[$idx].embedding | @json")
scripts/embedding.sh:570:extract_code_files() {
scripts/embedding.sh:577:  local exclude_dirs="node_modules|dist|build|\.git|__pycache__|venv|\.venv|target|\.next"
scripts/embedding.sh:590:    ! -path "*/__pycache__/*" \
scripts/embedding.sh:652:  local code_files="$TEMP_DIR/code_files.tsv"
scripts/embedding.sh:653:  extract_code_files "$code_files"
scripts/embedding.sh:655:  if [ ! -s "$code_files" ]; then
scripts/embedding.sh:661:  batch_embed "$code_files" "$VECTOR_DB_DIR"
scripts/embedding.sh:694:  extract_code_files "$TEMP_DIR/all_files.tsv"
scripts/embedding.sh:757:semantic_search() {
scripts/embedding.sh:791:      if _semantic_search_with_embedding "ollama" "$query" "$top_k"; then
scripts/embedding.sh:797:        if _detect_openai_api && _semantic_search_with_embedding "openai" "$query" "$top_k"; then
scripts/embedding.sh:810:      if _semantic_search_with_embedding "openai" "$query" "$top_k"; then
scripts/embedding.sh:859:_semantic_search_with_embedding() {
scripts/embedding.sh:868:      log_debug "Mock Ê®°ÂºèÔºöÂàõÂª∫‰∏¥Êó∂ÊµãËØïÁ¥¢Âºï"
scripts/embedding.sh:900:      if ! call_embedding_api "$query" "$query_vector_file"; then
scripts/embedding.sh:917:  log_debug "ËÆ°ÁÆóÁõ∏‰ººÂ∫¶..."
scripts/embedding.sh:1077:  --debug              ÂêØÁî®Ë∞ÉËØïÊ®°Âºè
scripts/embedding.sh:1114:  embedding:
scripts/embedding.sh:1252:          --debug)
scripts/embedding.sh:1263:      semantic_search "$search_query"
scripts/embedding.sh:1321:    --debug)
tests/ci.bats:32:    if ! grep -qE "branches:.*\[.*main.*\]|branches:.*\[.*master.*\]" "$WORKFLOW_FILE"; then
tests/ci.bats:34:        if ! grep -qE "^\s*-\s*(main|master)\s*$" "$WORKFLOW_FILE"; then
tests/ci.bats:35:            skip_not_implemented "branch filter for main/master"
tests/ci.bats:40:@test "test_workflow_cycles: workflow runs dependency cycle check" {
tests/ci.bats:45:    grep -q "dependency-guard.sh --cycles" "$WORKFLOW_FILE" || skip_not_implemented "cycle check step"
tests/ci.bats:53:    grep -q "boundary-detector.sh detect" "$WORKFLOW_FILE" || skip_not_implemented "architecture rule check"
tests/ci.bats:61:    # Check for orphan detection step - could be via dependency-guard or dedicated script
tests/ci.bats:68:    # Alternative: orphan check might be part of dependency-guard with different flag
tests/ci.bats:69:    if grep -qE "dependency-guard.sh.*(--all|--orphan|--unused)" "$WORKFLOW_FILE"; then
tests/ci.bats:103:    # Check for exit code propagation
tests/ci.bats:118:    grep -q "dependency-guard.sh --cycles" "$GITLAB_TEMPLATE" || skip_not_implemented "gitlab cycle check"
tests/ci.bats:119:    grep -q "boundary-detector.sh detect" "$GITLAB_TEMPLATE" || skip_not_implemented "gitlab architecture check"
tests/ci.bats:134:    if grep -qE "dependency-guard.sh.*(--all|--orphan|--unused)" "$GITLAB_TEMPLATE"; then
scripts/llm-provider.sh:13:#   llm_rerank "Êü•ËØ¢" '[{"file":"a.ts","content":"..."}]'
scripts/llm-provider.sh:257:llm_rerank() {
scripts/llm-provider.sh:273:  # Ë∞ÉÁî® Provider ÁöÑ rerank ÂÆûÁé∞
scripts/llm-provider.sh:284:  if result=$(_llm_provider_rerank "$query" "$candidates"); then
scripts/llm-provider.sh:479:# llm_provider_rerank -> llm_rerank
scripts/llm-provider.sh:480:llm_provider_rerank() {
scripts/llm-provider.sh:481:  llm_rerank "$@"
scripts/llm-provider.sh:509:  llm_rerank "Êü•ËØ¢" '[{"file":"a.ts","content":"..."}]'
scripts/llm-provider.sh:519:  rerank               ÈáçÊéíÂ∫èÔºà‰ªé stdin ËØªÂèñ JSONÔºâ
scripts/llm-provider.sh:547:  echo '{"query":"test","candidates":[...]}' | ./llm-provider.sh rerank --provider anthropic
scripts/llm-provider.sh:568:      list|info|test|rerank|call)
scripts/llm-provider.sh:627:    rerank)
scripts/llm-provider.sh:634:      llm_rerank "$query" "$candidates"
tests/feature-toggle.bats:9:# Change: enhance-code-intelligence
tests/feature-toggle.bats:18:HOTSPOT_ANALYZER="${PROJECT_ROOT}/scripts/hotspot-analyzer.sh"
tests/feature-toggle.bats:19:BOUNDARY_DETECTOR="${PROJECT_ROOT}/scripts/boundary-detector.sh"
tests/feature-toggle.bats:41:    [[ "$output" == *"enhanced_hotspot"* ]] || skip "features section not found"
tests/feature-toggle.bats:60:@test "FT-002: disable enhanced_hotspot" {
tests/feature-toggle.bats:65:  enhanced_hotspot: false
tests/feature-toggle.bats:66:  intent_analysis: true
tests/feature-toggle.bats:67:  subgraph_retrieval: true
tests/feature-toggle.bats:68:  boundary_detection: true
tests/feature-toggle.bats:88:@test "FT-003: disable boundary_detection" {
tests/feature-toggle.bats:93:  enhanced_hotspot: true
tests/feature-toggle.bats:94:  intent_analysis: true
tests/feature-toggle.bats:95:  subgraph_retrieval: true
tests/feature-toggle.bats:96:  boundary_detection: false
tests/feature-toggle.bats:121:  enhanced_hotspot: false
tests/feature-toggle.bats:122:  intent_analysis: false
tests/feature-toggle.bats:123:  subgraph_retrieval: false
tests/feature-toggle.bats:124:  boundary_detection: false
tests/feature-toggle.bats:143:  enhanced_hotspot: false
tests/feature-toggle.bats:144:  intent_analysis: false
tests/feature-toggle.bats:145:  subgraph_retrieval: false
tests/feature-toggle.bats:146:  boundary_detection: false
tests/feature-toggle.bats:198:  enhanced_hotspot: "invalid"
tests/feature-toggle.bats:222:  enhanced_hotspot: false
tests/feature-toggle.bats:243:        "enhanced_hotspot"
tests/feature-toggle.bats:244:        "intent_analysis"
tests/feature-toggle.bats:245:        "subgraph_retrieval"
tests/feature-toggle.bats:246:        "boundary_detection"
tests/context-layer.bats:2:# context-layer.bats - Context Layer Contract Tests
tests/context-layer.bats:4:# Purpose: Verify commit semantic classification and bug fix history integration
tests/context-layer.bats:6:# Run: bats tests/context-layer.bats
tests/context-layer.bats:17:CONTEXT_LAYER="${PROJECT_ROOT}/scripts/context-layer.sh"
tests/context-layer.bats:18:HOTSPOT_ANALYZER="${PROJECT_ROOT}/scripts/hotspot-analyzer.sh"
tests/context-layer.bats:86:@test "CT-CTX-BASE-001: context-layer.sh exists and is executable" {
tests/context-layer.bats:87:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:91:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:99:# AC-009: Commit semantic classification
tests/context-layer.bats:103:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:127:@test "CT-CTX-001b: classifies 'bug' in message as fix" {
tests/context-layer.bats:128:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:138:    git commit -m "Fixed bug in authentication" --quiet
tests/context-layer.bats:150:        [ "$type" = "fix" ] || skip "Type should be 'fix' for 'bug' keyword, got '$type'"
tests/context-layer.bats:159:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:187:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:222:@test "CT-CTX-004: extracts correct bug fix count for file" {
tests/context-layer.bats:223:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:228:    run "$CONTEXT_LAYER" --bug-history --file test.txt --format json
tests/context-layer.bats:235:        local count=$(echo "$output" | jq -r '.bug_fix_count' 2>/dev/null)
tests/context-layer.bats:239:        local commits=$(echo "$output" | jq -r '.bug_fix_commits | length' 2>/dev/null)
tests/context-layer.bats:244:@test "CT-CTX-004b: bug history includes commit SHAs" {
tests/context-layer.bats:245:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:251:    run "$CONTEXT_LAYER" --bug-history --file test.txt --format json
tests/context-layer.bats:258:    local sha1=$(echo "$output" | jq -r '.bug_fix_commits[0]' 2>/dev/null)
tests/context-layer.bats:259:    [ "${#sha1}" -ge 7 ] || skip "SHA should be at least 7 characters"
tests/context-layer.bats:266:@test "CT-CTX-005: --with-bug-history enhances hotspot score" {
tests/context-layer.bats:267:    [ -x "$HOTSPOT_ANALYZER" ] || skip "hotspot-analyzer.sh not yet implemented"
tests/context-layer.bats:268:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:273:    # Get score without bug history (use 2>/dev/null to avoid stderr mixing)
tests/context-layer.bats:277:    [ "$status_without" -eq 0 ] || { cd - > /dev/null; skip "hotspot-analyzer.sh not working"; }
tests/context-layer.bats:280:        score_without=$(echo "$json_without" | jq -r '.hotspots[0].score // 0' 2>/dev/null)
tests/context-layer.bats:283:    # Get score with bug history
tests/context-layer.bats:285:    json_with=$("$HOTSPOT_ANALYZER" --with-bug-history --format json 2>/dev/null)
tests/context-layer.bats:287:    [ "$status_with" -eq 0 ] || { cd - > /dev/null; skip "--with-bug-history not yet implemented"; }
tests/context-layer.bats:290:        score_with=$(echo "$json_with" | jq -r '.hotspots[0].score // 0' 2>/dev/null)
tests/context-layer.bats:295:    # Score with bug history should be higher (due to bug fix ratio)
tests/context-layer.bats:298:        [ "$comparison" = "1" ] || skip "Score with bug history ($score_with) should be > without ($score_without)"
tests/context-layer.bats:302:@test "CT-CTX-005b: hotspot output includes bug_weight field" {
tests/context-layer.bats:303:    [ -x "$HOTSPOT_ANALYZER" ] || skip "hotspot-analyzer.sh not yet implemented"
tests/context-layer.bats:308:    run "$HOTSPOT_ANALYZER" --with-bug-history --format json
tests/context-layer.bats:312:    [ "$status" -eq 0 ] || skip "--with-bug-history not yet implemented"
tests/context-layer.bats:314:    [[ "$output" == *"bug"* ]] || skip "Output should mention bug-related field"
tests/context-layer.bats:319:# AC-014: hotspot-analyzer.sh baseline
tests/context-layer.bats:322:@test "CT-CTX-006: hotspot-analyzer without --with-bug-history unchanged" {
tests/context-layer.bats:323:    [ -x "$HOTSPOT_ANALYZER" ] || skip "hotspot-analyzer.sh not yet implemented"
tests/context-layer.bats:328:    # Output should NOT contain bug_weight when not using --with-bug-history
tests/context-layer.bats:329:    [[ "$output" != *"bug_weight"* ]] || skip "bug_weight should not appear without --with-bug-history"
tests/context-layer.bats:330:    [[ "$output" != *"bug_fix"* ]] || skip "bug_fix fields should not appear without --with-bug-history"
tests/context-layer.bats:333:@test "CT-CTX-006b: output format matches existing hotspot schema" {
tests/context-layer.bats:334:    [ -x "$HOTSPOT_ANALYZER" ] || skip "hotspot-analyzer.sh not yet implemented"
tests/context-layer.bats:350:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:368:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:401:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/context-layer.bats:417:    for msg in "fix: bug in parser" "fix(core): memory leak" "Fixed crash on startup" \
tests/context-layer.bats:514:    [ -x "$CONTEXT_LAYER" ] || skip "context-layer.sh not yet implemented"
tests/vuln-tracker.bats:2:# vuln-tracker.bats - ÂÆâÂÖ®ÊºèÊ¥ûÂü∫Á°ÄËøΩË∏™Ê®°ÂùóÊµãËØï
tests/vuln-tracker.bats:5:# ËßÑÊ†º: dev-playbooks/specs/vuln-tracker/spec.md
tests/vuln-tracker.bats:23:VULN_TRACKER="$SCRIPT_DIR/vuln-tracker.sh"
tests/vuln-tracker.bats:58:@test "T-VT-001: vuln-tracker scan executes npm audit and parses results" {
tests/vuln-tracker.bats:64:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh scan"
tests/vuln-tracker.bats:76:@test "T-VT-001b: vuln-tracker scan works with --dir option" {
tests/vuln-tracker.bats:80:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh scan --dir"
tests/vuln-tracker.bats:89:# @test T-VT-002: npm 7+ Ê†ºÂºèÊ≠£Á°ÆËß£Êûê .vulnerabilities ÁªìÊûÑ
tests/vuln-tracker.bats:90:@test "T-VT-002: vuln-tracker parses npm 7+ audit format correctly" {
tests/vuln-tracker.bats:98:  "vulnerabilities": {
tests/vuln-tracker.bats:107:          "dependency": "lodash",
tests/vuln-tracker.bats:121:    "vulnerabilities": {
tests/vuln-tracker.bats:135:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh parse npm7"
tests/vuln-tracker.bats:149:@test "T-VT-003: vuln-tracker parses npm 6.x audit format correctly" {
tests/vuln-tracker.bats:169:      "vulnerable_versions": "<4.17.21",
tests/vuln-tracker.bats:174:    "vulnerabilities": {
tests/vuln-tracker.bats:186:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh parse npm6"
tests/vuln-tracker.bats:200:@test "T-VT-004: vuln-tracker scan --severity filters by threshold" {
tests/vuln-tracker.bats:208:  "vulnerabilities": {
tests/vuln-tracker.bats:234:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh --severity"
tests/vuln-tracker.bats:248:@test "T-VT-004b: vuln-tracker supports all severity levels" {
tests/vuln-tracker.bats:256:  "vulnerabilities": {
tests/vuln-tracker.bats:266:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh --severity moderate"
tests/vuln-tracker.bats:284:@test "T-VT-005: vuln-tracker trace shows dependency chain" {
tests/vuln-tracker.bats:298:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh trace"
tests/vuln-tracker.bats:304:    assert_contains_any "$output" "lodash" "dependency" "chain" "src/index.js"
tests/vuln-tracker.bats:308:@test "T-VT-005b: vuln-tracker trace handles transitive dependencies" {
tests/vuln-tracker.bats:315:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh trace transitive"
tests/vuln-tracker.bats:326:@test "T-VT-006: vuln-tracker gracefully handles npm audit failure" {
tests/vuln-tracker.bats:335:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh scan fallback"
tests/vuln-tracker.bats:345:@test "T-VT-006b: vuln-tracker handles invalid project directory" {
tests/vuln-tracker.bats:349:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh invalid dir"
tests/vuln-tracker.bats:361:@test "T-VT-007: vuln-tracker outputs valid JSON with --format json" {
tests/vuln-tracker.bats:367:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh --format json"
tests/vuln-tracker.bats:379:    # vulnerabilities Â∫îËØ•ÊòØÊï∞ÁªÑÔºàÂèØËÉΩ‰∏∫Á©∫Ôºâ
tests/vuln-tracker.bats:380:    local vuln_type
tests/vuln-tracker.bats:381:    vuln_type=$(echo "$json" | jq -r '.vulnerabilities | type' 2>/dev/null)
tests/vuln-tracker.bats:382:    [ "$vuln_type" = "array" ]
tests/vuln-tracker.bats:386:@test "T-VT-007b: vuln-tracker JSON output includes complete vulnerability structure" {
tests/vuln-tracker.bats:391:    cat > "$TEST_TEMP_DIR/mock/vuln-audit.json" << 'EOF'
tests/vuln-tracker.bats:394:  "vulnerabilities": {
tests/vuln-tracker.bats:405:    run "$VULN_TRACKER" parse --input "$TEST_TEMP_DIR/mock/vuln-audit.json" --format json
tests/vuln-tracker.bats:406:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh parse --format json"
tests/vuln-tracker.bats:429:@test "T-VT-008: vuln-tracker outputs Markdown table with --format md" {
tests/vuln-tracker.bats:434:    cat > "$TEST_TEMP_DIR/mock/vuln-audit.json" << 'EOF'
tests/vuln-tracker.bats:437:  "vulnerabilities": {
tests/vuln-tracker.bats:447:    run "$VULN_TRACKER" parse --input "$TEST_TEMP_DIR/mock/vuln-audit.json" --format md
tests/vuln-tracker.bats:448:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh --format md"
tests/vuln-tracker.bats:463:@test "T-VT-008b: vuln-tracker Markdown output includes severity badges" {
tests/vuln-tracker.bats:467:    cat > "$TEST_TEMP_DIR/mock/critical-vuln.json" << 'EOF'
tests/vuln-tracker.bats:470:  "vulnerabilities": {
tests/vuln-tracker.bats:480:    run "$VULN_TRACKER" parse --input "$TEST_TEMP_DIR/mock/critical-vuln.json" --format md
tests/vuln-tracker.bats:481:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh --format md badges"
tests/vuln-tracker.bats:494:@test "T-VT-009: vuln-tracker shows friendly message when no vulnerabilities" {
tests/vuln-tracker.bats:502:  "vulnerabilities": {},
tests/vuln-tracker.bats:504:    "vulnerabilities": {
tests/vuln-tracker.bats:517:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh no vulnerabilities"
tests/vuln-tracker.bats:522:    assert_contains_any "$output" "Êú™ÂèëÁé∞" "no vulnerabilities" "No vulnerabilities" "0 vulnerabilities" "clean"
tests/vuln-tracker.bats:526:@test "T-VT-009b: vuln-tracker JSON output correct when no vulnerabilities" {
tests/vuln-tracker.bats:533:  "vulnerabilities": {}
tests/vuln-tracker.bats:538:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh no vuln json"
tests/vuln-tracker.bats:551:    # vulnerabilities Â∫îËØ•ÊòØÁ©∫Êï∞ÁªÑ
tests/vuln-tracker.bats:552:    local vuln_count
tests/vuln-tracker.bats:553:    vuln_count=$(echo "$json" | jq '.vulnerabilities | length' 2>/dev/null)
tests/vuln-tracker.bats:554:    [ "$vuln_count" = "0" ] || [ "$vuln_count" = "null" ]
tests/vuln-tracker.bats:562:@test "T-VT-010: vuln-tracker excludes devDependencies by default" {
tests/vuln-tracker.bats:567:    cat > "$TEST_TEMP_DIR/mock/dev-vuln-audit.json" << 'EOF'
tests/vuln-tracker.bats:570:  "vulnerabilities": {
tests/vuln-tracker.bats:590:    run "$VULN_TRACKER" parse --input "$TEST_TEMP_DIR/mock/dev-vuln-audit.json"
tests/vuln-tracker.bats:591:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh exclude dev"
tests/vuln-tracker.bats:603:@test "T-VT-010b: vuln-tracker --include-dev includes devDependencies" {
tests/vuln-tracker.bats:607:    cat > "$TEST_TEMP_DIR/mock/dev-vuln-audit.json" << 'EOF'
tests/vuln-tracker.bats:610:  "vulnerabilities": {
tests/vuln-tracker.bats:625:    run "$VULN_TRACKER" parse --input "$TEST_TEMP_DIR/mock/dev-vuln-audit.json" --include-dev
tests/vuln-tracker.bats:626:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh --include-dev"
tests/vuln-tracker.bats:639:# @test EDGE-001: Á©∫ vulnerabilities ÂØπË±°
tests/vuln-tracker.bats:640:@test "EDGE-001: vuln-tracker handles empty vulnerabilities object" {
tests/vuln-tracker.bats:644:    echo '{"vulnerabilities": {}}' > "$TEST_TEMP_DIR/mock/empty.json"
tests/vuln-tracker.bats:647:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh empty"
tests/vuln-tracker.bats:653:@test "EDGE-002: vuln-tracker handles malformed JSON gracefully" {
tests/vuln-tracker.bats:669:@test "EDGE-003: vuln-tracker handles very long package names" {
tests/vuln-tracker.bats:678:  "vulnerabilities": {
tests/vuln-tracker.bats:688:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh long name"
tests/vuln-tracker.bats:695:@test "EDGE-004: vuln-tracker handles special characters in package names" {
tests/vuln-tracker.bats:701:  "vulnerabilities": {
tests/vuln-tracker.bats:711:    skip_if_not_ready "$status" "$output" "vuln-tracker.sh special chars"
tests/vuln-tracker.bats:722:@test "HELP-001: vuln-tracker --help shows usage" {
tests/vuln-tracker.bats:735:@test "HELP-002: vuln-tracker shows help with no arguments" {
tests/regression.bats:23:# Build cache file path - use fixed name based on test file (not $$)
tests/regression.bats:25:_get_build_cache_file() {
tests/regression.bats:26:    local cache_dir
tests/regression.bats:28:        cache_dir="$BATS_FILE_TMPDIR"
tests/regression.bats:30:        cache_dir="$TMPDIR"
tests/regression.bats:32:        cache_dir="/tmp"
tests/regression.bats:34:        cache_dir="${BATS_TEST_DIRNAME:-.}"
tests/regression.bats:36:    echo "${cache_dir}/.regression-build-cache"
tests/regression.bats:41:    local cache_file
tests/regression.bats:42:    cache_file="$(_get_build_cache_file)"
tests/regression.bats:44:    # Run build once and cache results
tests/regression.bats:48:        npm run build > "$cache_file" 2>&1
tests/regression.bats:49:        echo "$?" >> "$cache_file"
tests/regression.bats:55:    local cache_file
tests/regression.bats:56:    cache_file="$(_get_build_cache_file)"
tests/regression.bats:57:    rm -f "$cache_file" 2>/dev/null || true
tests/regression.bats:60:# Get cached build output and status
tests/regression.bats:62:    local cache_file
tests/regression.bats:63:    cache_file="$(_get_build_cache_file)"
tests/regression.bats:64:    if [ -f "$cache_file" ]; then
tests/regression.bats:65:        # Last line is status, rest is output
tests/regression.bats:66:        BUILD_STATUS=$(tail -1 "$cache_file")
tests/regression.bats:67:        # Use sed to get all but last line (portable across macOS and Linux)
tests/regression.bats:68:        BUILD_OUTPUT=$(sed '$d' "$cache_file")
tests/regression.bats:71:        BUILD_OUTPUT="Build cache not available"
tests/regression.bats:87:    local script="./scripts/embedding.sh"
tests/regression.bats:88:    [ -x "$script" ] || skip "embedding.sh not executable"
tests/regression.bats:98:    local script="./scripts/call-chain.sh"
tests/regression.bats:99:    [ -x "$script" ] || skip "call-chain.sh not executable"
tests/regression.bats:102:@test "CT-REG-003: ci_bug_locate tool still available" {
tests/regression.bats:104:    run grep "ci_bug_locate" "$SERVER_TS"
tests/regression.bats:108:@test "CT-REG-003b: ci_bug_locate script executable" {
tests/regression.bats:109:    local script="./scripts/bug-locator.sh"
tests/regression.bats:110:    [ -x "$script" ] || skip "bug-locator.sh not executable"
tests/regression.bats:113:@test "CT-REG-004: ci_complexity tool still available" {
tests/regression.bats:115:    run grep "ci_complexity" "$SERVER_TS"
tests/regression.bats:119:@test "CT-REG-004b: ci_complexity script executable" {
tests/regression.bats:120:    local script="./scripts/complexity.sh"
tests/regression.bats:121:    [ -x "$script" ] || skip "complexity.sh not executable"
tests/regression.bats:124:@test "CT-REG-005: ci_graph_rag tool still available" {
tests/regression.bats:126:    run grep "ci_graph_rag" "$SERVER_TS"
tests/regression.bats:130:@test "CT-REG-005b: ci_graph_rag script executable" {
tests/regression.bats:131:    local script="./scripts/graph-rag.sh"
tests/regression.bats:132:    [ -x "$script" ] || skip "graph-rag.sh not executable"
tests/regression.bats:142:    local script="./scripts/indexer.sh"
tests/regression.bats:143:    [ -x "$script" ] || skip "indexer.sh not executable"
tests/regression.bats:146:@test "CT-REG-007: ci_hotspot tool still available" {
tests/regression.bats:148:    run grep "ci_hotspot" "$SERVER_TS"
tests/regression.bats:152:@test "CT-REG-007b: ci_hotspot script executable" {
tests/regression.bats:153:    local script="./scripts/hotspot-analyzer.sh"
tests/regression.bats:154:    [ -x "$script" ] || skip "hotspot-analyzer.sh not executable"
tests/regression.bats:157:@test "CT-REG-008: ci_boundary tool still available" {
tests/regression.bats:159:    run grep "ci_boundary" "$SERVER_TS"
tests/regression.bats:163:@test "CT-REG-008b: ci_boundary script executable" {
tests/regression.bats:164:    local script="./scripts/boundary-detector.sh"
tests/regression.bats:165:    [ -x "$script" ] || skip "boundary-detector.sh not executable"
tests/regression.bats:204:@test "CT-REG-SCRIPT-002: cache-utils.sh still sources correctly" {
tests/regression.bats:205:    local script="./scripts/cache-utils.sh"
tests/regression.bats:206:    [ -f "$script" ] || skip "cache-utils.sh not found"
tests/regression.bats:211:@test "CT-REG-SCRIPT-003: hotspot-analyzer.sh basic functionality" {
tests/regression.bats:212:    local script="./scripts/hotspot-analyzer.sh"
tests/regression.bats:213:    [ -x "$script" ] || skip "hotspot-analyzer.sh not executable"
tests/regression.bats:219:@test "CT-REG-SCRIPT-004: hotspot-analyzer.sh JSON output unchanged" {
tests/regression.bats:220:    local script="./scripts/hotspot-analyzer.sh"
tests/regression.bats:221:    [ -x "$script" ] || skip "hotspot-analyzer.sh not executable"
tests/regression.bats:227:    [[ "$output" == *"hotspots"* ]]
tests/regression.bats:256:    # Should have at least 8 existing tools
tests/regression.bats:257:    [ "$tool_count" -ge 8 ] || skip "Expected at least 8 MCP tools, found $tool_count"
tests/regression.bats:267:        "ci_bug_locate"
tests/regression.bats:268:        "ci_complexity"
tests/regression.bats:269:        "ci_graph_rag"
tests/regression.bats:271:        "ci_hotspot"
tests/regression.bats:272:        "ci_boundary"
tests/regression.bats:285:@test "CT-REG-API-001: ci_hotspot accepts path parameter" {
tests/regression.bats:288:    run grep -A 20 "ci_hotspot" "$SERVER_TS"
tests/regression.bats:289:    [[ "$output" == *"path"* ]] || skip "ci_hotspot should accept path parameter"
tests/regression.bats:292:@test "CT-REG-API-002: ci_hotspot accepts format parameter" {
tests/regression.bats:295:    run grep -A 20 "ci_hotspot" "$SERVER_TS"
tests/regression.bats:296:    [[ "$output" == *"format"* ]] || skip "ci_hotspot should accept format parameter"
tests/regression.bats:315:@test "CT-REG-NEW-002: new ci_federation does not conflict" {
tests/regression.bats:318:    # If ci_federation is added, it should not break other tools
tests/regression.bats:319:    if grep -q "ci_federation" "$SERVER_TS"; then
tests/regression.bats:323:        [ "$tool_count" -ge 9 ] || skip "ci_federation may have replaced an existing tool"
tests/adr-parser.bats:2:# adr-parser.bats - ADR parsing and linking tests
tests/adr-parser.bats:9:ADR_PARSER="$SCRIPT_DIR/adr-parser.sh"
tests/adr-parser.bats:10:GRAPH_STORE="$SCRIPT_DIR/graph-store.sh"
tests/adr-parser.bats:15:    export GRAPH_DB_PATH="$DEVBOOKS_DIR/graph.db"
tests/adr-parser.bats:23:@test "test_parse_madr: adr-parser extracts MADR fields" {
tests/adr-parser.bats:27:    local adr_dir="$TEST_TEMP_DIR/docs/adr"
tests/adr-parser.bats:28:    mkdir -p "$adr_dir"
tests/adr-parser.bats:29:    cat > "$adr_dir/0001-use-sqlite.md" << 'EOF'
tests/adr-parser.bats:30:# ADR-001: Use SQLite for graph storage
tests/adr-parser.bats:36:We need a lightweight graph store.
tests/adr-parser.bats:39:Use SQLite with WAL mode for graph-store.sh.
tests/adr-parser.bats:46:    run "$ADR_PARSER" parse "$adr_dir/0001-use-sqlite.md" --format json
tests/adr-parser.bats:47:    skip_if_not_ready "$status" "$output" "adr-parser.sh parse"
tests/adr-parser.bats:50:    local adr_id
tests/adr-parser.bats:51:    adr_id=$(echo "$output" | jq -r '.adrs[0].id // empty')
tests/adr-parser.bats:52:    if [ "$adr_id" != "ADR-001" ]; then
tests/adr-parser.bats:57:@test "test_parse_nygard: adr-parser extracts Nygard fields" {
tests/adr-parser.bats:61:    local adr_dir="$TEST_TEMP_DIR/docs/adr"
tests/adr-parser.bats:62:    mkdir -p "$adr_dir"
tests/adr-parser.bats:63:    cat > "$adr_dir/0002-record-decisions.md" << 'EOF'
tests/adr-parser.bats:85:    run "$ADR_PARSER" parse "$adr_dir/0002-record-decisions.md" --format json
tests/adr-parser.bats:86:    skip_if_not_ready "$status" "$output" "adr-parser.sh parse"
tests/adr-parser.bats:89:    local adr_id
tests/adr-parser.bats:90:    adr_id=$(echo "$output" | jq -r '.adrs[0].id // empty')
tests/adr-parser.bats:91:    if [ "$adr_id" != "1" ]; then
tests/adr-parser.bats:96:@test "test_keywords: adr-parser extracts keywords" {
tests/adr-parser.bats:100:    local adr_dir="$TEST_TEMP_DIR/docs/adr"
tests/adr-parser.bats:101:    mkdir -p "$adr_dir"
tests/adr-parser.bats:102:    cat > "$adr_dir/0003-keywords.md" << 'EOF'
tests/adr-parser.bats:103:# ADR-003: Cache subgraph data
tests/adr-parser.bats:109:We need SQLite caching for graph-store.sh.
tests/adr-parser.bats:112:Use SQLite with WAL mode for subgraph-cache.db.
tests/adr-parser.bats:115:    run "$ADR_PARSER" parse "$adr_dir/0003-keywords.md" --format json
tests/adr-parser.bats:116:    skip_if_not_ready "$status" "$output" "adr-parser.sh parse"
tests/adr-parser.bats:120:    has_sqlite=$(echo "$output" | jq -r '.adrs[0].keywords | any(. == "SQLite")')
tests/adr-parser.bats:126:@test "test_adr_graph_link: adr-parser links keywords to graph nodes" {
tests/adr-parser.bats:133:    skip_if_not_ready "$status" "$output" "graph-store.sh init"
tests/adr-parser.bats:135:    "$GRAPH_STORE" add-node --id "sym:graph-store" --symbol "graph-store.sh" --kind "file" --file "scripts/graph-store.sh"
tests/adr-parser.bats:137:    local adr_dir="$TEST_TEMP_DIR/docs/adr"
tests/adr-parser.bats:138:    mkdir -p "$adr_dir"
tests/adr-parser.bats:139:    cat > "$adr_dir/0004-link.md" << 'EOF'
tests/adr-parser.bats:140:# ADR-004: Use graph-store.sh for storage
tests/adr-parser.bats:146:Link graph-store.sh with ADRs.
tests/adr-parser.bats:149:    run "$ADR_PARSER" scan --link --adr-dir "$adr_dir" --format json
tests/adr-parser.bats:150:    skip_if_not_ready "$status" "$output" "adr-parser.sh scan --link"
tests/adr-parser.bats:159:@test "test_no_adr_dir: adr-parser handles missing ADR directory" {
tests/adr-parser.bats:163:    local empty_dir="$TEST_TEMP_DIR/no-adr"
tests/adr-parser.bats:166:    run "$ADR_PARSER" scan --adr-dir "$empty_dir" --format json
tests/adr-parser.bats:167:    skip_if_not_ready "$status" "$output" "adr-parser.sh scan"
tests/adr-parser.bats:171:    count=$(echo "$output" | jq -r '.adrs | length')
tests/llm-provider.bats:37:    result=$(source "$LLM_PROVIDER_SCRIPT" && llm_provider_rerank "test query" '[{"file":"a.ts"}]')
tests/llm-provider.bats:47:    result=$(source "$LLM_PROVIDER_SCRIPT" && llm_provider_rerank "test" '[]')
tests/llm-provider.bats:153:llm_provider_rerank() {
tests/llm-provider.bats:201:# @full: Provider rerank ÂäüËÉΩÊµãËØï
tests/llm-provider.bats:205:    result=$(source "$LLM_PROVIDER_SCRIPT" && llm_provider_rerank "query" '[{"file":"a.ts"},{"file":"b.ts"}]')
tests/llm-provider.bats:215:    result=$(source "$LLM_PROVIDER_SCRIPT" && llm_provider_call "Analyze this code")
tests/drift-detector.bats:11:    export DRIFT_DETECTOR_SCRIPT="${SCRIPTS_DIR}/drift-detector.sh"
tests/drift-detector.bats:35:        "dependency_violations": $violations,
tests/drift-detector.bats:36:        "boundary_clarity": 0.85
tests/drift-detector.bats:53:@test "T-DD-001: drift-detector.sh script exists and is executable" {
tests/drift-detector.bats:74:    echo "$result" | jq -e '.drift_detected == true'
tests/drift-detector.bats:80:@test "T-DD-003: Detects dependency direction violations" {
tests/drift-detector.bats:110:    echo "$result" | jq -e '.violations[] | select(.type == "dependency_violation")'
tests/drift-detector.bats:114:@test "T-DD-004: Detects module boundary blurring" {
tests/drift-detector.bats:122:        "boundary_clarity": 0.90,
tests/drift-detector.bats:133:        "boundary_clarity": 0.65,
tests/drift-detector.bats:141:    echo "$result" | jq -e '.drift_detected == true'
tests/drift-detector.bats:142:    echo "$result" | jq -e '.changes[] | select(.type == "boundary_blur")'
tests/drift-detector.bats:167:    jq -e '.metrics | has("dependency_violations")' "$SNAPSHOTS_DIR/new.json"
tests/drift-detector.bats:205:@test "T-DD-008: Detects hotspot file coupling increase" {
tests/drift-detector.bats:212:    "hotspot_files": [
tests/drift-detector.bats:223:    "hotspot_files": [
tests/drift-detector.bats:232:    echo "$result" | jq -e '.changes[] | select(.type == "hotspot_coupling_increase")'
tests/drift-detector.bats:237:@test "T-DD-009: Comprehensive drift detection" {
tests/drift-detector.bats:247:        "dependency_violations": 0,
tests/drift-detector.bats:248:        "boundary_clarity": 0.90,
tests/drift-detector.bats:260:        "dependency_violations": 5,
tests/drift-detector.bats:261:        "boundary_clarity": 0.60,
tests/drift-detector.bats:270:    echo "$result" | jq -e '.drift_detected == true'
tests/incremental-indexing.bats:4:# Purpose: Verify ast-diff.sh incremental indexing functionality
tests/incremental-indexing.bats:9:# Change: enhance-code-intelligence
tests/incremental-indexing.bats:17:AST_DIFF="${PROJECT_ROOT}/scripts/ast-diff.sh"
tests/incremental-indexing.bats:18:SCIP_INDEX="${PROJECT_ROOT}/index.scip"
tests/incremental-indexing.bats:24:@test "II-BASE-001: ast-diff.sh exists and is executable" {
tests/incremental-indexing.bats:79:    skip "SCIP dependency check not yet implemented"
tests/incremental-indexing.bats:229:    local files="src/server.ts,scripts/common.sh,scripts/call-chain.sh,scripts/graph-rag.sh,scripts/bug-locator.sh"
tests/incremental-indexing.bats:277:    run "$AST_DIFF" --incremental --file "index.scip" 2>&1
tests/performance.bats:17:CACHE_MANAGER="${PROJECT_ROOT}/scripts/cache-manager.sh"
tests/performance.bats:18:DEPENDENCY_GUARD="${PROJECT_ROOT}/scripts/dependency-guard.sh"
tests/performance.bats:19:FEDERATION_LITE="${PROJECT_ROOT}/scripts/federation-lite.sh"
tests/performance.bats:20:HOTSPOT_ANALYZER="${PROJECT_ROOT}/scripts/hotspot-analyzer.sh"
tests/performance.bats:30:    export CACHE_DIR="$TEST_TEMP_DIR/cache"
tests/performance.bats:68:# P95 < 100ms for cached queries
tests/performance.bats:71:@test "CT-PERF-001: cache hit latency P95 < 100ms" {
tests/performance.bats:72:    [ -x "$CACHE_MANAGER" ] || skip "[NOT_IMPL] cache-manager.sh not yet implemented"
tests/performance.bats:77:    # Warm up cache
tests/performance.bats:79:    [ "$status" -eq 0 ] || skip "[NOT_IMPL] cache-manager.sh get not yet implemented"
tests/performance.bats:108:# P95 < 500ms for complete query (with cache support)
tests/performance.bats:112:    [ -x "$HOTSPOT_ANALYZER" ] || skip "hotspot-analyzer.sh not yet implemented"
tests/performance.bats:143:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/performance.bats:169:        local exit_code=$?
tests/performance.bats:170:        if [ "$exit_code" -eq 0 ] && [ "$MEASURED_TIME_MS" -gt 0 ]; then
tests/performance.bats:190:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/performance.bats:227:        local exit_code=$?
tests/performance.bats:228:        if [ "$exit_code" -eq 0 ] && [ "$MEASURED_TIME_MS" -gt 0 ]; then
tests/performance.bats:248:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/performance.bats:251:    # The extract_imports_ts function has O(n*m) complexity with jq calls
tests/performance.bats:258:    local exit_code=$?
tests/performance.bats:262:    # Skip if timeout (exit code 124) or signal (exit code > 128)
tests/performance.bats:263:    if [ "$exit_code" -eq 124 ]; then
tests/performance.bats:265:    elif [ "$exit_code" -gt 128 ]; then
tests/performance.bats:266:        skip "Cycle detection crashed (signal $((exit_code - 128)))"
tests/performance.bats:269:    [ "$exit_code" -eq 0 ] || skip "Cycle detection not yet implemented"
tests/performance.bats:277:@test "CT-PERF-006: federation indexing < 10s for 3 repos" {
tests/performance.bats:278:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/performance.bats:280:    # Create 3 test repos with contracts
tests/performance.bats:302:    # Create federation config
tests/performance.bats:303:    cat > "$TEST_TEMP_DIR/federation.yaml" << EOF
tests/performance.bats:305:federation:
tests/performance.bats:309:      contracts: ["**/*.proto"]
tests/performance.bats:312:      contracts: ["**/*.proto"]
tests/performance.bats:315:      contracts: ["**/*.proto"]
tests/performance.bats:320:    export FEDERATION_CONFIG="$TEST_TEMP_DIR/federation.yaml"
tests/performance.bats:321:    export FEDERATION_INDEX="$TEST_TEMP_DIR/federation-index.json"
tests/performance.bats:324:    local exit_code=$?
tests/performance.bats:326:    [ "$exit_code" -eq 0 ] || skip "Federation indexing not yet implemented"
tests/performance.bats:334:@test "CT-PERF-BASELINE-001: hotspot-analyzer baseline" {
tests/performance.bats:335:    [ -x "$HOTSPOT_ANALYZER" ] || skip "hotspot-analyzer.sh not yet implemented"
tests/performance.bats:338:    local exit_code=$?
tests/performance.bats:340:    [ "$exit_code" -eq 0 ]
tests/performance.bats:346:@test "CT-PERF-BASELINE-002: bug-locator baseline" {
tests/performance.bats:347:    local script="./scripts/bug-locator.sh"
tests/performance.bats:348:    [ -x "$script" ] || skip "bug-locator.sh not executable"
tests/performance.bats:351:    local exit_code=$?
tests/performance.bats:361:@test "CT-PERF-MEM-001: no memory leak in repeated cache operations" {
tests/performance.bats:362:    [ -x "$CACHE_MANAGER" ] || skip "cache-manager.sh not yet implemented"
tests/performance.bats:367:    # Run 100 cache operations
tests/performance.bats:372:    # Check cache directory size is reasonable
tests/performance.bats:373:    local cache_size=$(du -sk "$CACHE_DIR" 2>/dev/null | cut -f1 || echo "0")
tests/performance.bats:376:    [ "$cache_size" -lt 10240 ] || skip "Cache size ${cache_size}KB seems excessive"
tests/intent-classification.bats:2:# intent-classification.bats - Intent Classification Contract Tests
tests/intent-classification.bats:4:# Purpose: Verify intent classification algorithm (4-type: debug/refactor/docs/feature)
tests/intent-classification.bats:6:# Run: bats tests/intent-classification.bats
tests/intent-classification.bats:17:# - CT-IC-005: Priority rules (debug > refactor > docs > feature)
tests/intent-classification.bats:38:    # Source the common.sh to get access to get_intent_type function
tests/intent-classification.bats:44:# Helper function to check if get_intent_type is available
tests/intent-classification.bats:46:    if ! declare -f get_intent_type &>/dev/null; then
tests/intent-classification.bats:47:        skip "get_intent_type function not yet implemented"
tests/intent-classification.bats:52:# CT-IC-001: DEBUG Classification - error/fix/bug triggers
tests/intent-classification.bats:53:# Scenario: SC-IC-001 - Pure debug intent
tests/intent-classification.bats:56:@test "CT-IC-001: DEBUG classification - fix keyword triggers debug" {
tests/intent-classification.bats:60:    result=$(get_intent_type "fix the authentication bug")
tests/intent-classification.bats:62:    [ "$result" = "debug" ]
tests/intent-classification.bats:65:@test "CT-IC-001: DEBUG classification - debug keyword triggers debug" {
tests/intent-classification.bats:69:    result=$(get_intent_type "debug the login flow")
tests/intent-classification.bats:71:    [ "$result" = "debug" ]
tests/intent-classification.bats:74:@test "CT-IC-001: DEBUG classification - bug keyword triggers debug" {
tests/intent-classification.bats:78:    result=$(get_intent_type "found a bug in the parser")
tests/intent-classification.bats:80:    [ "$result" = "debug" ]
tests/intent-classification.bats:83:@test "CT-IC-001: DEBUG classification - error keyword triggers debug" {
tests/intent-classification.bats:87:    result=$(get_intent_type "error in validation")
tests/intent-classification.bats:89:    [ "$result" = "debug" ]
tests/intent-classification.bats:92:@test "CT-IC-001: DEBUG classification - crash keyword triggers debug" {
tests/intent-classification.bats:96:    result=$(get_intent_type "app crash on startup")
tests/intent-classification.bats:98:    [ "$result" = "debug" ]
tests/intent-classification.bats:101:@test "CT-IC-001: DEBUG classification - issue keyword triggers debug" {
tests/intent-classification.bats:105:    result=$(get_intent_type "resolve the issue with caching")
tests/intent-classification.bats:107:    [ "$result" = "debug" ]
tests/intent-classification.bats:112:# Scenario: SC-IC-002 - Pure refactor intent
tests/intent-classification.bats:119:    result=$(get_intent_type "optimize database queries")
tests/intent-classification.bats:128:    result=$(get_intent_type "refactor the payment module")
tests/intent-classification.bats:137:    result=$(get_intent_type "clean up the code")
tests/intent-classification.bats:146:    result=$(get_intent_type "improve performance")
tests/intent-classification.bats:155:    result=$(get_intent_type "simplify the algorithm")
tests/intent-classification.bats:162:# Scenario: SC-IC-003 - Pure docs intent
tests/intent-classification.bats:169:    result=$(get_intent_type "write documentation for API")
tests/intent-classification.bats:178:    result=$(get_intent_type "add comment to function")
tests/intent-classification.bats:187:    result=$(get_intent_type "update the readme file")
tests/intent-classification.bats:196:    result=$(get_intent_type "explain how this works")
tests/intent-classification.bats:205:    result=$(get_intent_type "write a guide for setup")
tests/intent-classification.bats:212:# Scenario: SC-IC-004 - Default feature intent
tests/intent-classification.bats:219:    result=$(get_intent_type "add user registration")
tests/intent-classification.bats:228:    result=$(get_intent_type "create new endpoint")
tests/intent-classification.bats:237:    result=$(get_intent_type "build authentication system")
tests/intent-classification.bats:246:    result=$(get_intent_type "implement new feature")
tests/intent-classification.bats:255:    result=$(get_intent_type "something completely random")
tests/intent-classification.bats:265:@test "CT-IC-005: Priority - debug > refactor when both present" {
tests/intent-classification.bats:269:    result=$(get_intent_type "fix and optimize the login flow")
tests/intent-classification.bats:271:    [ "$result" = "debug" ]
tests/intent-classification.bats:278:    result=$(get_intent_type "improve and document the API")
tests/intent-classification.bats:283:@test "CT-IC-005: Priority - debug > docs when both present" {
tests/intent-classification.bats:287:    result=$(get_intent_type "fix the error and write documentation")
tests/intent-classification.bats:289:    [ "$result" = "debug" ]
tests/intent-classification.bats:292:@test "CT-IC-005: Priority - debug > refactor > docs combined" {
tests/intent-classification.bats:296:    result=$(get_intent_type "fix bug, optimize code, and write docs")
tests/intent-classification.bats:298:    [ "$result" = "debug" ]
tests/intent-classification.bats:304:# This test validates the interface contract
tests/intent-classification.bats:311:    result=$(get_intent_type "fix the bug")
tests/intent-classification.bats:315:        debug|refactor|docs|feature)
tests/intent-classification.bats:327:    local debug_result refactor_result docs_result feature_result
tests/intent-classification.bats:329:    debug_result=$(get_intent_type "fix bug")
tests/intent-classification.bats:330:    refactor_result=$(get_intent_type "optimize code")
tests/intent-classification.bats:331:    docs_result=$(get_intent_type "write documentation")
tests/intent-classification.bats:332:    feature_result=$(get_intent_type "add new button")
tests/intent-classification.bats:334:    [ "$debug_result" = "debug" ]
tests/intent-classification.bats:349:    result=$(get_intent_type "")
tests/intent-classification.bats:358:    result=$(get_intent_type "   ")
tests/intent-classification.bats:367:    result=$(get_intent_type "hello world")
tests/intent-classification.bats:377:@test "CT-IC-008: Case insensitive - uppercase FIX triggers debug" {
tests/intent-classification.bats:381:    result=$(get_intent_type "FIX THE BUG")
tests/intent-classification.bats:383:    [ "$result" = "debug" ]
tests/intent-classification.bats:386:@test "CT-IC-008: Case insensitive - mixed case Fix triggers debug" {
tests/intent-classification.bats:390:    result=$(get_intent_type "Fix the Bug")
tests/intent-classification.bats:392:    [ "$result" = "debug" ]
tests/intent-classification.bats:399:    result=$(get_intent_type "OPTIMIZE THE CODE")
tests/intent-classification.bats:408:    result=$(get_intent_type "Document the API")
tests/intent-classification.bats:414:# CT-IC-009: Compound Query - Supports combined intents
tests/intent-classification.bats:415:# Note: Returns highest priority intent from compound query
tests/intent-classification.bats:418:@test "CT-IC-009: Compound query - multiple debug keywords" {
tests/intent-classification.bats:422:    result=$(get_intent_type "fix the bug and resolve the crash issue")
tests/intent-classification.bats:424:    [ "$result" = "debug" ]
tests/intent-classification.bats:431:    result=$(get_intent_type "optimize and clean up the codebase")
tests/intent-classification.bats:440:    result=$(get_intent_type "I need to fix something in the authentication module")
tests/intent-classification.bats:442:    [ "$result" = "debug" ]
tests/intent-classification.bats:454:        "fix the bug"
tests/intent-classification.bats:458:        "debug login"
tests/intent-classification.bats:463:        "clean code"
tests/intent-classification.bats:471:            get_intent_type "$input" > /dev/null
tests/intent-classification.bats:497:    result=$(get_intent_type "!@#$%^&*()")
tests/intent-classification.bats:506:    result=$(get_intent_type "12345")
tests/intent-classification.bats:515:    result=$(get_intent_type "Ê∑ªÂä†Ê≥®Èáä")
tests/intent-classification.bats:517:    # Note: Current implementation requires at least one ASCII letter.
tests/intent-classification.bats:518:    # Pure Chinese input without letters defaults to feature due to boundary check.
tests/intent-classification.bats:527:    result=$(get_intent_type "add Ê≥®Èáä")
tests/intent-classification.bats:537:    long_input="This is a very long input string that contains the word fix somewhere in the middle and should still be classified correctly as a debug intent because it contains a debug keyword"
tests/intent-classification.bats:540:    result=$(get_intent_type "$long_input")
tests/intent-classification.bats:542:    [ "$result" = "debug" ]
tests/intent-classification.bats:549:    result=$(get_intent_type "I want to refactor")
tests/intent-classification.bats:558:    result=$(get_intent_type "Fix this issue please")
tests/intent-classification.bats:560:    [ "$result" = "debug" ]
tests/intent-classification.bats:567:@test "CT-IC-REGRESSION-001: All debug keywords recognized" {
tests/intent-classification.bats:570:    local keywords=("fix" "debug" "bug" "crash" "fail" "error" "issue" "resolve" "problem" "broken")
tests/intent-classification.bats:574:        result=$(get_intent_type "I need to $keyword something")
tests/intent-classification.bats:575:        [ "$result" = "debug" ] || fail "Keyword '$keyword' not recognized as debug"
tests/intent-classification.bats:586:        result=$(get_intent_type "I need to $keyword the code")
tests/intent-classification.bats:598:        result=$(get_intent_type "I need to add a $keyword")
tests/bug-locator.bats:2:# bug-locator.bats - AC-009 Bug Locator Regression Tests
tests/bug-locator.bats:4:# Purpose: Verify bug-locator.sh core functionality remains consistent after changes
tests/bug-locator.bats:6:# Run: bats tests/bug-locator.bats
tests/bug-locator.bats:9:# Change: enhance-code-intelligence
tests/bug-locator.bats:19:BUG_LOCATOR="${PROJECT_ROOT}/scripts/bug-locator.sh"
tests/bug-locator.bats:32:@test "BL-001: bug-locator.sh exists and is executable" {
tests/bug-locator.bats:91:@test "BL-009: CKB MCP unavailable returns empty candidates or hotspot fallback" {
tests/bug-locator.bats:156:@test "test_with_impact_field: --with-impact adds impact field to output" {
tests/bug-locator.bats:159:    run "$BUG_LOCATOR" --error "test error" --with-impact --format json
tests/bug-locator.bats:160:    skip_if_not_ready "$status" "$output" "bug-locator.sh --with-impact"
tests/bug-locator.bats:163:        skip_not_implemented "bug-locator impact json output"
tests/bug-locator.bats:166:    local has_impact
tests/bug-locator.bats:167:    has_impact=$(echo "$output" | jq 'if type=="array" then any(.impact?; . != null) else false end')
tests/bug-locator.bats:168:    if [ "$has_impact" != "true" ]; then
tests/bug-locator.bats:169:        skip_not_implemented "impact field"
tests/bug-locator.bats:173:@test "test_with_impact_total: impact.total_affected is present" {
tests/bug-locator.bats:176:    run "$BUG_LOCATOR" --error "test error" --with-impact --format json
tests/bug-locator.bats:177:    skip_if_not_ready "$status" "$output" "bug-locator.sh --with-impact"
tests/bug-locator.bats:180:        skip_not_implemented "bug-locator impact json output"
tests/bug-locator.bats:184:    total=$(echo "$output" | jq -r 'if type=="array" then .[0].impact.total_affected // empty else empty end')
tests/bug-locator.bats:186:        skip_not_implemented "impact total_affected"
tests/bug-locator.bats:190:@test "test_with_impact_files: impact.affected_files is an array" {
tests/bug-locator.bats:193:    run "$BUG_LOCATOR" --error "test error" --with-impact --format json
tests/bug-locator.bats:194:    skip_if_not_ready "$status" "$output" "bug-locator.sh --with-impact"
tests/bug-locator.bats:197:        skip_not_implemented "bug-locator impact json output"
tests/bug-locator.bats:201:    is_array=$(echo "$output" | jq -r 'if type=="array" then (.[] | select(.impact != null) | .impact.affected_files | type) else "" end' | head -n 1)
tests/bug-locator.bats:203:        skip_not_implemented "impact affected_files"
tests/bug-locator.bats:208:@test "test_with_impact_empty_files: impact.affected_files can be empty array" {
tests/bug-locator.bats:212:    run "$BUG_LOCATOR" --error "nonexistent_symbol_xyz_12345" --with-impact --format json
tests/bug-locator.bats:213:    skip_if_not_ready "$status" "$output" "bug-locator.sh --with-impact"
tests/bug-locator.bats:216:        skip_not_implemented "bug-locator impact json output"
tests/bug-locator.bats:223:            .[] | select(.impact != null) | .impact.affected_files | type
tests/bug-locator.bats:229:    # If we got impact data, affected_files must be an array
tests/bug-locator.bats:233:            skip_not_implemented "impact affected_files array type"
tests/bug-locator.bats:241:            any(.impact?.affected_files == null)
tests/bug-locator.bats:248:        skip_not_implemented "impact affected_files should not be null"
tests/bug-locator.bats:252:@test "test_with_impact_scoring: impact scoring adjusts final score" {
tests/bug-locator.bats:255:    run "$BUG_LOCATOR" --error "test error" --with-impact --format json
tests/bug-locator.bats:256:    skip_if_not_ready "$status" "$output" "bug-locator.sh --with-impact"
tests/bug-locator.bats:259:        skip_not_implemented "bug-locator impact json output"
tests/bug-locator.bats:267:        skip_not_implemented "impact scoring fields"
tests/bug-locator.bats:271:@test "test_without_impact_compat: default output remains compatible without impact" {
tests/bug-locator.bats:275:    skip_if_not_ready "$status" "$output" "bug-locator.sh default output"
tests/bug-locator.bats:278:        skip_not_implemented "bug-locator json output"
tests/bug-locator.bats:284:    # Verify impact field is NOT present in default output (backward compatibility)
tests/bug-locator.bats:285:    local has_impact
tests/bug-locator.bats:287:        has_impact=$(echo "$output" | jq 'any(.impact?; . != null)')
tests/bug-locator.bats:289:        # Object type - check for impact in candidates array or root
tests/bug-locator.bats:290:        has_impact=$(echo "$output" | jq '(.impact != null) or ((.candidates // []) | any(.impact?; . != null))')
tests/bug-locator.bats:293:    if [ "$has_impact" = "true" ]; then
tests/bug-locator.bats:294:        skip_not_implemented "backward compatibility: impact should not be present by default"
tests/semantic-anomaly.bats:11:    export SEMANTIC_ANOMALY_SCRIPT="${SCRIPTS_DIR}/semantic-anomaly.sh"
tests/semantic-anomaly.bats:12:    export FIXTURES_DIR="${BATS_TEST_DIRNAME}/fixtures/semantic-anomaly"
tests/semantic-anomaly.bats:283:@test "T-SA-012: False positive rate < 20% for clean code" {
tests/llm-rerank.bats:2:# llm-rerank.bats - LLM ÈáçÊéíÂ∫èÊµãËØï
tests/llm-rerank.bats:22:# ‰ª•‰∏ãÁéØÂ¢ÉÂèòÈáèÁî®‰∫éÊµãËØïÈáçËØïÈÄªËæëÔºåÈúÄË¶Å graph-rag.sh ÊîØÊåÅÔºö
tests/llm-rerank.bats:34:# ÂÆûÁé∞ËÄÖÂèØÂèÇËÄÉÊ≠§ËßÑËåÉÂú® graph-rag.sh ‰∏≠Ê∑ªÂä† mock ÊîØÊåÅ„ÄÇ
tests/llm-rerank.bats:41:GRAPH_RAG="$SCRIPT_DIR/graph-rag.sh"
tests/llm-rerank.bats:47:# Usage: use_fixture "llm-rerank-enabled.yaml"
tests/llm-rerank.bats:60:# ÈÄöËøáÊ£ÄÊü• graph-rag.sh Ê∫êÁ†ÅÊàñËøêË°åÊµãËØïÊù•Âà§Êñ≠
tests/llm-rerank.bats:69:    if "$GRAPH_RAG" --help 2>&1 | grep -qi "mock\|test\|debug"; then
tests/llm-rerank.bats:84:    use_fixture "llm-rerank-disabled.yaml" 2>/dev/null || {
tests/llm-rerank.bats:88:  llm_rerank:
tests/llm-rerank.bats:107:@test "SC-LR-001: graph-rag skips rerank when disabled" {
tests/llm-rerank.bats:110:    run "$GRAPH_RAG" --query "test query" --rerank
tests/llm-rerank.bats:112:    skip_if_not_ready "$status" "$output" "graph-rag.sh rerank disabled"
tests/llm-rerank.bats:115:    assert_json_field "$output" ".metadata.reranked" "false"
tests/llm-rerank.bats:119:@test "SC-LR-002: graph-rag reranks when enabled with mock" {
tests/llm-rerank.bats:123:    use_fixture "llm-rerank-enabled.yaml" || skip "Fixture llm-rerank-enabled.yaml not found"
tests/llm-rerank.bats:129:    run "$GRAPH_RAG" --query "test query" --rerank
tests/llm-rerank.bats:131:    skip_if_not_ready "$status" "$output" "graph-rag.sh rerank enabled"
tests/llm-rerank.bats:134:    assert_json_field "$output" ".metadata.reranked" "true"
tests/llm-rerank.bats:142:@test "SC-LR-003: graph-rag falls back on timeout" {
tests/llm-rerank.bats:147:  llm_rerank:
tests/llm-rerank.bats:156:    run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:158:    skip_if_not_ready "$status" "$output" "graph-rag.sh timeout"
tests/llm-rerank.bats:164:@test "SC-LR-004: graph-rag falls back when API key missing" {
tests/llm-rerank.bats:169:  llm_rerank:
tests/llm-rerank.bats:176:    run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:178:    skip_if_not_ready "$status" "$output" "graph-rag.sh api key"
tests/llm-rerank.bats:184:@test "SC-LR-005: graph-rag uses OpenAI provider" {
tests/llm-rerank.bats:188:    use_fixture "llm-rerank-openai.yaml" || skip "Fixture llm-rerank-openai.yaml not found"
tests/llm-rerank.bats:193:    run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:195:    skip_if_not_ready "$status" "$output" "graph-rag.sh openai"
tests/llm-rerank.bats:201:@test "SC-LR-006: graph-rag uses Ollama provider" {
tests/llm-rerank.bats:207:        use_fixture "llm-rerank-ollama.yaml" || skip "Fixture llm-rerank-ollama.yaml not found"
tests/llm-rerank.bats:211:        run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:213:        skip_if_not_ready "$status" "$output" "graph-rag.sh ollama config"
tests/llm-rerank.bats:225:    use_fixture "llm-rerank-ollama.yaml" || skip "Fixture llm-rerank-ollama.yaml not found"
tests/llm-rerank.bats:227:    run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:229:    skip_if_not_ready "$status" "$output" "graph-rag.sh ollama"
tests/llm-rerank.bats:235:@test "SC-LR-007: graph-rag falls back on invalid JSON response" {
tests/llm-rerank.bats:240:  llm_rerank:
tests/llm-rerank.bats:248:    run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:250:    skip_if_not_ready "$status" "$output" "graph-rag.sh invalid json"
tests/llm-rerank.bats:256:@test "SC-LR-008: graph-rag truncates long candidates" {
tests/llm-rerank.bats:264:  llm_rerank:
tests/llm-rerank.bats:280:    run "$GRAPH_RAG" --query "test" --rerank --format json
tests/llm-rerank.bats:282:    skip_if_not_ready "$status" "$output" "graph-rag.sh truncation"
tests/llm-rerank.bats:319:@test "SC-LR-009: graph-rag retries on transient failure" {
tests/llm-rerank.bats:324:        skip "LLM mock mechanism not implemented in graph-rag.sh (see Mock Interface Spec in file header)"
tests/llm-rerank.bats:329:  llm_rerank:
tests/llm-rerank.bats:340:    run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:342:    skip_if_not_ready "$status" "$output" "graph-rag.sh retry"
tests/llm-rerank.bats:349:    if [[ "$output" == *'"reranked"'*'true'* ]]; then
tests/llm-rerank.bats:366:# È¢ÑÊúüË°å‰∏∫ÔºöËÑöÊú¨Â∫îÈôçÁ∫ßÂà∞ÂéüÂßãÊéíÂ∫èÔºàreranked=falseÔºâ
tests/llm-rerank.bats:368:@test "SC-LR-010: graph-rag falls back after max retries" {
tests/llm-rerank.bats:373:        skip "LLM mock mechanism not implemented in graph-rag.sh (see Mock Interface Spec in file header)"
tests/llm-rerank.bats:378:  llm_rerank:
tests/llm-rerank.bats:389:    run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:391:    skip_if_not_ready "$status" "$output" "graph-rag.sh max retry"
tests/llm-rerank.bats:398:    if [[ "$output" == *'"reranked"'*'false'* ]]; then
tests/llm-rerank.bats:414:@test "SC-LR-011: graph-rag skips rerank for empty candidates" {
tests/llm-rerank.bats:419:  llm_rerank:
tests/llm-rerank.bats:427:    run "$GRAPH_RAG" --query "nonexistent_xyz_123" --rerank
tests/llm-rerank.bats:429:    skip_if_not_ready "$status" "$output" "graph-rag.sh empty"
tests/llm-rerank.bats:437:@test "CT-LR-003: rerank result contains required fields" {
tests/llm-rerank.bats:442:  llm_rerank:
tests/llm-rerank.bats:450:    run "$GRAPH_RAG" --query "test" --rerank
tests/llm-rerank.bats:452:    skip_if_not_ready "$status" "$output" "graph-rag.sh format"
tests/keystroke-cancel.bats:11:    export DAEMON_SCRIPT="${SCRIPTS_DIR}/daemon.sh"
tests/keystroke-cancel.bats:36:        result=$(daemon_start_with_cancel "sleep" "10" "--name=keystroke-cancel-test")
tests/keystroke-cancel.bats:44:        daemon_cancel "$pid"
tests/keystroke-cancel.bats:63:@test "T-KC-009: Cancel returns exit code 130" {
tests/keystroke-cancel.bats:69:    result=$(daemon_start_with_cancel "sleep" "10")
tests/keystroke-cancel.bats:73:    daemon_cancel "$pid"
tests/keystroke-cancel.bats:74:    wait "$pid" 2>/dev/null || exit_code=$?
tests/keystroke-cancel.bats:76:    [ "$exit_code" -eq 130 ]
tests/keystroke-cancel.bats:103:    result=$(daemon_start_with_cancel "${TEMP_DIR}/parent.sh")
tests/keystroke-cancel.bats:115:    daemon_cancel "$parent_pid"
tests/keystroke-cancel.bats:147:    result=$(daemon_start_with_cancel "${TEMP_DIR}/with-resources.sh")
tests/keystroke-cancel.bats:155:    daemon_cancel "$pid"
tests/keystroke-cancel.bats:172:    result=$(daemon_start_with_cancel "sleep" "10")
tests/keystroke-cancel.bats:180:    daemon_cancel "$pid"
tests/keystroke-cancel.bats:210:        result=$(daemon_start_with_cancel "sleep" "100")
tests/keystroke-cancel.bats:217:    daemon_cancel_all
tests/keystroke-cancel.bats:247:    result=$(daemon_start_with_cancel "${TEMP_DIR}/ignore-signals.sh")
tests/keystroke-cancel.bats:251:    daemon_cancel "$pid" --timeout 100
tests/keystroke-cancel.bats:258:    wait "$pid" 2>/dev/null || exit_code=$?
tests/keystroke-cancel.bats:259:    [ "$exit_code" -eq 137 ] || [ "$exit_code" -eq 130 ]
tests/keystroke-cancel.bats:278:    result=$(daemon_start_with_cancel "${TEMP_DIR}/progressive.sh")
tests/keystroke-cancel.bats:286:    cancel_result=$(daemon_cancel "$pid")
tests/keystroke-cancel.bats:305:    result=$(daemon_start_with_cancel "sleep" "10")
tests/keystroke-cancel.bats:307:    daemon_cancel "$pid"
tests/keystroke-cancel.bats:317:        result=$(daemon_start_with_cancel "sleep" "10")
tests/keystroke-cancel.bats:321:        daemon_cancel "$pid"
tests/context-compressor.bats:11:    export CONTEXT_COMPRESSOR_SCRIPT="${SCRIPTS_DIR}/context-compressor.sh"
tests/context-compressor.bats:12:    export FIXTURES_DIR="${BATS_TEST_DIRNAME}/fixtures/context-compressor"
tests/context-compressor.bats:47:    private readonly cache: Cache;
tests/context-compressor.bats:49:    constructor(db: Database, cache: Cache) {
tests/context-compressor.bats:51:        this.cache = cache;
tests/context-compressor.bats:184:    result=$("$CONTEXT_COMPRESSOR_SCRIPT" --budget 500 --hotspot "$FIXTURES_DIR/")
tests/context-compressor.bats:281:@test "T-CC-005: Incremental compression reuses cache" {
tests/context-compressor.bats:282:    create_ts_fixture "cached" 100
tests/context-compressor.bats:288:    result1=$("$CONTEXT_COMPRESSOR_SCRIPT" --cache "$FIXTURES_DIR/cached.ts")
tests/context-compressor.bats:294:    result2=$("$CONTEXT_COMPRESSOR_SCRIPT" --cache "$FIXTURES_DIR/cached.ts")
tests/context-compressor.bats:302:    echo "$result2" | jq -e '.metadata.cache_hits > 0'
tests/context-compressor.bats:344:    cat > "$FIXTURES_DIR/semantic.ts" << 'EOF'
tests/context-compressor.bats:345:// Critical semantic elements that must be preserved:
tests/context-compressor.bats:373:    result=$("$CONTEXT_COMPRESSOR_SCRIPT" --mode skeleton "$FIXTURES_DIR/semantic.ts")
tests/context-compressor.bats:378:    semantic_elements=(
tests/context-compressor.bats:389:    for element in "${semantic_elements[@]}"; do
tests/context-compressor.bats:395:    total=${#semantic_elements[@]}
tests/boundary-detector.bats:2:# boundary-detector.bats - AC-004 Boundary Detection Acceptance Tests
tests/boundary-detector.bats:4:# Purpose: Verify boundary-detector.sh boundary detection functionality
tests/boundary-detector.bats:6:# Run: bats tests/boundary-detector.bats
tests/boundary-detector.bats:9:# Change: enhance-code-intelligence
tests/boundary-detector.bats:17:BOUNDARY_DETECTOR="${PROJECT_ROOT}/scripts/boundary-detector.sh"
tests/boundary-detector.bats:23:@test "BD-001: boundary-detector.sh exists and is executable" {
tests/boundary-detector.bats:30:    [[ "$output" == *"boundary"* ]] || [[ "$output" == *"Boundary"* ]]
tests/boundary-detector.bats:37:@test "BD-002: detect library code (node_modules)" {
tests/boundary-detector.bats:44:@test "BD-002b: detect library code (vendor)" {
tests/boundary-detector.bats:50:@test "BD-003: detect generated code (dist)" {
tests/boundary-detector.bats:56:@test "BD-003b: detect generated code (build)" {
tests/boundary-detector.bats:62:@test "BD-004: detect user code (src)" {
tests/boundary-detector.bats:190:@test "BD-BOUNDARY-006: unicode in path handled" {
tests/boundary-detector.bats:192:    # Unicode paths should be handled
tests/boundary-detector.bats:195:    skip "Unicode path not yet supported"
tests/boundary-detector.bats:210:# Spec: dev-playbooks/changes/algorithm-optimization-parity/specs/boundary-detection/spec.md
tests/boundary-detector.bats:213:@test "CT-BD-001: node_modules fast path matching - returns library type" {
tests/boundary-detector.bats:219:    skip_if_not_ready "$status" "$output" "CT-BD-001: Fast path matching"
tests/boundary-detector.bats:228:@test "CT-BD-002: vendor fast path matching - returns library type" {
tests/boundary-detector.bats:234:    skip_if_not_ready "$status" "$output" "CT-BD-002: Vendor fast path matching"
tests/boundary-detector.bats:243:@test "CT-BD-003: user code path - returns user type" {
tests/boundary-detector.bats:249:    skip_if_not_ready "$status" "$output" "CT-BD-003: User code path detection"
tests/boundary-detector.bats:273:@test "CT-BD-005: dist directory fast path - returns generated/library type" {
tests/boundary-detector.bats:279:    skip_if_not_ready "$status" "$output" "CT-BD-005: Dist directory fast path"
tests/graph-rag.bats:2:# graph-rag.bats - M4: Subgraph Smart Pruning Acceptance Tests
tests/graph-rag.bats:4:# Purpose: Verify graph-rag.sh subgraph smart pruning functionality
tests/graph-rag.bats:6:# Run: bats tests/graph-rag.bats
tests/graph-rag.bats:17:GRAPH_RAG="${PROJECT_ROOT}/scripts/graph-rag.sh"
tests/graph-rag.bats:23:@test "GR-BASE-001: graph-rag.sh exists and is executable" {
tests/graph-rag.bats:27:@test "GR-BASE-002: graph-rag.sh shows help" {
tests/graph-rag.bats:33:@test "GR-BASE-003: graph-rag.sh shows version" {
tests/graph-rag.bats:40:# M4: Subgraph Smart Pruning Tests (T-SP-001 ~ T-SP-008)
tests/graph-rag.bats:45:# When: Call graph-rag.sh search "query" --budget 4000
tests/graph-rag.bats:48:    run "$GRAPH_RAG" --query "test query" --token-budget 4000 --format json --mock-embedding 2>&1
tests/graph-rag.bats:63:# Given: Known relevance/hotspot/distance values
tests/graph-rag.bats:65:# Then: Priority = relevance * 0.4 + hotspot * 0.3 + (1/distance) * 0.3
tests/graph-rag.bats:68:    # Priority = relevance * 0.4 + hotspot * 0.3 + (1/distance) * 0.3
tests/graph-rag.bats:70:    run "$GRAPH_RAG" --query "priority test" --format json --mock-embedding 2>&1
tests/graph-rag.bats:93:        # È™åËØÅ‰ºòÂÖàÁ∫ßÂÖ¨ÂºèÔºöPriority = relevance * 0.4 + hotspot * 0.3 + (1/distance) * 0.3
tests/graph-rag.bats:96:        has_formula_fields=$(echo "$candidates" | jq '.[0] | has("relevance") and has("hotspot") and has("distance")')
tests/graph-rag.bats:99:            local relevance hotspot distance expected_priority actual_priority
tests/graph-rag.bats:101:            hotspot=$(echo "$candidates" | jq -r '.[0].hotspot // 0')
tests/graph-rag.bats:106:            expected_priority=$(awk -v r="$relevance" -v h="$hotspot" -v d="$distance" \
tests/graph-rag.bats:125:@test "T-SP-003: test_budget_boundary - precise budget boundary control" {
tests/graph-rag.bats:127:    run "$GRAPH_RAG" --query "boundary test" --token-budget 1000 --format json --mock-embedding 2>&1
tests/graph-rag.bats:128:    skip_if_not_ready "$status" "$output" "Budget boundary control"
tests/graph-rag.bats:145:    run "$GRAPH_RAG" --query "default budget test" --format json --mock-embedding 2>&1
tests/graph-rag.bats:163:    run "$GRAPH_RAG" --query "zero budget test" --token-budget 0 --format json --mock-embedding 2>&1
tests/graph-rag.bats:186:    run "$GRAPH_RAG" --query "oversized test" --token-budget 100 --format json --mock-embedding 2>&1
tests/graph-rag.bats:207:# Given: intent-learner has preference data
tests/graph-rag.bats:210:@test "T-SP-007: test_intent_preference_integration - intent learner integration" {
tests/graph-rag.bats:211:    # This test requires intent-learner preference data
tests/graph-rag.bats:216:    run "$GRAPH_RAG" --query "auth user" --format json --mock-embedding 2>&1
tests/graph-rag.bats:225:    # Check if metadata indicates intent preference was applied
tests/graph-rag.bats:229:    # If intent integration is implemented, there should be some indication
tests/graph-rag.bats:230:    [[ "$metadata" == *"intent"* ]] || \
tests/graph-rag.bats:253:    run "$GRAPH_RAG" --query "test" --cwd "$TEST_TEMP_DIR" --format json --mock-embedding 2>&1
tests/graph-rag.bats:293:    run "$GRAPH_RAG" --query "test" --token-budget -100 --format json --mock-embedding 2>&1
tests/graph-rag.bats:313:    run "$GRAPH_RAG" --query "test" --token-budget 1000000 --format json --mock-embedding 2>&1
tests/graph-rag.bats:345:    run "$GRAPH_RAG" --query "output test" --format json --mock-embedding 2>&1
tests/graph-rag.bats:356:    run "$GRAPH_RAG" --query "output test" --format json --mock-embedding 2>&1
tests/graph-rag.bats:367:    run "$GRAPH_RAG" --query "output test" --format json --mock-embedding 2>&1
tests/graph-rag.bats:384:# Priority = relevance√ó0.4 + hotspot√ó0.3 + (1/distance)√ó0.3
tests/graph-rag.bats:385:# ËæìÂÖ•: {relevance_score: 0.8, hotspot: 0.6, distance: 2}
tests/graph-rag.bats:389:    run "$GRAPH_RAG" --query "priority formula test" --format json --mock-embedding 2>&1
tests/graph-rag.bats:405:    # È™åËØÅ‰ºòÂÖàÁ∫ßÂÖ¨ÂºèÔºöPriority = relevance √ó 0.4 + hotspot √ó 0.3 + (1/distance) √ó 0.3
tests/graph-rag.bats:407:    has_formula_fields=$(echo "$candidates" | jq '.[0] | has("relevance_score") and has("hotspot") and has("distance") and has("priority")')
tests/graph-rag.bats:414:    local relevance hotspot distance actual_priority expected_priority
tests/graph-rag.bats:416:    hotspot=$(echo "$candidates" | jq -r '.[0].hotspot // 0')
tests/graph-rag.bats:421:    expected_priority=$(awk -v r="$relevance" -v h="$hotspot" -v d="$distance" \
tests/graph-rag.bats:437:        fail "Priority formula mismatch: expected $expected_priority, actual $actual_priority (relevance=$relevance, hotspot=$hotspot, distance=$distance)"
tests/graph-rag.bats:443:# ËæìÂÖ•: {relevance_score: 0.5, hotspot: 0.5, distance: 0}
tests/graph-rag.bats:445:@test "CT-PS-002: zero distance boundary handling" {
tests/graph-rag.bats:459:    run "$GRAPH_RAG" --query "zeroDistanceTest" --cwd "$test_dir" --format json --mock-embedding 2>&1
tests/graph-rag.bats:489:# Ë¶ÜÁõñÂú∫ÊôØ SC-PS-003: Áº∫Â∞ë hotspot Âíå distance Êó∂‰ΩøÁî®ÈªòËÆ§ÂÄº 0 Âíå 1
tests/graph-rag.bats:490:# ËæìÂÖ•: {relevance_score: 0.9}ÔºàÁº∫Â∞ë hotspot Âíå distanceÔºâ
tests/graph-rag.bats:493:    run "$GRAPH_RAG" --query "missing fields test" --format json --mock-embedding 2>&1
tests/graph-rag.bats:525:# ÈÖçÁΩÆ: {relevance: 0.6, hotspot: 0.2, distance: 0.2}
tests/graph-rag.bats:526:# ËæìÂÖ•: {relevance_score: 0.8, hotspot: 0.4, distance: 1}
tests/graph-rag.bats:539:    hotspot: 0.2
tests/graph-rag.bats:549:    run "$GRAPH_RAG" --query "customWeightTest" --cwd "$test_dir" --format json --mock-embedding 2>&1
tests/graph-rag.bats:582:    run "$GRAPH_RAG" --query "greedy selection test" --token-budget 500 --format json --mock-embedding 2>&1
tests/graph-rag.bats:622:    run "$GRAPH_RAG" --query "large fragment test" --token-budget 200 --format json --mock-embedding 2>&1
tests/graph-rag.bats:652:    run "$GRAPH_RAG" --query "all oversized test" --token-budget 10 --format json --mock-embedding 2>&1
tests/graph-rag.bats:681:    run "$GRAPH_RAG" --query "zero budget greedy test" --token-budget 0 --format json --mock-embedding 2>&1
tests/graph-rag.bats:725:    run "$GRAPH_RAG" --query "token estimation" --cwd "$TEST_TEMP_DIR" --format json --mock-embedding 2>&1
tests/upgrade-capabilities.bats:4:# Change-ID: 20260118-0057-upgrade-code-intelligence-capabilities
tests/upgrade-capabilities.bats:14:    export TEST_DB_PATH="${TEST_TEMP_DIR}/test-graph.db"
tests/upgrade-capabilities.bats:17:    if [ -f ".devbooks/graph.db" ]; then
tests/upgrade-capabilities.bats:18:        backup_file ".devbooks/graph.db"
tests/upgrade-capabilities.bats:25:    if [ -f ".devbooks/graph.db.bak" ]; then
tests/upgrade-capabilities.bats:26:        restore_file ".devbooks/graph.db"
tests/upgrade-capabilities.bats:53:    bash scripts/graph-store.sh init >/dev/null 2>&1
tests/upgrade-capabilities.bats:57:    source scripts/scip-to-graph.sh
tests/upgrade-capabilities.bats:61:    bash scripts/graph-store.sh batch-import --file "$temp_json" >/dev/null 2>&1
tests/upgrade-capabilities.bats:87:    bash scripts/graph-store.sh init >/dev/null 2>&1
tests/upgrade-capabilities.bats:91:    source scripts/scip-to-graph.sh
tests/upgrade-capabilities.bats:95:    bash scripts/graph-store.sh batch-import --file "$temp_json" >/dev/null 2>&1
tests/upgrade-capabilities.bats:119:    bash scripts/graph-store.sh init >/dev/null 2>&1
tests/upgrade-capabilities.bats:123:    source scripts/scip-to-graph.sh
tests/upgrade-capabilities.bats:127:    bash scripts/graph-store.sh batch-import --file "$temp_json" >/dev/null 2>&1
tests/upgrade-capabilities.bats:150:    run bash scripts/scip-to-graph.sh --input "$test_file" --output "$TEST_DB_PATH" --fallback-regex
tests/upgrade-capabilities.bats:194:    run bash scripts/graph-store.sh migrate --apply
tests/upgrade-capabilities.bats:240:    run bash scripts/graph-store.sh migrate --apply
tests/upgrade-capabilities.bats:269:    run bash scripts/graph-store.sh migrate --apply
tests/upgrade-capabilities.bats:294:    run bash scripts/graph-store.sh migrate --apply
tests/upgrade-capabilities.bats:310:@test "T-CKB-001 (@smoke): CKB available returns real graph data" {
tests/upgrade-capabilities.bats:323:    # Act: ËøêË°å graph-rag Êü•ËØ¢Ôºå‰ΩøÁî® Mock CKB
tests/upgrade-capabilities.bats:325:    run bash scripts/graph-rag.sh --query "test function" --format json
tests/upgrade-capabilities.bats:349:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:353:    run bash scripts/graph-rag.sh --query "test function" --format json
tests/upgrade-capabilities.bats:383:    run bash scripts/graph-rag.sh --query "test function" --format json
tests/upgrade-capabilities.bats:429:    run bash scripts/graph-rag.sh --query "test1" --format json
tests/upgrade-capabilities.bats:432:    run bash scripts/graph-rag.sh --query "test2" --format json
tests/upgrade-capabilities.bats:454:    run bash scripts/graph-rag.sh --query "test function" --format json
tests/upgrade-capabilities.bats:473:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:495:    run bash scripts/graph-rag.sh --query "test function" --fusion-depth 0 --format json
tests/upgrade-capabilities.bats:500:    run bash scripts/graph-rag.sh --query "test function" --fusion-depth 1 --format json
tests/upgrade-capabilities.bats:518:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:522:    run bash scripts/graph-rag.sh --query "test function" --fusion-depth 1 --budget "$budget" --format json
tests/upgrade-capabilities.bats:537:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:540:    local mock_ckb_script="${TEST_TEMP_DIR}/mock-ckb-fast.sh"
tests/upgrade-capabilities.bats:550:    run bash scripts/graph-rag.sh --query "test function" --fusion-depth 0 --format json
tests/upgrade-capabilities.bats:558:    run bash scripts/graph-rag.sh --query "test function" --fusion-depth 1 --format json
tests/upgrade-capabilities.bats:572:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:585:    run bash scripts/graph-rag.sh --query "testFunc" --fusion-depth 1 --format json
tests/upgrade-capabilities.bats:597:    # È™åËØÅÁªìÊûúÂåÖÂê´ÈôçÁ∫ßÊï∞ÊçÆÔºàgraph-rag.sh ËæìÂá∫‰ΩøÁî® .candidates ‰∏çÊòØ .resultsÔºâ
tests/upgrade-capabilities.bats:608:    # Arrange: Á°Æ‰øù daemon Êú™ËøêË°å
tests/upgrade-capabilities.bats:609:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:611:    # Act: ÂêØÂä® daemon
tests/upgrade-capabilities.bats:612:    run bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:618:    run bash scripts/daemon.sh status --format json
tests/upgrade-capabilities.bats:632:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:636:    # Arrange: Á°Æ‰øù daemon Êú™ËøêË°å
tests/upgrade-capabilities.bats:637:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:642:    run bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:647:    [ "$elapsed_time" -lt 2 ] || fail "Expected daemon startup < 2s, but took ${elapsed_time}s"
tests/upgrade-capabilities.bats:649:    # È™åËØÅ daemon Â∑≤ÂêØÂä®
tests/upgrade-capabilities.bats:650:    run bash scripts/daemon.sh status
tests/upgrade-capabilities.bats:654:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:657:@test "T-WARMUP-003: Warmup timeout 30s does not affect daemon" {
tests/upgrade-capabilities.bats:658:    # Arrange: Á°Æ‰øù daemon Êú™ËøêË°å
tests/upgrade-capabilities.bats:659:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:664:    # Act: ÂêØÂä® daemon
tests/upgrade-capabilities.bats:665:    run bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:670:    # Ê£ÄÊü• daemon Áä∂ÊÄÅ
tests/upgrade-capabilities.bats:671:    run bash scripts/daemon.sh status --format json
tests/upgrade-capabilities.bats:674:    # Assert: È™åËØÅ daemon ‰ªçÂú®ËøêË°å
tests/upgrade-capabilities.bats:681:    # daemon.sh ËæìÂá∫‰ΩøÁî® .running Âíå .state ËÄå‰∏çÊòØ .status
tests/upgrade-capabilities.bats:682:    local daemon_running
tests/upgrade-capabilities.bats:683:    daemon_running=$(echo "$json" | jq -r '.running' 2>/dev/null || echo "false")
tests/upgrade-capabilities.bats:684:    [ "$daemon_running" = "true" ] || fail "Expected daemon to be running after warmup timeout"
tests/upgrade-capabilities.bats:687:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:696:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:698:    # Act: ËøêË°å graph-rag Êü•ËØ¢
tests/upgrade-capabilities.bats:699:    run bash scripts/graph-rag.sh --query "test" --format json
tests/upgrade-capabilities.bats:722:    run bash scripts/graph-rag.sh --query "test" --format json
tests/upgrade-capabilities.bats:741:    run bash scripts/graph-rag.sh --query "test" --fusion-depth 1 --format json
tests/upgrade-capabilities.bats:769:    run bash scripts/graph-rag.sh --query "test" --format json
tests/upgrade-capabilities.bats:784:# Contract Tests: graph-store.sh CLI (CT-GS-001 ~ CT-GS-005)
tests/upgrade-capabilities.bats:789:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:792:    run bash scripts/graph-store.sh migrate --status --format json
tests/upgrade-capabilities.bats:814:    run bash scripts/graph-store.sh migrate --apply
tests/upgrade-capabilities.bats:835:    run bash scripts/graph-store.sh migrate --apply
tests/upgrade-capabilities.bats:849:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:857:    run bash scripts/graph-store.sh stats --format json
tests/upgrade-capabilities.bats:884:    run bash scripts/graph-store.sh migrate --apply
tests/upgrade-capabilities.bats:896:# Contract Tests: graph-rag.sh CLI (CT-GR-001 ~ CT-GR-007)
tests/upgrade-capabilities.bats:901:    run bash scripts/graph-rag.sh --query "test" --fusion-depth 2 --format json
tests/upgrade-capabilities.bats:915:    run bash scripts/graph-rag.sh --query "test" --format json
tests/upgrade-capabilities.bats:938:    run bash scripts/graph-rag.sh --query "test" --fusion-depth 2 --format json
tests/upgrade-capabilities.bats:953:run bash scripts/graph-rag.sh --query "test" --include-virtual --format json
tests/upgrade-capabilities.bats:962:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:965:    run bash scripts/graph-rag.sh --query "test" --format json
tests/upgrade-capabilities.bats:983:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:986:    run bash scripts/graph-rag.sh --query "test" --fusion-depth 0 --format json
tests/upgrade-capabilities.bats:989:    run bash scripts/graph-rag.sh --query "test" --fusion-depth 1 --format json
tests/upgrade-capabilities.bats:1002:    run bash scripts/graph-rag.sh --query "test" --fusion-depth 0 --format json
tests/upgrade-capabilities.bats:1015:# Contract Tests: daemon.sh CLI (CT-DM-001 ~ CT-DM-007)
tests/upgrade-capabilities.bats:1020:    # Arrange: Á°Æ‰øù daemon Êú™ËøêË°å
tests/upgrade-capabilities.bats:1021:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1023:    # Act: ÂêØÂä® daemon
tests/upgrade-capabilities.bats:1024:    run bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:1030:    run bash scripts/daemon.sh status --format json
tests/upgrade-capabilities.bats:1042:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1046:    # Arrange: Á°Æ‰øù daemon ËøêË°å
tests/upgrade-capabilities.bats:1047:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1048:    bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:1051:    run bash scripts/daemon.sh status --format json
tests/upgrade-capabilities.bats:1063:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1067:    # Arrange: ÂêØÂä® daemon Âπ∂Á≠âÂæÖÈ¢ÑÁÉ≠ÂÆåÊàê
tests/upgrade-capabilities.bats:1068:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1069:    bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:1073:    run bash scripts/daemon.sh status --format json
tests/upgrade-capabilities.bats:1085:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1091:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1094:    # Act: ÂêØÂä® daemon
tests/upgrade-capabilities.bats:1095:    run bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:1099:    run bash scripts/daemon.sh status --format json
tests/upgrade-capabilities.bats:1102:    # Assert: È™åËØÅ daemon ‰ªçÂú®ËøêË°å
tests/upgrade-capabilities.bats:1106:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1110:    # Arrange: Á¶ÅÁî®È¢ÑÁÉ≠Ôºàdaemon.sh ‰ΩøÁî® DAEMON_ ÂâçÁºÄÔºâ
tests/upgrade-capabilities.bats:1111:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1114:    # Act: ÂêØÂä® daemon
tests/upgrade-capabilities.bats:1115:    run bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:1118:    run bash scripts/daemon.sh status --format json
tests/upgrade-capabilities.bats:1131:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1136:    # Arrange: Á°Æ‰øù daemon Êú™ËøêË°å
tests/upgrade-capabilities.bats:1137:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1142:    run bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:1147:    [ "$elapsed_time" -le 3 ] || fail "Expected daemon startup <= 3s, but took ${elapsed_time}s"
tests/upgrade-capabilities.bats:1150:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1154:    # Arrange: ÂêØÂä® daemon
tests/upgrade-capabilities.bats:1155:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1156:    bash scripts/daemon.sh start
tests/upgrade-capabilities.bats:1162:    run bash scripts/daemon.sh status --format json
tests/upgrade-capabilities.bats:1169:    # daemon.sh ËæìÂá∫ .running Âíå .state ËÄå‰∏çÊòØ .status
tests/upgrade-capabilities.bats:1178:    bash scripts/daemon.sh stop 2>/dev/null || true
tests/upgrade-capabilities.bats:1187:    run bash scripts/graph-rag.sh --query "" --format json
tests/upgrade-capabilities.bats:1200:    run bash scripts/graph-rag.sh --query "$long_query" --format json
tests/upgrade-capabilities.bats:1213:    run bash scripts/graph-rag.sh --query "test" --fusion-depth -1 --format json
tests/upgrade-capabilities.bats:1222:    bash scripts/graph-store.sh init
tests/upgrade-capabilities.bats:1225:    run bash scripts/graph-rag.sh --query "test" --format json
tests/scip-to-graph.bats:2:# scip-to-graph.bats - SCIP Ëß£ÊûêËΩ¨Êç¢ÊµãËØï
tests/scip-to-graph.bats:23:SCIP_TO_GRAPH="$SCRIPT_DIR/scip-to-graph.sh"
tests/scip-to-graph.bats:24:GRAPH_STORE="$SCRIPT_DIR/graph-store.sh"
tests/scip-to-graph.bats:28:    export GRAPH_DB_PATH="$TEST_TEMP_DIR/graph.db"
tests/scip-to-graph.bats:42:@test "SC-SP-001: scip-to-graph parse creates nodes and edges from SCIP index" {
tests/scip-to-graph.bats:44:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:46:    export SCIP_INDEX_PATH="$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:50:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh parse"
tests/scip-to-graph.bats:68:@test "SC-SP-002: scip-to-graph maps Definition (symbol_roles=1) to DEFINES" {
tests/scip-to-graph.bats:70:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:72:    export SCIP_INDEX_PATH="$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:75:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh parse"
tests/scip-to-graph.bats:84:@test "SC-SP-003: scip-to-graph maps ReadAccess (symbol_roles=8) to CALLS" {
tests/scip-to-graph.bats:86:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:88:    export SCIP_INDEX_PATH="$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:91:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh parse"
tests/scip-to-graph.bats:104:@test "SC-SP-004: scip-to-graph parse fails when SCIP index not found" {
tests/scip-to-graph.bats:107:    export SCIP_INDEX_PATH="$TEST_TEMP_DIR/nonexistent.scip"
tests/scip-to-graph.bats:111:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh error handling"
tests/scip-to-graph.bats:114:    assert_contains "$output" "npx scip-typescript"
tests/scip-to-graph.bats:118:@test "SC-SP-005: scip-to-graph falls back to regex when SCIP parsing fails" {
tests/scip-to-graph.bats:122:    echo "invalid protobuf data" > "$TEST_TEMP_DIR/bad.scip"
tests/scip-to-graph.bats:123:    export SCIP_INDEX_PATH="$TEST_TEMP_DIR/bad.scip"
tests/scip-to-graph.bats:134:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh fallback"
tests/scip-to-graph.bats:164:@test "SC-SP-006: scip-to-graph detects index newer than database" {
tests/scip-to-graph.bats:166:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:169:    cp "$BATS_TEST_DIRNAME/../index.scip" "$TEST_TEMP_DIR/index.scip"
tests/scip-to-graph.bats:170:    export SCIP_INDEX_PATH="$TEST_TEMP_DIR/index.scip"
tests/scip-to-graph.bats:174:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh parse"
tests/scip-to-graph.bats:182:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh incremental"
tests/scip-to-graph.bats:188:@test "SC-SP-007: scip-to-graph skips parse when database is up-to-date" {
tests/scip-to-graph.bats:190:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:193:    cp "$BATS_TEST_DIRNAME/../index.scip" "$TEST_TEMP_DIR/index.scip"
tests/scip-to-graph.bats:194:    export SCIP_INDEX_PATH="$TEST_TEMP_DIR/index.scip"
tests/scip-to-graph.bats:198:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh parse"
tests/scip-to-graph.bats:206:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh incremental skip"
tests/scip-to-graph.bats:212:@test "SC-SP-008: scip-to-graph force rebuilds database" {
tests/scip-to-graph.bats:214:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:216:    export SCIP_INDEX_PATH="$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:220:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh parse"
tests/scip-to-graph.bats:225:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh force"
tests/scip-to-graph.bats:231:@test "SC-SP-009: scip-to-graph outputs parse statistics in JSON" {
tests/scip-to-graph.bats:233:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:235:    export SCIP_INDEX_PATH="$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:239:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh stats"
tests/scip-to-graph.bats:246:    assert_json_field "$output" ".source" "scip"
tests/scip-to-graph.bats:250:@test "SC-SP-010: scip-to-graph uses custom SCIP_INDEX_PATH" {
tests/scip-to-graph.bats:252:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:255:    cp "$BATS_TEST_DIRNAME/../index.scip" "$TEST_TEMP_DIR/custom.scip"
tests/scip-to-graph.bats:256:    export SCIP_INDEX_PATH="$TEST_TEMP_DIR/custom.scip"
tests/scip-to-graph.bats:260:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh custom path"
tests/scip-to-graph.bats:273:@test "AC-N04: scip-to-graph parses all TypeScript files" {
tests/scip-to-graph.bats:275:    skip_if_no_file "$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:277:    export SCIP_INDEX_PATH="$BATS_TEST_DIRNAME/../index.scip"
tests/scip-to-graph.bats:281:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh coverage"
tests/data-flow-tracing.bats:4:# Purpose: Verify call-chain.sh --data-flow functionality (M3: AC-004)
tests/data-flow-tracing.bats:28:CALL_CHAIN="${PROJECT_ROOT}/scripts/call-chain.sh"
tests/data-flow-tracing.bats:35:@test "DF-BASE-001: call-chain.sh exists and is executable" {
tests/data-flow-tracing.bats:183:@test "DF-CROSS-003: file boundary recorded in path" {
tests/data-flow-tracing.bats:330:    [[ "$output" == *"flowchart"* ]] || [[ "$output" == *"graph"* ]] || skip "Not valid Mermaid output"
tests/cod-visualizer.bats:2:# cod-visualizer.bats - COD Êû∂ÊûÑÂèØËßÜÂåñÊ®°ÂùóÊµãËØï
tests/cod-visualizer.bats:21:COD_VISUALIZER="$SCRIPT_DIR/cod-visualizer.sh"
tests/cod-visualizer.bats:22:GRAPH_STORE="$SCRIPT_DIR/graph-store.sh"
tests/cod-visualizer.bats:26:    export GRAPH_DB_PATH="$TEST_TEMP_DIR/graph.db"
tests/cod-visualizer.bats:33:    _setup_test_graph_data
tests/cod-visualizer.bats:45:_setup_test_graph_data() {
tests/cod-visualizer.bats:46:    # Â¶ÇÊûú graph-store.sh ÂèØÁî®Ôºå‰ΩøÁî®ÂÆÉÂàùÂßãÂåñÊï∞ÊçÆÂ∫ì
tests/cod-visualizer.bats:57:            "$GRAPH_STORE" add-node --id "file:scripts/graph-store.sh" --symbol "graph-store.sh" --kind "file" --file "scripts/graph-store.sh" 2>/dev/null || true
tests/cod-visualizer.bats:58:            "$GRAPH_STORE" add-node --id "file:scripts/hotspot-analyzer.sh" --symbol "hotspot-analyzer.sh" --kind "file" --file "scripts/hotspot-analyzer.sh" 2>/dev/null || true
tests/cod-visualizer.bats:66:            "$GRAPH_STORE" add-edge --source "file:scripts/graph-store.sh" --target "file:scripts/common.sh" --type CALLS 2>/dev/null || true
tests/cod-visualizer.bats:67:            "$GRAPH_STORE" add-edge --source "file:scripts/hotspot-analyzer.sh" --target "file:scripts/common.sh" --type CALLS 2>/dev/null || true
tests/cod-visualizer.bats:68:            "$GRAPH_STORE" add-edge --source "file:scripts/hotspot-analyzer.sh" --target "file:scripts/graph-store.sh" --type CALLS 2>/dev/null || true
tests/cod-visualizer.bats:72:    # Â¶ÇÊûú graph-store.sh ‰∏çÂèØÁî®ÔºåÁõ¥Êé•Áî® sqlite3 ÂàõÂª∫
tests/cod-visualizer.bats:105:    ('file:scripts/graph-store.sh', 'graph-store.sh', 'file', 'scripts/graph-store.sh'),
tests/cod-visualizer.bats:106:    ('file:scripts/hotspot-analyzer.sh', 'hotspot-analyzer.sh', 'file', 'scripts/hotspot-analyzer.sh'),
tests/cod-visualizer.bats:116:    ('file:scripts/graph-store.sh', 'file:scripts/common.sh', 'CALLS'),
tests/cod-visualizer.bats:117:    ('file:scripts/hotspot-analyzer.sh', 'file:scripts/common.sh', 'CALLS'),
tests/cod-visualizer.bats:118:    ('file:scripts/hotspot-analyzer.sh', 'file:scripts/graph-store.sh', 'CALLS');
tests/cod-visualizer.bats:128:@test "T-CV-001: cod-visualizer generates module-level Mermaid output" {
tests/cod-visualizer.bats:132:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh generate mermaid"
tests/cod-visualizer.bats:137:    assert_contains "$output" "graph TD"
tests/cod-visualizer.bats:151:@test "T-CV-002: cod-visualizer generates file-level D3.js JSON for module" {
tests/cod-visualizer.bats:155:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh module d3json"
tests/cod-visualizer.bats:181:@test "T-CV-003a: cod-visualizer includes hotspot styling in Mermaid output" {
tests/cod-visualizer.bats:184:    run "$COD_VISUALIZER" generate --level 2 --format mermaid --include-hotspots
tests/cod-visualizer.bats:185:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh mermaid with hotspots"
tests/cod-visualizer.bats:195:@test "T-CV-003b: cod-visualizer includes hotspot field in D3.js JSON" {
tests/cod-visualizer.bats:198:    run "$COD_VISUALIZER" module scripts/ --format d3json --include-hotspots
tests/cod-visualizer.bats:199:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh d3json with hotspots"
tests/cod-visualizer.bats:204:    # È™åËØÅËäÇÁÇπÂåÖÂê´ hotspot Â≠óÊÆµ
tests/cod-visualizer.bats:205:    # Ëá≥Â∞ëÁ¨¨‰∏Ä‰∏™ËäÇÁÇπÂ∫îËØ•Êúâ hotspot Â≠óÊÆµ
tests/cod-visualizer.bats:206:    local has_hotspot
tests/cod-visualizer.bats:207:    has_hotspot=$(echo "$output" | jq '[.nodes[].hotspot] | map(select(. != null)) | length')
tests/cod-visualizer.bats:208:    [ "$has_hotspot" -ge 1 ]
tests/cod-visualizer.bats:216:@test "T-CV-004a: cod-visualizer includes complexity in Mermaid node labels" {
tests/cod-visualizer.bats:219:    run "$COD_VISUALIZER" generate --level 2 --format mermaid --include-complexity
tests/cod-visualizer.bats:220:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh mermaid with complexity"
tests/cod-visualizer.bats:226:    assert_contains_any "$output" "[" "(" "complexity" "CC:"
tests/cod-visualizer.bats:230:@test "T-CV-004b: cod-visualizer includes complexity field in D3.js JSON" {
tests/cod-visualizer.bats:233:    run "$COD_VISUALIZER" module scripts/ --format d3json --include-complexity
tests/cod-visualizer.bats:234:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh d3json with complexity"
tests/cod-visualizer.bats:239:    # È™åËØÅËäÇÁÇπÂåÖÂê´ complexity Â≠óÊÆµ
tests/cod-visualizer.bats:240:    local has_complexity
tests/cod-visualizer.bats:241:    has_complexity=$(echo "$output" | jq '[.nodes[].complexity] | map(select(. != null)) | length')
tests/cod-visualizer.bats:242:    [ "$has_complexity" -ge 1 ]
tests/cod-visualizer.bats:250:@test "T-CV-005: cod-visualizer Mermaid output is syntactically valid" {
tests/cod-visualizer.bats:254:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh mermaid syntax"
tests/cod-visualizer.bats:259:    # ÂøÖÈ°ª‰ª• graph TD Êàñ graph LR Á≠âÂºÄÂ§¥
tests/cod-visualizer.bats:260:    assert_contains_any "$output" "graph TD" "graph LR" "graph TB" "graph RL" "flowchart TD" "flowchart LR"
tests/cod-visualizer.bats:277:@test "T-CV-006: cod-visualizer D3.js JSON conforms to expected schema" {
tests/cod-visualizer.bats:281:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh d3json schema"
tests/cod-visualizer.bats:318:@test "T-CV-007: cod-visualizer handles empty module gracefully" {
tests/cod-visualizer.bats:325:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh empty module"
tests/cod-visualizer.bats:342:@test "T-CV-007b: cod-visualizer handles empty module in Mermaid format" {
tests/cod-visualizer.bats:346:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh empty module mermaid"
tests/cod-visualizer.bats:351:    assert_contains_any "$output" "graph TD" "graph LR" "flowchart" "%% Empty"
tests/cod-visualizer.bats:359:@test "T-CV-008a: cod-visualizer writes Mermaid output to file" {
tests/cod-visualizer.bats:365:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh output to file"
tests/cod-visualizer.bats:375:    assert_contains_any "$file_content" "graph TD" "graph LR" "flowchart"
tests/cod-visualizer.bats:379:@test "T-CV-008b: cod-visualizer writes D3.js JSON output to file" {
tests/cod-visualizer.bats:385:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh d3json output to file"
tests/cod-visualizer.bats:408:@test "BC-CV-001: cod-visualizer rejects invalid format" {
tests/cod-visualizer.bats:416:        assert_contains_any "$output" "Invalid format" "Unknown format" "graph TD" "nodes"
tests/cod-visualizer.bats:425:@test "BC-CV-002: cod-visualizer handles invalid level parameter" {
tests/cod-visualizer.bats:429:    skip_if_not_ready "$status" "$output" "cod-visualizer.sh invalid level"
tests/cod-visualizer.bats:434:        assert_contains_any "$output" "graph TD" "graph LR" "flowchart"
tests/cod-visualizer.bats:442:@test "BC-CV-003: cod-visualizer handles non-existent module path" {
tests/cod-visualizer.bats:462:@test "BC-CV-004: cod-visualizer handles missing database gracefully" {
tests/cod-visualizer.bats:473:        assert_contains_any "$output" "graph TD" "Empty" "No data"
tests/long-term-memory.bats:11:    export INTENT_LEARNER_SCRIPT="${SCRIPTS_DIR}/intent-learner.sh"
tests/long-term-memory.bats:156:@test "T-LTM-008: Symbols are indexed for fast retrieval" {
tests/long-term-memory.bats:163:    store_turn "conv-001" "user" "The processOrder function in OrderService class has a bug"
tests/long-term-memory.bats:358:        store_turn "conv-perf" "user" "Test message $i about some code symbol"
tests/subgraph-retrieval.bats:2:# subgraph-retrieval.bats - AC-003 Subgraph Retrieval Acceptance Tests
tests/subgraph-retrieval.bats:4:# Purpose: Verify graph-rag.sh subgraph retrieval functionality
tests/subgraph-retrieval.bats:6:# Run: bats tests/subgraph-retrieval.bats
tests/subgraph-retrieval.bats:9:# Change: enhance-code-intelligence
tests/subgraph-retrieval.bats:17:GRAPH_RAG="${PROJECT_ROOT}/scripts/graph-rag.sh"
tests/subgraph-retrieval.bats:23:@test "SR-BASE-001: graph-rag.sh exists and is executable" {
tests/subgraph-retrieval.bats:28:# Subgraph Retrieval Tests (SR-001 ~ SR-003)
tests/subgraph-retrieval.bats:31:@test "SR-001: subgraph includes call edges (--calls-->)" {
tests/subgraph-retrieval.bats:32:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:33:    skip_if_not_ready "$status" "$output" "Subgraph retrieval"
tests/subgraph-retrieval.bats:37:@test "SR-002: subgraph includes reference edges (--refs-->)" {
tests/subgraph-retrieval.bats:38:    run "$GRAPH_RAG" --symbol "TOOLS" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:39:    skip_if_not_ready "$status" "$output" "Subgraph retrieval"
tests/subgraph-retrieval.bats:44:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --depth 3 --format json 2>&1
tests/subgraph-retrieval.bats:45:    skip_if_not_ready "$status" "$output" "Subgraph retrieval"
tests/subgraph-retrieval.bats:50:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --depth 10 --format json 2>&1
tests/subgraph-retrieval.bats:60:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:72:@test "SR-OUTPUT-001: subgraph output includes nodes array" {
tests/subgraph-retrieval.bats:73:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:74:    [ "$status" -eq 0 ] || skip "Subgraph retrieval not yet implemented"
tests/subgraph-retrieval.bats:78:@test "SR-OUTPUT-002: subgraph output includes edges array" {
tests/subgraph-retrieval.bats:79:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:80:    [ "$status" -eq 0 ] || skip "Subgraph retrieval not yet implemented"
tests/subgraph-retrieval.bats:88:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:89:    [ "$status" -eq 0 ] || skip "Subgraph retrieval not yet implemented"
tests/subgraph-retrieval.bats:97:@test "SR-PARAM-001: --subgraph parameter support" {
tests/subgraph-retrieval.bats:99:    [[ "$output" == *"subgraph"* ]] || \
tests/subgraph-retrieval.bats:100:    [[ "$output" == *"graph"* ]] || \
tests/subgraph-retrieval.bats:102:    skip "Subgraph parameter not documented"
tests/subgraph-retrieval.bats:115:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:116:    [ "$status" -eq 0 ] || skip "Subgraph retrieval not yet implemented"
tests/subgraph-retrieval.bats:121:    run "$GRAPH_RAG" --symbol "TOOLS" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:122:    [ "$status" -eq 0 ] || skip "Subgraph retrieval not yet implemented"
tests/subgraph-retrieval.bats:130:@test "SR-COMPAT-001: without --subgraph returns linear list" {
tests/subgraph-retrieval.bats:140:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --depth 0 --format json 2>&1
tests/subgraph-retrieval.bats:152:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --depth -1 --format json 2>&1
tests/subgraph-retrieval.bats:161:    run "$GRAPH_RAG" --symbol "handleToolCall" --subgraph --depth 100 --format json 2>&1
tests/subgraph-retrieval.bats:170:    run "$GRAPH_RAG" --symbol "" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:178:    run "$GRAPH_RAG" --symbol "nonExistentSymbol12345" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:191:    run "$GRAPH_RAG" --symbol "some.symbol.with.dots" --subgraph --format json 2>&1
tests/subgraph-retrieval.bats:201:    run "$GRAPH_RAG" --symbol "$long_symbol" --subgraph --format json 2>&1
tests/pattern-learner.bats:270:    # Ê£ÄÊü•ÊòØÂê¶Ë∂ÖÊó∂Ôºàexit code 124 from timeoutÔºâ
tests/pattern-learner.bats:480:@test "SC-PD-010: auto-discover handles empty codebase gracefully" {
tests/pattern-learner.bats:542:      "last_confirmed": "2026-01-07T00:00:00Z",
tests/pattern-learner.bats:574:# ÂÜçÊ¨°ÂåπÈÖçÊ®°ÂºèÊó∂ÔºåÈáçÁΩÆ last_confirmed ÂíåÁΩÆ‰ø°Â∫¶
tests/pattern-learner.bats:591:      "last_confirmed": "2026-01-01T00:00:00Z"
tests/pattern-learner.bats:645:      "last_confirmed": "2026-01-15T00:00:00Z"
tests/pattern-learner.bats:651:      "last_confirmed": "2025-10-01T00:00:00Z"
tests/pattern-learner.bats:657:      "last_confirmed": "2026-01-10T00:00:00Z"
tests/pattern-learner.bats:692:        echo "Pattern with confidence exactly 0.3 should have been kept (boundary)" >&2
tests/pattern-learner.bats:712:      "last_confirmed": "2026-01-16T00:00:00Z"
tests/pattern-learner.bats:715:  "last_decay_run": "2026-01-16T00:00:00Z"
tests/pattern-learner.bats:752:        patterns_json+="{\"pattern_id\":\"perf-test-$i\",\"name\":\"Pattern $i\",\"confidence\":$confidence,\"last_confirmed\":\"2026-01-$(printf '%02d' $day)T00:00:00Z\"}"
tests/intent-learner.bats:2:# intent-learner.bats - ÊÑèÂõæÂÅèÂ•ΩÂ≠¶‰π†Ê®°ÂùóÊµãËØï
tests/intent-learner.bats:19:#   - recency_weight = 1 / (1 + days_since_last_query)
tests/intent-learner.bats:26:INTENT_LEARNER="$SCRIPT_DIR/intent-learner.sh"
tests/intent-learner.bats:29:HISTORY_FILE_NAME="intent-history.json"
tests/intent-learner.bats:49:@test "T-IL-001: intent-learner record creates history entry" {
tests/intent-learner.bats:53:    skip_if_not_ready "$status" "$output" "intent-learner.sh record"
tests/intent-learner.bats:82:@test "T-IL-001b: intent-learner record appends multiple entries" {
tests/intent-learner.bats:86:    skip_if_not_ready "$status" "$output" "intent-learner.sh record"
tests/intent-learner.bats:133:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:157:@test "T-IL-003: intent-learner cleanup removes entries older than 90 days" {
tests/intent-learner.bats:178:    skip_if_not_ready "$status" "$output" "intent-learner.sh cleanup"
tests/intent-learner.bats:206:    {"symbol": "boundary", "symbol_id": "src/boundary.ts::boundary", "action": "view", "timestamp": $ninety_days_ago},
tests/intent-learner.bats:213:    skip_if_not_ready "$status" "$output" "intent-learner.sh cleanup"
tests/intent-learner.bats:224:    [ "$symbol" = "boundary" ]
tests/intent-learner.bats:232:@test "T-IL-004: intent-learner get-preferences returns top N symbols" {
tests/intent-learner.bats:254:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:287:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:301:@test "T-IL-005: intent-learner get-preferences filters by prefix" {
tests/intent-learner.bats:320:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences --prefix"
tests/intent-learner.bats:352:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences --prefix"
tests/intent-learner.bats:367:@test "T-IL-006: intent-learner enforces max 10000 entries limit" {
tests/intent-learner.bats:391:    skip_if_not_ready "$status" "$output" "intent-learner.sh record"
tests/intent-learner.bats:430:    skip_if_not_ready "$status" "$output" "intent-learner.sh record"
tests/intent-learner.bats:450:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:470:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:504:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:550:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:574:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences (corrupt recovery)"
tests/intent-learner.bats:593:    skip_if_not_ready "$status" "$output" "intent-learner.sh record (corrupt recovery)"
tests/intent-learner.bats:616:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences (empty file)"
tests/intent-learner.bats:635:    skip_if_not_ready "$status" "$output" "intent-learner.sh record (special chars)"
tests/intent-learner.bats:696:        skip_if_not_ready "$status" "$output" "intent-learner.sh context save"
tests/intent-learner.bats:700:    skip_if_not_ready "$status" "$output" "intent-learner.sh context load"
tests/intent-learner.bats:717:    skip_if_not_ready "$status" "$output" "intent-learner.sh context save"
tests/intent-learner.bats:720:    skip_if_not_ready "$status" "$output" "intent-learner.sh context load"
tests/intent-learner.bats:759:        skip_if_not_ready "$status" "$output" "intent-learner.sh context save"
tests/intent-learner.bats:763:    skip_if_not_ready "$status" "$output" "intent-learner.sh context load"
tests/intent-learner.bats:788:    # Verify last entry is the newest (turn 11)
tests/intent-learner.bats:789:    local last_turn last_query
tests/intent-learner.bats:790:    last_turn=$(echo "$output" | jq -r '.context_window[-1].turn // empty')
tests/intent-learner.bats:791:    last_query=$(echo "$output" | jq -r '.context_window[-1].query // empty')
tests/intent-learner.bats:793:    if [ "$last_turn" != "11" ]; then
tests/intent-learner.bats:797:    if [ "$last_query" != "query-11" ]; then
tests/intent-learner.bats:829:    skip_if_not_ready "$status" "$output" "intent-learner.sh context apply-weight"
tests/intent-learner.bats:886:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:937:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:975:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:1018:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences"
tests/intent-learner.bats:1063:    skip_if_not_ready "$status" "$output" "intent-learner.sh get-preferences --prefix"
tests/intent-learner.bats:1111:    skip_if_not_ready "$status" "$output" "intent-learner.sh context apply-weight"
tests/intent-learner.bats:1153:    skip_if_not_ready "$status" "$output" "intent-learner.sh context apply-weight"
tests/intent-learner.bats:1194:    skip_if_not_ready "$status" "$output" "intent-learner.sh context apply-weight"
tests/intent-learner.bats:1237:    skip_if_not_ready "$status" "$output" "intent-learner.sh context apply-weight"
tests/intent-learner.bats:1284:    skip_if_not_ready "$status" "$output" "intent-learner.sh context apply-weight"
tests/intent-learner.bats:1327:    skip_if_not_ready "$status" "$output" "intent-learner.sh context apply-weight"
tests/federation-lite.bats:2:# federation-lite.bats - Federation Lite Contract Tests
tests/federation-lite.bats:4:# Purpose: Verify cross-repository contract discovery and indexing
tests/federation-lite.bats:6:# Run: bats tests/federation-lite.bats
tests/federation-lite.bats:17:FEDERATION_LITE="${PROJECT_ROOT}/scripts/federation-lite.sh"
tests/federation-lite.bats:26:    export FEDERATION_CONFIG="$TEST_TEMP_DIR/federation.yaml"
tests/federation-lite.bats:27:    export FEDERATION_INDEX="$TEST_TEMP_DIR/federation-index.json"
tests/federation-lite.bats:51:    # API contracts repo
tests/federation-lite.bats:52:    mkdir -p "$base/api-contracts"
tests/federation-lite.bats:53:    cd "$base/api-contracts"
tests/federation-lite.bats:118:    git commit -m "init contracts" --quiet
tests/federation-lite.bats:148:# Helper: Create federation config
tests/federation-lite.bats:149:create_federation_config() {
tests/federation-lite.bats:156:federation:
tests/federation-lite.bats:158:    - name: "api-contracts"
tests/federation-lite.bats:159:      path: "$base_dir/api-contracts"
tests/federation-lite.bats:160:      contracts:
tests/federation-lite.bats:166:      contracts:
tests/federation-lite.bats:177:# Helper: Setup federation test environment (reduces test boilerplate)
tests/federation-lite.bats:178:# Usage: setup_federation_test
tests/federation-lite.bats:179:# Creates: test repos, federation config, changes to main-repo directory
tests/federation-lite.bats:181:setup_federation_test() {
tests/federation-lite.bats:183:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:191:@test "CT-FED-BASE-001: federation-lite.sh exists and is executable" {
tests/federation-lite.bats:192:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:196:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:199:    [[ "$output" == *"federation"* ]] || [[ "$output" == *"index"* ]] || [[ "$output" == *"contract"* ]]
tests/federation-lite.bats:208:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:210:    setup_federation_test
tests/federation-lite.bats:219:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:229:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:232:    setup_federation_test
tests/federation-lite.bats:239:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:244:    [[ "$content" == *"api-contracts"* ]] || skip "api-contracts not indexed"
tests/federation-lite.bats:253:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:261:federation:
tests/federation-lite.bats:268:    contract_patterns:
tests/federation-lite.bats:284:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:286:    # Should have discovered the api-contracts and shared-types repos
tests/federation-lite.bats:290:        [ "$repo_count" -ge 2 ] || skip "Auto-discover should find at least 2 repos"
tests/federation-lite.bats:299:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:302:    setup_federation_test
tests/federation-lite.bats:309:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:319:@test "CT-FED-003b: proto contract has type=proto" {
tests/federation-lite.bats:320:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:323:    setup_federation_test
tests/federation-lite.bats:330:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:334:    # Find proto contract and check type
tests/federation-lite.bats:335:    local proto_type=$(echo "$content" | jq -r '.repositories[].contracts[] | select(.path | endswith(".proto")) | .type' 2>/dev/null | head -1)
tests/federation-lite.bats:336:    [ "$proto_type" = "proto" ] || skip "Proto contract should have type=proto, got '$proto_type'"
tests/federation-lite.bats:344:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:347:    setup_federation_test
tests/federation-lite.bats:354:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:363:@test "CT-FED-004b: openapi contract has type=openapi" {
tests/federation-lite.bats:364:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:367:    setup_federation_test
tests/federation-lite.bats:374:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:378:    local openapi_type=$(echo "$content" | jq -r '.repositories[].contracts[] | select(.path | endswith(".yaml")) | .type' 2>/dev/null | head -1)
tests/federation-lite.bats:379:    [ "$openapi_type" = "openapi" ] || skip "OpenAPI contract should have type=openapi, got '$openapi_type'"
tests/federation-lite.bats:387:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:389:    setup_federation_test
tests/federation-lite.bats:404:    [[ "$output" == *"api-contracts"* ]] || [[ "$output" == *"proto"* ]] || \
tests/federation-lite.bats:409:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:412:    setup_federation_test
tests/federation-lite.bats:432:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:434:    setup_federation_test
tests/federation-lite.bats:461:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:469:federation:
tests/federation-lite.bats:471:    - name: "api-contracts"
tests/federation-lite.bats:472:      path: "$TEST_TEMP_DIR/api-contracts"
tests/federation-lite.bats:473:      contracts:
tests/federation-lite.bats:478:      contracts:
tests/federation-lite.bats:495:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:499:        # Should have 1 repo (api-contracts), not 2
tests/federation-lite.bats:506:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:513:federation:
tests/federation-lite.bats:517:      contracts:
tests/federation-lite.bats:540:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:543:    setup_federation_test
tests/federation-lite.bats:550:    local original_hash=$(cat "$FEDERATION_INDEX" | jq -r '.repositories[0].contracts[0].hash // "none"' 2>/dev/null)
tests/federation-lite.bats:552:    # Modify one contract
tests/federation-lite.bats:553:    echo "// modified" >> "$TEST_TEMP_DIR/api-contracts/user.proto"
tests/federation-lite.bats:563:    local new_hash=$(cat "$FEDERATION_INDEX" | jq -r '.repositories[0].contracts[0].hash // "none"' 2>/dev/null)
tests/federation-lite.bats:574:@test "CT-FED-009: validates federation.yaml schema" {
tests/federation-lite.bats:575:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:594:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:597:    setup_federation_test
tests/federation-lite.bats:604:    [ -f "$FEDERATION_INDEX" ] || skip "federation-index.json not created"
tests/federation-lite.bats:617:@test "CT-FED-011: ci_federation tool registered in server.ts" {
tests/federation-lite.bats:621:    run grep -l "ci_federation" "$server_ts"
tests/federation-lite.bats:622:    [ "$status" -eq 0 ] || skip "ci_federation not yet registered"
tests/federation-lite.bats:625:@test "CT-FED-011b: ci_federation has correct input schema" {
tests/federation-lite.bats:629:    run grep -A 30 "ci_federation" "$server_ts"
tests/federation-lite.bats:630:    [[ "$output" == *"ci_federation"* ]] || skip "ci_federation not yet registered"
tests/federation-lite.bats:643:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:666:federation:
tests/federation-lite.bats:670:      contracts:
tests/federation-lite.bats:685:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:714:federation:
tests/federation-lite.bats:718:      contracts:
tests/federation-lite.bats:733:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:735:    # Create empty repo (no contracts)
tests/federation-lite.bats:750:federation:
tests/federation-lite.bats:754:      contracts:
tests/federation-lite.bats:763:    # Should succeed (no contracts is valid) or warn
tests/federation-lite.bats:764:    [ "$status" -eq 0 ] || [[ "$output" == *"no contracts"* ]] || \
tests/federation-lite.bats:774:# Helper: Setup virtual edge test environment with graph.db
tests/federation-lite.bats:778:    # Create graph.db with virtual_edges table
tests/federation-lite.bats:779:    export GRAPH_DB="$base/graph.db"
tests/federation-lite.bats:791:    contract_type TEXT,
tests/federation-lite.bats:792:    contract_bonus REAL DEFAULT 0.0,
tests/federation-lite.bats:828:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:840:# When: Call federation-lite.sh generate-virtual-edges
tests/federation-lite.bats:841:# Then: Virtual edge generated, written to graph.db virtual_edges table
tests/federation-lite.bats:844:@test "T-FV-001: test_proto_virtual_edge - Proto contract virtual edge generation" {
tests/federation-lite.bats:851:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:855:    # First update the federation index
tests/federation-lite.bats:869:    # Verify virtual edge was created in graph.db
tests/federation-lite.bats:870:    local edge_count=$(sqlite3 "$GRAPH_DB" "SELECT COUNT(*) FROM virtual_edges WHERE contract_type='proto'")
tests/federation-lite.bats:871:    [ "$edge_count" -ge 1 ] || fail "Expected at least 1 proto virtual edge, got $edge_count"
tests/federation-lite.bats:886:# Formula: exact_match*0.6 + signature_similarity*0.3 + contract_bonus*0.1
tests/federation-lite.bats:895:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:912:    # Expected: exact_match=0.7 (prefix match), signature_similarity=0.6, contract_bonus=0.1 (proto)
tests/federation-lite.bats:964:    export GRAPH_DB="$TEST_TEMP_DIR/graph.db"
tests/federation-lite.bats:968:        contract_type TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
tests/federation-lite.bats:972:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:995:# Given: graph.db contains virtual edges
tests/federation-lite.bats:996:# When: Call federation-lite.sh query-virtual <symbol>
tests/federation-lite.bats:1007:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1073:    export GRAPH_DB="$TEST_TEMP_DIR/graph.db"
tests/federation-lite.bats:1077:        confidence_level TEXT, contract_type TEXT,
tests/federation-lite.bats:1082:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1107:# Then: contract_bonus = 0.05, VIRTUAL_CALLS edge generated
tests/federation-lite.bats:1110:@test "T-FV-006: test_openapi_virtual_edge - OpenAPI contract virtual edge" {
tests/federation-lite.bats:1116:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1134:        "SELECT contract_type, edge_type FROM virtual_edges
tests/federation-lite.bats:1136:         AND contract_type = 'openapi'
tests/federation-lite.bats:1141:    # Verify contract_bonus is 0.05 for OpenAPI
tests/federation-lite.bats:1143:        "SELECT contract_bonus FROM virtual_edges
tests/federation-lite.bats:1144:         WHERE contract_type = 'openapi'
tests/federation-lite.bats:1150:            [ "$is_correct" -eq 1 ] || skip "OpenAPI contract_bonus should be 0.05, got $bonus"
tests/federation-lite.bats:1157:         WHERE contract_type = 'openapi'
tests/federation-lite.bats:1175:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1197:    cd "$TEST_TEMP_DIR/api-contracts"
tests/federation-lite.bats:1232:# Then: Virtual edge should be removed from graph.db
tests/federation-lite.bats:1241:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1262:    cd "$TEST_TEMP_DIR/api-contracts"
tests/federation-lite.bats:1289:    # Re-update federation index
tests/federation-lite.bats:1344:    export GRAPH_DB="$TEST_TEMP_DIR/graph.db"
tests/federation-lite.bats:1348:        exact_match REAL, signature_similarity REAL, contract_bonus REAL,
tests/federation-lite.bats:1349:        contract_type TEXT,
tests/federation-lite.bats:1354:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1389:# Formula: confidence = exact√ó0.6 + signature√ó0.3 + contract√ó0.1
tests/federation-lite.bats:1396:    # Create graph.db with confidence test schema
tests/federation-lite.bats:1411:    contract_bonus REAL DEFAULT 0.0,
tests/federation-lite.bats:1412:    contract_type TEXT,
tests/federation-lite.bats:1449:    [ -x "$FEDERATION_LITE" ] || skip "federation-lite.sh not yet implemented"
tests/federation-lite.bats:1460:# Given: exact=0.8, signature=0.7, contract=0.1
tests/federation-lite.bats:1465:@test "CT-VE-001: confidence formula - exact√ó0.6 + signature√ó0.3 + contract√ó0.1" {
tests/federation-lite.bats:1472:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1498:        "SELECT confidence, exact_match, signature_similarity, contract_bonus
tests/federation-lite.bats:1509:    local contract=$(echo "$edge_data" | cut -d'|' -f4)
tests/federation-lite.bats:1511:    # Verify formula: confidence = exact√ó0.6 + signature√ó0.3 + contract√ó0.1
tests/federation-lite.bats:1512:    if command -v bc &> /dev/null && [ -n "$exact" ] && [ -n "$signature" ] && [ -n "$contract" ]; then
tests/federation-lite.bats:1513:        local expected=$(echo "scale=4; $exact * 0.6 + $signature * 0.3 + $contract * 0.1" | bc -l)
tests/federation-lite.bats:1519:            fail "Confidence formula mismatch: got $confidence, expected $expected (exact=$exact, sig=$signature, contract=$contract)"
tests/federation-lite.bats:1539:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1600:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1666:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1697:    # Verify at least one VIRTUAL_CALLS edge exists
tests/federation-lite.bats:1702:        fail "Expected at least 1 VIRTUAL_CALLS edge, got $virtual_calls_count"
tests/federation-lite.bats:1718:    create_federation_config "$FEDERATION_CONFIG" "$TEST_TEMP_DIR"
tests/federation-lite.bats:1774:        # Log for debugging
tests/hotspot-analyzer.bats:2:# hotspot-analyzer.bats - AC-001 Hotspot Algorithm Acceptance Tests
tests/hotspot-analyzer.bats:4:# Purpose: Verify hotspot-analyzer.sh core functionality
tests/hotspot-analyzer.bats:6:# Run: bats tests/hotspot-analyzer.bats
tests/hotspot-analyzer.bats:9:# Change: enhance-code-intelligence
tests/hotspot-analyzer.bats:17:HOTSPOT_ANALYZER="${PROJECT_ROOT}/scripts/hotspot-analyzer.sh"
tests/hotspot-analyzer.bats:24:@test "HS-001: hotspot-analyzer.sh exists and is executable" {
tests/hotspot-analyzer.bats:31:    [[ "$output" == *"Hotspot"* ]] || [[ "$output" == *"hotspot"* ]]
tests/hotspot-analyzer.bats:45:@test "HS-003: default returns Top-20 hotspot files in JSON" {
tests/hotspot-analyzer.bats:48:    [[ "$output" == *"hotspots"* ]]
tests/hotspot-analyzer.bats:58:        count=$(echo "$output" | jq '.hotspots | length')
tests/hotspot-analyzer.bats:63:@test "HS-005: hotspot score formula (Frequency x Complexity)" {
tests/hotspot-analyzer.bats:67:    [[ "$output" == *"complexity"* ]]
tests/hotspot-analyzer.bats:82:    local exit_code=$?
tests/hotspot-analyzer.bats:84:    [ "$exit_code" -eq 0 ]
tests/hotspot-analyzer.bats:135:    # Should either return empty hotspots array or error
tests/hotspot-analyzer.bats:139:            count=$(echo "$output" | jq '.hotspots | length' 2>/dev/null || echo "0")
tests/hotspot-analyzer.bats:188:    # Should return empty hotspots or appropriate message
tests/hotspot-analyzer.bats:215:# Spec: dev-playbooks/changes/algorithm-optimization-parity/specs/hotspot-weighting/spec.md
tests/hotspot-analyzer.bats:219:# Weighted score formula: score = churn*0.4 + complexity*0.3 + coupling*0.2 + age*0.1
tests/hotspot-analyzer.bats:223:@test "CT-HW-001: weighted score formula - score = churn*0.4 + complexity*0.3 + coupling*0.2 + age*0.1" {
tests/hotspot-analyzer.bats:239:    if [[ "$json" == *"churn"* ]] && [[ "$json" == *"complexity"* ]]; then
tests/hotspot-analyzer.bats:241:        local first_hotspot
tests/hotspot-analyzer.bats:242:        first_hotspot=$(echo "$json" | jq '.hotspots[0]' 2>/dev/null)
tests/hotspot-analyzer.bats:244:        if [ "$first_hotspot" != "null" ] && [ -n "$first_hotspot" ]; then
tests/hotspot-analyzer.bats:245:            local churn complexity coupling age score
tests/hotspot-analyzer.bats:246:            churn=$(echo "$first_hotspot" | jq -r '.churn // .churn_norm // 0' 2>/dev/null)
tests/hotspot-analyzer.bats:247:            complexity=$(echo "$first_hotspot" | jq -r '.complexity // .complexity_norm // 0' 2>/dev/null)
tests/hotspot-analyzer.bats:248:            coupling=$(echo "$first_hotspot" | jq -r '.coupling // .coupling_norm // 0' 2>/dev/null)
tests/hotspot-analyzer.bats:249:            age=$(echo "$first_hotspot" | jq -r '.age // .age_norm // 0' 2>/dev/null)
tests/hotspot-analyzer.bats:250:            score=$(echo "$first_hotspot" | jq -r '.score // 0' 2>/dev/null)
tests/hotspot-analyzer.bats:270:    # Get hotspots list
tests/hotspot-analyzer.bats:271:    local hotspots_count
tests/hotspot-analyzer.bats:272:    hotspots_count=$(echo "$json" | jq '.hotspots | length' 2>/dev/null)
tests/hotspot-analyzer.bats:274:    if [ "$hotspots_count" -gt 0 ] 2>/dev/null; then
tests/hotspot-analyzer.bats:275:        # Check normalized factors for each hotspot
tests/hotspot-analyzer.bats:277:        while [ "$i" -lt "$hotspots_count" ] && [ "$i" -lt 5 ]; do
tests/hotspot-analyzer.bats:278:            local hotspot
tests/hotspot-analyzer.bats:279:            hotspot=$(echo "$json" | jq ".hotspots[$i]" 2>/dev/null)
tests/hotspot-analyzer.bats:283:            churn_norm=$(echo "$hotspot" | jq -r '.churn_norm // .churn_normalized // .factors.churn // "skip"' 2>/dev/null)
tests/hotspot-analyzer.bats:289:            # Check complexity_norm
tests/hotspot-analyzer.bats:290:            local complexity_norm
tests/hotspot-analyzer.bats:291:            complexity_norm=$(echo "$hotspot" | jq -r '.complexity_norm // .complexity_normalized // .factors.complexity // "skip"' 2>/dev/null)
tests/hotspot-analyzer.bats:292:            if [ "$complexity_norm" != "skip" ] && [ "$complexity_norm" != "null" ]; then
tests/hotspot-analyzer.bats:293:                float_gte "$complexity_norm" "0" || fail "complexity_norm should be >= 0"
tests/hotspot-analyzer.bats:294:                float_gte "1" "$complexity_norm" || skip "complexity_norm ($complexity_norm) > 1, may need investigation"
tests/hotspot-analyzer.bats:300:        skip "No hotspots returned for normalization test"
tests/hotspot-analyzer.bats:325:    assert_contains "$json" "hotspots" "Output should contain hotspots"
tests/hotspot-analyzer.bats:340:    if [[ "$json" == *"recency"* ]] || [[ "$json" == *"age"* ]] || [[ "$json" == *"last_modified"* ]]; then
tests/hotspot-analyzer.bats:341:        # Get first hotspot
tests/hotspot-analyzer.bats:342:        local first_hotspot
tests/hotspot-analyzer.bats:343:        first_hotspot=$(echo "$json" | jq '.hotspots[0]' 2>/dev/null)
tests/hotspot-analyzer.bats:345:        if [ "$first_hotspot" != "null" ]; then
tests/hotspot-analyzer.bats:348:            recency=$(echo "$first_hotspot" | jq -r '.recency_factor // .age_factor // .recency_boost // "none"' 2>/dev/null)
tests/hotspot-analyzer.bats:361:    # Test high coupling increases hotspot score (penalizes high coupling)
tests/hotspot-analyzer.bats:372:        # Get hotspots list
tests/hotspot-analyzer.bats:373:        local hotspots_count
tests/hotspot-analyzer.bats:374:        hotspots_count=$(echo "$json" | jq '.hotspots | length' 2>/dev/null)
tests/hotspot-analyzer.bats:376:        if [ "$hotspots_count" -gt 1 ] 2>/dev/null; then
tests/hotspot-analyzer.bats:377:            # Check coupling for multiple hotspots
tests/hotspot-analyzer.bats:382:            while [ "$i" -lt "$hotspots_count" ] && [ "$i" -lt 10 ]; do
tests/hotspot-analyzer.bats:383:                local hotspot
tests/hotspot-analyzer.bats:384:                hotspot=$(echo "$json" | jq ".hotspots[$i]" 2>/dev/null)
tests/hotspot-analyzer.bats:387:                coupling=$(echo "$hotspot" | jq -r '.coupling // .coupling_score // 0' 2>/dev/null)
tests/hotspot-analyzer.bats:389:                score=$(echo "$hotspot" | jq -r '.score // 0' 2>/dev/null)
tests/hotspot-analyzer.bats:393:                    high_coupling_file=$(echo "$hotspot" | jq -r '.file // .path' 2>/dev/null)
tests/hotspot-analyzer.bats:437:    local exit_code=$?
tests/hotspot-analyzer.bats:444:    if [ "$exit_code" -ne 0 ]; then
tests/hotspot-analyzer.bats:445:        skip "Weighted hotspot analysis not yet implemented for performance test"
tests/dependency-guard.bats:2:# dependency-guard.bats - Dependency Guard Contract Tests
tests/dependency-guard.bats:4:# Purpose: Verify circular dependency detection and architecture rule validation
tests/dependency-guard.bats:6:# Run: bats tests/dependency-guard.bats
tests/dependency-guard.bats:17:DEPENDENCY_GUARD="${PROJECT_ROOT}/scripts/dependency-guard.sh"
tests/dependency-guard.bats:36:# Helper: Create test project with circular dependency
tests/dependency-guard.bats:117:@test "CT-GUARD-BASE-001: dependency-guard.sh exists and is executable" {
tests/dependency-guard.bats:118:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:122:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:125:    [[ "$output" == *"cycles"* ]] || [[ "$output" == *"rules"* ]] || [[ "$output" == *"dependency"* ]]
tests/dependency-guard.bats:130:# AC-006: Circular dependency detection
tests/dependency-guard.bats:134:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:155:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:176:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:200:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:239:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:274:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:311:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:365:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:392:    # Should check both main.ts and helper.ts (dependency)
tests/dependency-guard.bats:402:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:448:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:495:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:500:    # Linear dependency chain (no cycles)
tests/dependency-guard.bats:518:        [ "$cycle_count" -eq 0 ] || skip "False positive detected: $cycle_count cycles in non-cyclic code"
tests/dependency-guard.bats:527:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:539:    [ "$status" -eq 0 ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:546:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:558:    [ "$status" -eq 0 ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:597:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:630:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:662:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:698:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:727:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:765:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:790:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:828:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:871:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/dependency-guard.bats:901:@test "SC-OD-010: orphan-check handles empty graph gracefully" {
tests/dependency-guard.bats:902:    [ -x "$DEPENDENCY_GUARD" ] || skip "dependency-guard.sh not yet implemented"
tests/augment-context.bats:231:    local task_has_query task_has_intent
tests/augment-context.bats:233:    task_has_intent=$(echo "$output" | jq -r 'has("task_context") and (.task_context | has("intent") or has("inferred_intent") or has("action"))')
tests/impact-analyzer.bats:2:# impact-analyzer.bats - ‰º†ÈÄíÊÄßÂΩ±ÂìçÂàÜÊûêÊµãËØï
tests/impact-analyzer.bats:16:#   Impact(node, depth) = base_impact √ó (decay_factor ^ depth)
tests/impact-analyzer.bats:17:#   - base_impact = 1.0
tests/impact-analyzer.bats:25:IMPACT_ANALYZER="$SCRIPT_DIR/impact-analyzer.sh"
tests/impact-analyzer.bats:26:GRAPH_STORE="$SCRIPT_DIR/graph-store.sh"
tests/impact-analyzer.bats:30:    export GRAPH_DB_PATH="$TEST_TEMP_DIR/graph.db"
tests/impact-analyzer.bats:118:@test "T-IA-001: impact-analyzer analyze returns impact matrix via BFS traversal" {
tests/impact-analyzer.bats:126:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh analyze"
tests/impact-analyzer.bats:142:@test "T-IA-001b: impact-analyzer analyze uses default depth when not specified" {
tests/impact-analyzer.bats:150:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh analyze (default depth)"
tests/impact-analyzer.bats:160:@test "T-IA-002: impact-analyzer file returns merged deduplicated affected nodes" {
tests/impact-analyzer.bats:168:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh file"
tests/impact-analyzer.bats:181:@test "T-IA-002b: impact-analyzer file deduplicates nodes correctly" {
tests/impact-analyzer.bats:189:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh file deduplication"
tests/impact-analyzer.bats:206:@test "T-IA-003: impact-analyzer calculates confidence correctly with decay_factor=0.8" {
tests/impact-analyzer.bats:214:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh confidence calculation"
tests/impact-analyzer.bats:225:    b_confidence=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:B" or .symbol == "funcB") | .confidence // .impact // empty' 2>/dev/null | head -1)
tests/impact-analyzer.bats:234:    c_confidence=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:C" or .symbol == "funcC") | .confidence // .impact // empty' 2>/dev/null | head -1)
tests/impact-analyzer.bats:243:    d_confidence=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:D" or .symbol == "funcD") | .confidence // .impact // empty' 2>/dev/null | head -1)
tests/impact-analyzer.bats:252:@test "T-IA-003b: impact-analyzer confidence decreases with depth" {
tests/impact-analyzer.bats:260:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh confidence order"
tests/impact-analyzer.bats:266:    b_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:B" or .symbol == "funcB") | .confidence // .impact // 0' 2>/dev/null | head -1)
tests/impact-analyzer.bats:267:    c_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:C" or .symbol == "funcC") | .confidence // .impact // 0' 2>/dev/null | head -1)
tests/impact-analyzer.bats:268:    d_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:D" or .symbol == "funcD") | .confidence // .impact // 0' 2>/dev/null | head -1)
tests/impact-analyzer.bats:282:@test "T-IA-004: impact-analyzer threshold filters out low confidence nodes" {
tests/impact-analyzer.bats:292:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh threshold"
tests/impact-analyzer.bats:305:@test "T-IA-004b: impact-analyzer high threshold filters more nodes" {
tests/impact-analyzer.bats:314:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh high threshold"
tests/impact-analyzer.bats:325:@test "T-IA-004c: impact-analyzer uses default threshold 0.1" {
tests/impact-analyzer.bats:335:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh default threshold"
tests/impact-analyzer.bats:350:@test "T-IA-005: impact-analyzer mermaid output is valid syntax" {
tests/impact-analyzer.bats:358:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh mermaid format"
tests/impact-analyzer.bats:362:    # Â∫îËØ•‰ª• graph Êàñ flowchart ÂºÄÂ§¥
tests/impact-analyzer.bats:363:    assert_contains_any "$output" "graph" "flowchart"
tests/impact-analyzer.bats:373:@test "T-IA-005b: impact-analyzer mermaid output includes confidence labels" {
tests/impact-analyzer.bats:381:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh mermaid labels"
tests/impact-analyzer.bats:394:@test "T-IA-006: impact-analyzer depth limit prevents infinite loop on cycles" {
tests/impact-analyzer.bats:403:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh cycle handling"
tests/impact-analyzer.bats:412:@test "T-IA-006b: impact-analyzer stops traversal at specified depth" {
tests/impact-analyzer.bats:421:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh depth limit"
tests/impact-analyzer.bats:434:@test "T-IA-006c: impact-analyzer visits cyclic nodes only once" {
tests/impact-analyzer.bats:442:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh cycle dedup"
tests/impact-analyzer.bats:464:@test "T-IA-007: impact-analyzer returns empty matrix for leaf symbol" {
tests/impact-analyzer.bats:472:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh leaf node"
tests/impact-analyzer.bats:483:@test "T-IA-007b: impact-analyzer handles non-existent symbol gracefully" {
tests/impact-analyzer.bats:492:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh nonexistent symbol"
tests/impact-analyzer.bats:508:@test "T-IA-007c: impact-analyzer handles empty database gracefully" {
tests/impact-analyzer.bats:517:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh empty database"
tests/impact-analyzer.bats:530:@test "T-IA-PARAM-001: impact-analyzer requires symbol argument" {
tests/impact-analyzer.bats:535:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh argument validation"
tests/impact-analyzer.bats:543:@test "T-IA-PARAM-002: impact-analyzer validates depth is positive integer" {
tests/impact-analyzer.bats:559:@test "T-IA-PARAM-003: impact-analyzer validates threshold is between 0 and 1" {
tests/impact-analyzer.bats:575:@test "T-IA-PARAM-004: impact-analyzer shows help with --help" {
tests/impact-analyzer.bats:588:# ÂèÇËÄÉËßÑÊ†º: dev-playbooks/changes/algorithm-optimization-parity/specs/impact-analysis/spec.md
tests/impact-analyzer.bats:613:setup_large_graph() {
tests/impact-analyzer.bats:638:@test "CT-IA-001: impact-analyzer returns all nodes up to N hops (SC-IA-001)" {
tests/impact-analyzer.bats:647:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh 5-hop traversal"
tests/impact-analyzer.bats:665:@test "CT-IA-001b: impact-analyzer respects max_depth parameter" {
tests/impact-analyzer.bats:674:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh depth-3 traversal"
tests/impact-analyzer.bats:692:@test "CT-IA-002: impact-analyzer applies 20% decay per hop (SC-IA-001)" {
tests/impact-analyzer.bats:700:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh confidence decay"
tests/impact-analyzer.bats:714:    a_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:A" or .symbol == "funcA") | .confidence // .impact // empty' 2>/dev/null | head -1)
tests/impact-analyzer.bats:715:    b_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:B" or .symbol == "funcB") | .confidence // .impact // empty' 2>/dev/null | head -1)
tests/impact-analyzer.bats:716:    c_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:C" or .symbol == "funcC") | .confidence // .impact // empty' 2>/dev/null | head -1)
tests/impact-analyzer.bats:717:    d_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:D" or .symbol == "funcD") | .confidence // .impact // empty' 2>/dev/null | head -1)
tests/impact-analyzer.bats:718:    e_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.id == "sym:func:E" or .symbol == "funcE") | .confidence // .impact // empty' 2>/dev/null | head -1)
tests/impact-analyzer.bats:747:@test "CT-IA-002b: impact-analyzer confidence strictly decreases with depth" {
tests/impact-analyzer.bats:755:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh confidence ordering"
tests/impact-analyzer.bats:761:    a_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.symbol == "funcA") | .confidence // .impact // 1' 2>/dev/null | head -1)
tests/impact-analyzer.bats:762:    b_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.symbol == "funcB") | .confidence // .impact // 1' 2>/dev/null | head -1)
tests/impact-analyzer.bats:763:    c_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.symbol == "funcC") | .confidence // .impact // 1' 2>/dev/null | head -1)
tests/impact-analyzer.bats:764:    d_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.symbol == "funcD") | .confidence // .impact // 1' 2>/dev/null | head -1)
tests/impact-analyzer.bats:765:    e_conf=$(echo "$output" | jq -r '.affected_nodes[] | select(.symbol == "funcE") | .confidence // .impact // 1' 2>/dev/null | head -1)
tests/impact-analyzer.bats:786:@test "CT-IA-003: impact-analyzer detects and truncates cycles (SC-IA-003)" {
tests/impact-analyzer.bats:795:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh cycle detection"
tests/impact-analyzer.bats:811:@test "CT-IA-003b: impact-analyzer completes in reasonable time with cycles" {
tests/impact-analyzer.bats:829:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh cycle performance"
tests/impact-analyzer.bats:837:@test "CT-IA-004: impact-analyzer stops when confidence < threshold (SC-IA-002)" {
tests/impact-analyzer.bats:847:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh threshold cutoff"
tests/impact-analyzer.bats:861:@test "CT-IA-004b: impact-analyzer uses default threshold 0.1" {
tests/impact-analyzer.bats:871:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh default threshold"
tests/impact-analyzer.bats:883:@test "CT-IA-004c: impact-analyzer threshold stops downstream traversal" {
tests/impact-analyzer.bats:893:    skip_if_not_ready "$status" "$output" "impact-analyzer.sh threshold stops traversal"
tests/impact-analyzer.bats:911:@test "CT-IA-005: impact-analyzer analyzes 5000-edge graph in < 200ms" {
tests/impact-analyzer.bats:916:    setup_large_graph 500 5000
tests/impact-analyzer.bats:923:    local exit_code=$?
tests/impact-analyzer.bats:925:    skip_if_not_ready "$exit_code" "" "impact-analyzer.sh large graph analysis"
tests/impact-analyzer.bats:934:@test "CT-IA-005b: impact-analyzer performance with multiple runs (P95 < 200ms)" {
tests/impact-analyzer.bats:939:    setup_large_graph 500 5000
tests/ast-delta.bats:2:# ast-delta.bats - AST Delta Â¢ûÈáèÁ¥¢ÂºïÊ®°ÂùóÊµãËØï
tests/ast-delta.bats:20:AST_DELTA="$SCRIPT_DIR/ast-delta.sh"
tests/ast-delta.bats:29:    export GRAPH_DB_PATH="$TEST_TEMP_DIR/graph.db"
tests/ast-delta.bats:31:    export AST_CACHE_DIR="$TEST_TEMP_DIR/.ast-cache"
tests/ast-delta.bats:97:# Usage: create_ast_cache <file_path> [version_stamp]
tests/ast-delta.bats:98:create_ast_cache() {
tests/ast-delta.bats:101:    local cache_file="$AST_CACHE_DIR/$(echo "$file_path" | tr '/' '_').ast"
tests/ast-delta.bats:103:    cat > "$cache_file" << EOF
tests/ast-delta.bats:115:    echo "$cache_file"
tests/ast-delta.bats:118:# ÂàõÂª∫Â∏¶ÊúâÁâàÊú¨Êà≥ÁöÑ graph.db
tests/ast-delta.bats:119:# Usage: create_graph_db [version_stamp]
tests/ast-delta.bats:120:create_graph_db() {
tests/ast-delta.bats:123:    # ‰ΩøÁî® graph-store.sh ÂàùÂßãÂåñÔºàÂ¶ÇÊûúÂèØÁî®Ôºâ
tests/ast-delta.bats:124:    if [ -x "$SCRIPT_DIR/graph-store.sh" ]; then
tests/ast-delta.bats:125:        "$SCRIPT_DIR/graph-store.sh" init 2>/dev/null || true
tests/ast-delta.bats:155:    sqlite3 "$GRAPH_DB_PATH" "INSERT OR REPLACE INTO metadata (key, value) VALUES ('ast_cache_version', '$version');"
tests/ast-delta.bats:164:        touch "$DEVBOOKS_DIR/.ast-delta-temp-$i.tmp"
tests/ast-delta.bats:185:@test "T-AD-001: ast-delta update performs single file incremental update" {
tests/ast-delta.bats:189:    create_graph_db "v1.0"
tests/ast-delta.bats:191:    create_ast_cache "$test_file" "v1.0"
tests/ast-delta.bats:193:    # When: Ë∞ÉÁî® ast-delta.sh update <file-path>
tests/ast-delta.bats:195:    skip_if_not_ready "$status" "$output" "ast-delta.sh update"
tests/ast-delta.bats:197:    # Then: Ëß£ÊûêÊñ∞ AST„ÄÅËÆ°ÁÆóÂ∑ÆÂºÇ„ÄÅÊõ¥Êñ∞ graph.db
tests/ast-delta.bats:210:@test "T-AD-001b: ast-delta update detects symbol changes" {
tests/ast-delta.bats:214:    create_graph_db "v1.0"
tests/ast-delta.bats:216:    create_ast_cache "$test_file" "v1.0"
tests/ast-delta.bats:220:    skip_if_not_ready "$status" "$output" "ast-delta.sh update (initial)"
tests/ast-delta.bats:230:    skip_if_not_ready "$status" "$output" "ast-delta.sh update (after change)"
tests/ast-delta.bats:241:@test "T-AD-002: ast-delta batch updates multiple files since commit" {
tests/ast-delta.bats:245:    create_graph_db "v1.0"
tests/ast-delta.bats:256:        create_ast_cache "$TEST_REPO_DIR/src/module$i.ts" "v1.0"
tests/ast-delta.bats:263:    # When: Ë∞ÉÁî® ast-delta.sh batch --since HEAD~1
tests/ast-delta.bats:265:    skip_if_not_ready "$status" "$output" "ast-delta.sh batch"
tests/ast-delta.bats:275:@test "T-AD-002b: ast-delta batch uses incremental path for small changes" {
tests/ast-delta.bats:279:    create_graph_db "v1.0"
tests/ast-delta.bats:296:    skip_if_not_ready "$status" "$output" "ast-delta.sh batch (incremental)"
tests/ast-delta.bats:308:@test "T-AD-003: ast-delta triggers full rebuild when cache version mismatch" {
tests/ast-delta.bats:311:    # Given: AST ÁºìÂ≠òÁâàÊú¨Êà≥‰∏é graph.db ‰∏ç‰∏ÄËá¥
tests/ast-delta.bats:312:    create_graph_db "v1.0"
tests/ast-delta.bats:314:    create_ast_cache "$test_file" "v2.0"  # ÁâàÊú¨‰∏çÂåπÈÖç
tests/ast-delta.bats:320:    # When: Ë∞ÉÁî® ast-delta.sh update <file-path>
tests/ast-delta.bats:322:    skip_if_not_ready "$status" "$output" "ast-delta.sh update (cache mismatch)"
tests/ast-delta.bats:328:    assert_contains_any "$output" "FULL_REBUILD" "full_rebuild" "cache invalidated" "rebuilding" "FALLBACK" "fallback"
tests/ast-delta.bats:332:@test "T-AD-003b: ast-delta triggers rebuild when cache missing" {
tests/ast-delta.bats:336:    create_graph_db "v1.0"
tests/ast-delta.bats:342:    skip_if_not_ready "$status" "$output" "ast-delta.sh update (no cache)"
tests/ast-delta.bats:349:@test "T-AD-004: ast-delta falls back to SCIP when tree-sitter unavailable" {
tests/ast-delta.bats:353:    create_graph_db "v1.0"
tests/ast-delta.bats:360:    # When: Ë∞ÉÁî® ast-delta.sh update <file-path>
tests/ast-delta.bats:367:    skip_if_not_ready "$status" "$output" "ast-delta.sh update (SCIP fallback)"
tests/ast-delta.bats:377:@test "T-AD-004b: ast-delta SCIP fallback still provides symbol extraction" {
tests/ast-delta.bats:381:    create_graph_db "v1.0"
tests/ast-delta.bats:391:    skip_if_not_ready "$status" "$output" "ast-delta.sh SCIP mode"
tests/ast-delta.bats:398:@test "T-AD-005: ast-delta triggers full rebuild for large batch" {
tests/ast-delta.bats:402:    create_graph_db "v1.0"
tests/ast-delta.bats:418:    # When: Ë∞ÉÁî® ast-delta.sh batch --since <ref>
tests/ast-delta.bats:420:    skip_if_not_ready "$status" "$output" "ast-delta.sh batch (large)"
tests/ast-delta.bats:430:@test "T-AD-005b: ast-delta batch threshold is configurable" {
tests/ast-delta.bats:434:    create_graph_db "v1.0"
tests/ast-delta.bats:454:    skip_if_not_ready "$status" "$output" "ast-delta.sh batch (low threshold)"
tests/ast-delta.bats:465:@test "T-AD-006: ast-delta single file update P95 latency <= 120ms" {
tests/ast-delta.bats:469:    create_graph_db "v1.0"
tests/ast-delta.bats:472:    create_ast_cache "$test_file" "v1.0"
tests/ast-delta.bats:476:    skip_if_not_ready "$status" "$output" "ast-delta.sh update (warmup)"
tests/ast-delta.bats:503:@test "T-AD-006b: ast-delta incremental update faster than full rebuild" {
tests/ast-delta.bats:507:    create_graph_db "v1.0"
tests/ast-delta.bats:510:    create_ast_cache "$test_file" "v1.0"
tests/ast-delta.bats:514:    skip_if_not_ready "$status" "$output" "ast-delta.sh (speed warmup)"
tests/ast-delta.bats:554:@test "T-AD-007: ast-delta cleans up orphan temp files on invocation" {
tests/ast-delta.bats:558:    create_graph_db "v1.0"
tests/ast-delta.bats:567:    orphan_count_before=$(find "$DEVBOOKS_DIR" -name ".ast-delta-temp-*.tmp" 2>/dev/null | wc -l | tr -d ' ')
tests/ast-delta.bats:570:    # When: ‰∏ãÊ¨°Ë∞ÉÁî® ast-delta.shÔºàËÆæÁΩÆÊµãËØïÁéØÂ¢ÉÂèòÈáè‰ª•Á´ãÂç≥Ê∏ÖÁêÜÔºâ
tests/ast-delta.bats:576:    skip_if_not_ready "$status" "$output" "ast-delta.sh update (cleanup)"
tests/ast-delta.bats:582:    orphan_count_after=$(find "$DEVBOOKS_DIR" -name ".ast-delta-temp-*.tmp" 2>/dev/null | wc -l | tr -d ' ')
tests/ast-delta.bats:587:@test "T-AD-007b: ast-delta atomic write prevents partial updates" {
tests/ast-delta.bats:591:    create_graph_db "v1.0"
tests/ast-delta.bats:593:    create_ast_cache "$test_file" "v1.0"
tests/ast-delta.bats:597:    skip_if_not_ready "$status" "$output" "ast-delta.sh (atomic setup)"
tests/ast-delta.bats:622:@test "T-AD-007c: ast-delta handles concurrent updates safely" {
tests/ast-delta.bats:626:    create_graph_db "v1.0"
tests/ast-delta.bats:629:        create_ast_cache "$TEST_REPO_DIR/src/concurrent$i.ts" "v1.0"
tests/ast-delta.bats:655:@test "BOUNDARY: ast-delta handles empty file gracefully" {
tests/ast-delta.bats:658:    create_graph_db "v1.0"
tests/ast-delta.bats:663:    skip_if_not_ready "$status" "$output" "ast-delta.sh (empty file)"
tests/ast-delta.bats:670:@test "BOUNDARY: ast-delta handles large file" {
tests/ast-delta.bats:673:    create_graph_db "v1.0"
tests/ast-delta.bats:678:    skip_if_not_ready "$status" "$output" "ast-delta.sh (large file)"
tests/ast-delta.bats:684:@test "BOUNDARY: ast-delta fails gracefully for non-existent file" {
tests/ast-delta.bats:687:    create_graph_db "v1.0"
tests/ast-delta.bats:697:@test "BOUNDARY: ast-delta handles non-TypeScript files" {
tests/ast-delta.bats:700:    create_graph_db "v1.0"
tests/ast-delta.bats:714:@test "BOUNDARY: ast-delta handles syntax error in file" {
tests/ast-delta.bats:717:    create_graph_db "v1.0"
tests/ast-delta.bats:726:    skip_if_not_ready "$status" "$output" "ast-delta.sh (syntax error)"
tests/ast-delta.bats:740:@test "CLI: ast-delta --help shows usage" {
tests/ast-delta.bats:750:@test "CLI: ast-delta without args shows usage" {
tests/ast-delta.bats:760:@test "CLI: ast-delta rejects invalid command" {
tests/daemon.bats:2:# daemon.bats - Â∏∏È©ªÂÆàÊä§ËøõÁ®ãÊµãËØï
tests/daemon.bats:25:DAEMON="$SCRIPT_DIR/daemon.sh"
tests/daemon.bats:26:GRAPH_STORE="$SCRIPT_DIR/graph-store.sh"
tests/daemon.bats:27:CACHE_MANAGER="$SCRIPT_DIR/cache-manager.sh"
tests/daemon.bats:64:    export DAEMON_SOCK="$DEVBOOKS_DIR/daemon.sock"
tests/daemon.bats:65:    export DAEMON_PID_FILE="$DEVBOOKS_DIR/daemon.pid"
tests/daemon.bats:66:    export GRAPH_DB_PATH="$DEVBOOKS_DIR/graph.db"
tests/daemon.bats:82:    # Ê∏ÖÁêÜÁõëÊéßËøõÁ®ãÔºàÂÖ≥ÈîÆÔºömonitor_loop Áã¨Á´ã‰∫é‰∏ª daemon ËøêË°åÔºâ
tests/daemon.bats:116:    rm -f "$saved_devbooks_dir/daemon.stop" "$saved_devbooks_dir/monitor.stop" 2>/dev/null || true
tests/daemon.bats:122:track_daemon_pid() {
tests/daemon.bats:129:# Helper: ÂêØÂä® daemon Âπ∂Ë∑üË∏™ÊâÄÊúâÁõ∏ÂÖ≥ PID
tests/daemon.bats:131:start_daemon_tracked() {
tests/daemon.bats:140:        [ -n "$dpid" ] && track_daemon_pid "$dpid"
tests/daemon.bats:145:        [ -n "$mpid" ] && track_daemon_pid "$mpid"
tests/daemon.bats:158:@test "SC-DM-001: daemon start creates PID file and socket" {
tests/daemon.bats:163:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:175:@test "SC-DM-002: daemon start rejects when already running" {
tests/daemon.bats:180:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:190:@test "SC-DM-003: daemon start cleans stale PID file" {
tests/daemon.bats:198:    skip_if_not_ready "$status" "$output" "daemon.sh stale cleanup"
tests/daemon.bats:208:@test "SC-DM-004: daemon responds to ping request" {
tests/daemon.bats:212:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:218:    skip_if_not_ready "$status" "$output" "daemon.sh ping"
tests/daemon.bats:225:@test "SC-DM-005: daemon handles query request" {
tests/daemon.bats:234:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:240:    skip_if_not_ready "$status" "$output" "daemon.sh query"
tests/daemon.bats:253:@test "SC-DM-006: daemon returns busy when queue is full" {
tests/daemon.bats:262:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:266:    track_daemon_pid "$(cat "$DAEMON_PID_FILE" 2>/dev/null)"
tests/daemon.bats:268:    # È™åËØÅ daemon ÊòØÂê¶ÊîØÊåÅÈòüÂàóÈôêÂà∂Âíå mock Âª∂Ëøü
tests/daemon.bats:276:        skip "daemon queue limit not yet implemented"
tests/daemon.bats:282:    track_daemon_pid "$pid1"
tests/daemon.bats:284:    # Á≠âÂæÖË∂≥Â§üÊó∂Èó¥Á°Æ‰øùÁ¨¨‰∏Ä‰∏™ËØ∑Ê±ÇÂ∑≤Ë¢´ daemon Êé•Êî∂Âπ∂ÂºÄÂßãÂ§ÑÁêÜ
tests/daemon.bats:289:        # Ê£ÄÊü• daemon ÊòØÂê¶Ê≠£Âú®Â§ÑÁêÜËØ∑Ê±ÇÔºàÈÄöËøá status ÊàñËøõÁ®ãÁä∂ÊÄÅÔºâ
tests/daemon.bats:292:            skip "Mock delay not supported - first request completed too fast"
tests/daemon.bats:317:    skip "Queue saturation not observable - daemon may not enforce queue limits"
tests/daemon.bats:325:@test "SC-DM-007: daemon stop gracefully shuts down" {
tests/daemon.bats:329:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:334:    skip_if_not_ready "$status" "$output" "daemon.sh stop"
tests/daemon.bats:343:@test "SC-DM-008: daemon auto-restarts after crash" {
tests/daemon.bats:347:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:352:    [ -n "$original_pid" ] || skip "Could not get daemon PID"
tests/daemon.bats:353:    track_daemon_pid "$original_pid"
tests/daemon.bats:371:                track_daemon_pid "$new_pid"
tests/daemon.bats:381:    skip "daemon auto-restart not yet implemented"
tests/daemon.bats:385:@test "SC-DM-009: daemon enters FAILED state after max restarts" {
tests/daemon.bats:392:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:400:            track_daemon_pid "$pid"
tests/daemon.bats:410:    skip_if_not_ready "$status" "$output" "daemon.sh status"
tests/daemon.bats:420:    skip "daemon max restart limit not yet implemented"
tests/daemon.bats:424:@test "SC-DM-010: daemon status returns running info" {
tests/daemon.bats:428:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:433:    skip_if_not_ready "$status" "$output" "daemon.sh status"
tests/daemon.bats:440:@test "SC-DM-011: daemon status returns not running" {
tests/daemon.bats:445:    skip_if_not_ready "$status" "$output" "daemon.sh status"
tests/daemon.bats:456:@test "SC-DM-012: daemon P95 latency is below ${PERF_P95_THRESHOLD_MS}ms for ${PERF_TEST_ITERATIONS} requests" {
tests/daemon.bats:460:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:489:@test "AC-N02: daemon cold start latency is recorded" {
tests/daemon.bats:496:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:511:@test "test_warmup_success: daemon warmup completes" {
tests/daemon.bats:516:        skip_if_not_ready "$status" "$output" "daemon.sh warmup"
tests/daemon.bats:544:        # Non-JSON output should at least mention warmup
tests/daemon.bats:552:@test "test_warmup_cache_populated: warmup populates cache entries" {
tests/daemon.bats:558:    skip_if_not_ready "$status" "$output" "daemon.sh warmup"
tests/daemon.bats:561:    skip_if_not_ready "$status" "$output" "cache-manager.sh stats"
tests/daemon.bats:566:        skip_not_implemented "cache stats total_entries"
tests/daemon.bats:570:@test "test_warmup_hotspot: warmup reports hotspot cache population" {
tests/daemon.bats:575:    skip_if_not_ready "$status" "$output" "daemon.sh warmup --format json"
tests/daemon.bats:577:    local hotspot_cached
tests/daemon.bats:578:    hotspot_cached=$(echo "$output" | jq -r '.hotspot_cached // empty')
tests/daemon.bats:579:    if [ -z "$hotspot_cached" ]; then
tests/daemon.bats:580:        skip_not_implemented "warmup hotspot cache reporting"
tests/daemon.bats:584:@test "test_warmup_symbols: warmup reports symbol cache population" {
tests/daemon.bats:589:    skip_if_not_ready "$status" "$output" "daemon.sh warmup --format json"
tests/daemon.bats:591:    local symbols_cached
tests/daemon.bats:592:    symbols_cached=$(echo "$output" | jq -r '.symbols_cached // empty')
tests/daemon.bats:593:    if [ -z "$symbols_cached" ]; then
tests/daemon.bats:594:        skip_not_implemented "warmup symbol cache reporting"
tests/daemon.bats:609:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:614:    track_daemon_pid "$pid1"
tests/daemon.bats:618:    skip_if_not_ready "$status" "$output" "daemon.sh query (new request)"
tests/daemon.bats:643:    # First verify flock is mentioned in the daemon script
tests/daemon.bats:653:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:656:    # Track daemon PID for cleanup
tests/daemon.bats:657:    track_daemon_pid "$(cat "$DAEMON_PID_FILE" 2>/dev/null)"
tests/daemon.bats:662:    track_daemon_pid "$pid1"
tests/daemon.bats:666:    track_daemon_pid "$pid2"
tests/daemon.bats:702:    # Verify daemon is still running (not crashed due to race condition)
tests/daemon.bats:704:        local daemon_pid
tests/daemon.bats:705:        daemon_pid=$(cat "$DAEMON_PID_FILE" 2>/dev/null)
tests/daemon.bats:706:        if [ -n "$daemon_pid" ] && ! kill -0 "$daemon_pid" 2>/dev/null; then
tests/daemon.bats:707:            skip_not_implemented "flock atomicity: daemon crashed during concurrent cancel"
tests/daemon.bats:726:    # Start daemon and trigger some cancellation scenarios
tests/daemon.bats:728:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:731:    track_daemon_pid "$(cat "$DAEMON_PID_FILE" 2>/dev/null)"
tests/daemon.bats:737:    track_daemon_pid "$pid1"
tests/daemon.bats:747:    # Stop daemon gracefully
tests/daemon.bats:756:        # List the leftover files for debugging
tests/daemon.bats:785:    skip_if_not_ready "$status" "$output" "daemon.sh start"
tests/daemon.bats:789:    skip_if_not_ready "$status" "$output" "daemon.sh query"
tests/indexer-scheduler.bats:2:# indexer-scheduler.bats - Indexing Pipeline Optimization Tests
tests/indexer-scheduler.bats:7:# Purpose: Verify indexer scheduler logic including:
tests/indexer-scheduler.bats:12:#   - ci_index_status semantic alignment (AC-005)
tests/indexer-scheduler.bats:21:# Run: bats tests/indexer-scheduler.bats
tests/indexer-scheduler.bats:28:INDEXER_SCRIPT="$SCRIPT_DIR/indexer.sh"
tests/indexer-scheduler.bats:29:SCIP_TO_GRAPH="$SCRIPT_DIR/scip-to-graph.sh"
tests/indexer-scheduler.bats:30:AST_DELTA="$SCRIPT_DIR/ast-delta.sh"
tests/indexer-scheduler.bats:31:GRAPH_STORE="$SCRIPT_DIR/graph-store.sh"
tests/indexer-scheduler.bats:32:EMBEDDING_SCRIPT="$SCRIPT_DIR/embedding.sh"
tests/indexer-scheduler.bats:41:    export GRAPH_DB_PATH="$TEST_TEMP_DIR/graph.db"
tests/indexer-scheduler.bats:43:    export AST_CACHE_DIR="$TEST_TEMP_DIR/.ast-cache"
tests/indexer-scheduler.bats:90:    local ast_delta_enabled="${2:-true}"
tests/indexer-scheduler.bats:98:  ast_delta:
tests/indexer-scheduler.bats:99:    enabled: $ast_delta_enabled
tests/indexer-scheduler.bats:101:  indexer:
tests/indexer-scheduler.bats:112:    cat > "$vendored_dir/scip.proto" << 'EOF'
tests/indexer-scheduler.bats:115:// Source: github.com/sourcegraph/scip
tests/indexer-scheduler.bats:118:package scip;
tests/indexer-scheduler.bats:130:    TextEncoding text_document_encoding = 4;
tests/indexer-scheduler.bats:135:# Create graph database with version stamp
tests/indexer-scheduler.bats:136:create_graph_db_with_version() {
tests/indexer-scheduler.bats:159:INSERT OR REPLACE INTO metadata (key, value) VALUES ('ast_cache_version', '$version');
tests/indexer-scheduler.bats:160:INSERT OR REPLACE INTO metadata (key, value) VALUES ('last_updated', datetime('now'));
tests/indexer-scheduler.bats:169:@test "IS-001: indexer invokes incremental path for single file change" {
tests/indexer-scheduler.bats:172:    # Given: tree-sitter available, AST cache version matches, single file change
tests/indexer-scheduler.bats:174:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:182:    # When: Trigger indexer with dry-run for single file
tests/indexer-scheduler.bats:186:    skip_if_not_ready "$status" "$output" "indexer.sh --dry-run"
tests/indexer-scheduler.bats:192:# @test IS-001b: Incremental path calls ast-delta.sh update
tests/indexer-scheduler.bats:193:@test "IS-001b: incremental path calls ast-delta update for single file" {
tests/indexer-scheduler.bats:199:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:206:    # When: Execute indexer --once for single file
tests/indexer-scheduler.bats:209:    skip_if_not_ready "$status" "$output" "indexer.sh --once"
tests/indexer-scheduler.bats:211:    # Then: Should invoke ast-delta path
tests/indexer-scheduler.bats:213:    assert_contains_any "$output" "ast-delta" "incremental" "update"
tests/indexer-scheduler.bats:222:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:240:    skip_if_not_ready "$status" "$output" "indexer.sh threshold check"
tests/indexer-scheduler.bats:257:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:262:    # When: Trigger indexer
tests/indexer-scheduler.bats:267:    skip_if_not_ready "$status" "$output" "indexer.sh fallback"
tests/indexer-scheduler.bats:275:# @test IS-002b: Fallback when cache version mismatch
tests/indexer-scheduler.bats:276:@test "IS-002b: fallback to full rebuild when cache version mismatch" {
tests/indexer-scheduler.bats:279:    # Given: AST cache version differs from graph.db version
tests/indexer-scheduler.bats:281:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:289:    # When: Trigger indexer
tests/indexer-scheduler.bats:292:    skip_if_not_ready "$status" "$output" "indexer.sh cache mismatch"
tests/indexer-scheduler.bats:297:    assert_contains_any "$output" "cache_version_mismatch" "version" "mismatch"
tests/indexer-scheduler.bats:306:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:324:    skip_if_not_ready "$status" "$output" "indexer.sh threshold exceeded"
tests/indexer-scheduler.bats:337:@test "IS-003: scip-to-graph uses vendored proto in offline mode" {
tests/indexer-scheduler.bats:344:    export VENDORED_PROTO_PATH="$TEST_TEMP_DIR/vendored/scip.proto"
tests/indexer-scheduler.bats:350:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh --check-proto"
tests/indexer-scheduler.bats:358:@test "IS-003b: scip-to-graph respects custom SCIP_PROTO_PATH" {
tests/indexer-scheduler.bats:364:    export SCIP_PROTO_PATH="$TEST_TEMP_DIR/custom-proto/scip.proto"
tests/indexer-scheduler.bats:369:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh custom proto"
tests/indexer-scheduler.bats:377:@test "IS-003c: scip-to-graph fails clearly when proto not found and download disabled" {
tests/indexer-scheduler.bats:390:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh missing proto"
tests/indexer-scheduler.bats:398:@test "IS-003d: scip-to-graph outputs proto_version in result" {
tests/indexer-scheduler.bats:404:    export VENDORED_PROTO_PATH="$TEST_TEMP_DIR/vendored/scip.proto"
tests/indexer-scheduler.bats:409:    skip_if_not_ready "$status" "$output" "scip-to-graph.sh proto version"
tests/indexer-scheduler.bats:421:@test "IS-004: indexer.sh --help shows existing options" {
tests/indexer-scheduler.bats:436:@test "IS-004b: indexer.sh --status returns daemon status" {
tests/indexer-scheduler.bats:443:    # Exit code 0 for running, non-zero for not running - both are valid
tests/indexer-scheduler.bats:444:    assert_contains_any "$output" "running" "not running" "status" "daemon"
tests/indexer-scheduler.bats:448:@test "IS-004c: indexer.sh --dry-run parameter supported" {
tests/indexer-scheduler.bats:454:    skip_if_not_ready "$status" "$output" "indexer.sh --dry-run"
tests/indexer-scheduler.bats:459:    # Should NOT modify graph.db
tests/indexer-scheduler.bats:468:@test "IS-004d: indexer.sh --once parameter supported" {
tests/indexer-scheduler.bats:474:    skip_if_not_ready "$status" "$output" "indexer.sh --once in help"
tests/indexer-scheduler.bats:484:# @test IS-005: ci_index_status routes to embedding.sh
tests/indexer-scheduler.bats:485:@test "IS-005: ci_index_status status action calls embedding.sh status" {
tests/indexer-scheduler.bats:489:    # We need to check that server.ts routes ci_index_status to embedding.sh
tests/indexer-scheduler.bats:491:    # Given: Server code exists
tests/indexer-scheduler.bats:501:    # Then: Should route to embedding.sh not indexer.sh
tests/indexer-scheduler.bats:502:    # First, verify embedding.sh is referenced
tests/indexer-scheduler.bats:503:    assert_contains_any "$output" "embedding" "embedding.sh"
tests/indexer-scheduler.bats:505:    # Then, verify indexer.sh is NOT used for this tool
tests/indexer-scheduler.bats:507:    # Note: We check if the handler block mentions indexer.sh as the route target
tests/indexer-scheduler.bats:509:    # actual routing patterns like "indexer.sh" in spawn/exec context
tests/indexer-scheduler.bats:510:    if echo "$output" | grep -E "(spawn|exec|call).*indexer\.sh" > /dev/null 2>&1; then
tests/indexer-scheduler.bats:511:        echo "FAIL: ci_index_status routes to indexer.sh but should route to embedding.sh" >&2
tests/indexer-scheduler.bats:516:    # Additional verification: the handler should explicitly use embedding.sh
tests/indexer-scheduler.bats:517:    # Check for embedding-related patterns in the handler
tests/indexer-scheduler.bats:518:    if ! echo "$output" | grep -E "(embedding|embed)" > /dev/null 2>&1; then
tests/indexer-scheduler.bats:519:        echo "FAIL: ci_index_status handler does not reference embedding" >&2
tests/indexer-scheduler.bats:525:@test "IS-005a: embedding.sh status returns valid status" {
tests/indexer-scheduler.bats:528:    # When: Call embedding.sh status
tests/indexer-scheduler.bats:531:    skip_if_not_ready "$status" "$output" "embedding.sh status"
tests/indexer-scheduler.bats:538:# @test IS-005b: ci_index_status build action maps to embedding.sh build
tests/indexer-scheduler.bats:539:@test "IS-005b: embedding.sh build command exists" {
tests/indexer-scheduler.bats:550:# @test IS-005c: ci_index_status clear action maps to embedding.sh clean
tests/indexer-scheduler.bats:551:@test "IS-005c: embedding.sh clean command exists" {
tests/indexer-scheduler.bats:572:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:582:    # When: Run indexer first time to establish baseline
tests/indexer-scheduler.bats:584:    skip_if_not_ready "$status" "$output" "indexer.sh first run"
tests/indexer-scheduler.bats:590:    # Run indexer second time
tests/indexer-scheduler.bats:592:    skip_if_not_ready "$status" "$output" "indexer.sh second run"
tests/indexer-scheduler.bats:597:    # Run indexer third time
tests/indexer-scheduler.bats:599:    skip_if_not_ready "$status" "$output" "indexer.sh third run"
tests/indexer-scheduler.bats:624:    create_test_config "$TEST_TEMP_DIR/config" false 10 2 true  # ast_delta disabled
tests/indexer-scheduler.bats:630:    skip_if_not_ready "$status" "$output" "indexer.sh full rebuild 1"
tests/indexer-scheduler.bats:636:    skip_if_not_ready "$status" "$output" "indexer.sh full rebuild 2"
tests/indexer-scheduler.bats:651:# True debounce testing with timing would require a daemon mode test, which is covered
tests/indexer-scheduler.bats:669:    skip_if_not_ready "$status" "$output" "indexer.sh batch processing"
tests/indexer-scheduler.bats:690:    skip_if_not_ready "$status" "$output" "indexer.sh config read"
tests/indexer-scheduler.bats:704:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:714:    # When: Trigger indexer twice rapidly (within debounce window)
tests/indexer-scheduler.bats:720:    skip_if_not_ready "$status" "$output" "indexer.sh timing test"
tests/indexer-scheduler.bats:771:    skip_if_not_ready "$status" "$output" "indexer.sh version stamp update"
tests/indexer-scheduler.bats:782:@test "IS-008b: AST cache cleared when version mismatch triggers rebuild" {
tests/indexer-scheduler.bats:787:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:792:    # Create stale cache files
tests/indexer-scheduler.bats:793:    touch "$AST_CACHE_DIR/stale_cache_file.ast"
tests/indexer-scheduler.bats:800:    skip_if_not_ready "$status" "$output" "indexer.sh cache clear"
tests/indexer-scheduler.bats:802:    # Then: Old cache files should be cleared
tests/indexer-scheduler.bats:804:    [ ! -f "$AST_CACHE_DIR/stale_cache_file.ast" ]
tests/indexer-scheduler.bats:813:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:823:    skip_if_not_ready "$status" "$output" "indexer.sh version check"
tests/indexer-scheduler.bats:827:    assert_not_contains "$output" "cache_version_mismatch"
tests/indexer-scheduler.bats:838:    # Given: ast_delta.enabled = false in config
tests/indexer-scheduler.bats:840:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:844:    # When: Trigger indexer (single file, would normally be incremental)
tests/indexer-scheduler.bats:847:    skip_if_not_ready "$status" "$output" "indexer.sh feature toggle"
tests/indexer-scheduler.bats:852:    assert_contains_any "$output" "feature_disabled" "disabled" "ast_delta"
tests/indexer-scheduler.bats:861:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:866:    # When: Trigger indexer
tests/indexer-scheduler.bats:871:    skip_if_not_ready "$status" "$output" "indexer.sh env toggle"
tests/indexer-scheduler.bats:884:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:902:    skip_if_not_ready "$status" "$output" "indexer.sh custom threshold"
tests/indexer-scheduler.bats:914:@test "IS-010: concurrent index operations don't corrupt graph.db" {
tests/indexer-scheduler.bats:919:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:927:    # Create temp files to capture output and exit codes from background processes
tests/indexer-scheduler.bats:935:    # When: Run multiple indexer processes concurrently, capturing output to files
tests/indexer-scheduler.bats:959:    # Collect exit codes
tests/indexer-scheduler.bats:960:    local code1 code2 code3
tests/indexer-scheduler.bats:961:    code1=$(cat "$exit1" 2>/dev/null || echo "1")
tests/indexer-scheduler.bats:962:    code2=$(cat "$exit2" 2>/dev/null || echo "1")
tests/indexer-scheduler.bats:963:    code3=$(cat "$exit3" 2>/dev/null || echo "1")
tests/indexer-scheduler.bats:965:    # Then: All processes should complete successfully (exit code 0)
tests/indexer-scheduler.bats:968:        if [ "$code1" -ne 0 ] || [ "$code2" -ne 0 ] || [ "$code3" -ne 0 ]; then
tests/indexer-scheduler.bats:970:            echo "Process 1 exit: $code1, output: $(cat "$out1" 2>/dev/null)" >&2
tests/indexer-scheduler.bats:971:            echo "Process 2 exit: $code2, output: $(cat "$out2" 2>/dev/null)" >&2
tests/indexer-scheduler.bats:972:            echo "Process 3 exit: $code3, output: $(cat "$out3" 2>/dev/null)" >&2
tests/indexer-scheduler.bats:1008:    create_graph_db_with_version "$GRAPH_DB_PATH" "v1.0"
tests/indexer-scheduler.bats:1071:    skip_if_not_ready "$status" "$output" "indexer.sh empty files"
tests/indexer-scheduler.bats:1088:    skip_if_not_ready "$status" "$output" "indexer.sh non-existent file"
tests/indexer-scheduler.bats:1107:  ast_delta:
tests/indexer-scheduler.bats:1110:  indexer:
tests/indexer-scheduler.bats:1129:@test "IS-CLI-001: indexer.sh --help output is complete" {
tests/indexer-scheduler.bats:1166:    skip_if_not_ready "$status" "$output" "indexer.sh JSON output"
tests/indexer-scheduler.bats:1187:    skip_if_not_ready "$status" "$output" "indexer.sh JSON files"
